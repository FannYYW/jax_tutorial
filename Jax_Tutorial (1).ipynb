{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "last_runtime": {
        "build_target": "//learning/deepmind/dm_python:dm_notebook3_tpu",
        "kind": "private"
      },
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Connect with GDM CPU/GPU/TPU runtime"
      ],
      "metadata": {
        "id": "Tv_cHcwCW1_b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import"
      ],
      "metadata": {
        "id": "pj2YbYVIQ0oG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I9Hr5qi26RW6"
      },
      "outputs": [],
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import grad, jit, vmap\n",
        "from jax import random\n",
        "from jax import jacobian\n",
        "from jax import jacfwd, jacrev\n",
        "\n",
        "from jax import vjp, jvp"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quick Start"
      ],
      "metadata": {
        "id": "nJY2hYi1Q34H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def selu(x, alpha=1.67, lmbda=1.05):\n",
        "  return lmbda * jnp.where(x > 0, x, alpha * jnp.exp(x) - alpha)\n",
        "\n",
        "x = jnp.arange(5.0)\n",
        "print(x, selu(x))"
      ],
      "metadata": {
        "id": "MdAHhSlFCV-T",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730499523180,
          "user_tz": 420,
          "elapsed": 4957,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "76b4127e-d791-415c-b42f-4a885d0cc2aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 1. 2. 3. 4.] [0.        1.05      2.1       3.1499999 4.2      ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "key = random.key(1701)\n",
        "x = random.normal(key, (1_000_000,))\n",
        "%timeit selu(x).block_until_ready()"
      ],
      "metadata": {
        "id": "Hw2DHs62GUtV",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730499530338,
          "user_tz": 420,
          "elapsed": 6996,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "dee5d2e0-2222-48a0-aa98-fca5f4161397"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The slowest run took 15883.98 times longer than the fastest. This could mean that an intermediate result is being cached \n",
            "1 loops, best of 5: 256 µs per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "selu_jit = jit(selu)\n",
        "_ = selu_jit(x)  # compiles on first call\n",
        "%timeit selu_jit(x).block_until_ready()"
      ],
      "metadata": {
        "id": "X2toFJwHHCFZ",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730499536674,
          "user_tz": 420,
          "elapsed": 6169,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "dc336680-4c7a-48fc-d683-123f70d4b891"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The slowest run took 4.16 times longer than the fastest. This could mean that an intermediate result is being cached \n",
            "10000 loops, best of 5: 83 µs per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sum_logistic(x):\n",
        "  return jnp.sum(1.0 / (1.0 + jnp.exp(-x)))\n",
        "\n",
        "x_small = jnp.arange(3.)\n",
        "derivative_fn = grad(sum_logistic)\n",
        "derivative_fn_jit=jit(derivative_fn)\n",
        "print(derivative_fn_jit(x_small))"
      ],
      "metadata": {
        "id": "D1Ct6b-EISp8",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730499538779,
          "user_tz": 420,
          "elapsed": 1935,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "dca994ee-d7de-4791-fb28-8defa4912158"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.25       0.1966118  0.10499343]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_small = jnp.arange(3.)\n",
        "print(x_small)\n",
        "print(jacobian(jnp.exp)(x_small))\n",
        "print(grad(jit(grad(jit(grad(sum_logistic)))))(1.0))"
      ],
      "metadata": {
        "id": "cz3ViBPnMMsP",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730499548899,
          "user_tz": 420,
          "elapsed": 9953,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "8c5accbd-497c-4f78-cf31-b4f07f8e2a83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0. 1. 2.]\n",
            "[[1.        0.        0.       ]\n",
            " [0.        2.71828   0.       ]\n",
            " [0.        0.        7.3890476]]\n",
            "-0.03532532\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from jax import jacfwd, jacrev\n",
        "def hessian(fun):\n",
        "  return jit(jacfwd(jacrev(fun)))\n",
        "print(hessian(sum_logistic)(x_small))"
      ],
      "metadata": {
        "id": "JwuLOa3WWL58",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730499550166,
          "user_tz": 420,
          "elapsed": 1101,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "b3f771bd-3327-4d65-bcad-efed5dab0c1c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-0.         -0.         -0.        ]\n",
            " [-0.         -0.09085784 -0.        ]\n",
            " [-0.         -0.         -0.07996242]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "key1, key2 = random.split(key)\n",
        "mat = random.normal(key1, (150, 100))\n",
        "batched_x = random.normal(key2, (10, 100))\n",
        "\n",
        "def apply_matrix(x):\n",
        "  return jnp.dot(mat, x)\n"
      ],
      "metadata": {
        "id": "Bx8b_AgsYWml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def naively_batched_apply_matrix(v_batched):\n",
        "  return jnp.stack([apply_matrix(v) for v in v_batched])\n",
        "\n",
        "print('Naively batched')\n",
        "%timeit naively_batched_apply_matrix(batched_x).block_until_ready()"
      ],
      "metadata": {
        "id": "92FlIstiYYFX",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730499557994,
          "user_tz": 420,
          "elapsed": 3440,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "6be4a52a-7849-4137-eefb-0f2d2d2290ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Naively batched\n",
            "The slowest run took 3102.01 times longer than the fastest. This could mean that an intermediate result is being cached \n",
            "1 loops, best of 5: 1.11 ms per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "@jit\n",
        "def batched_apply_matrix(batched_x):\n",
        "  return jnp.dot(batched_x, mat.T)\n",
        "\n",
        "# np.testing.assert_allclose(naively_batched_apply_matrix(batched_x),\n",
        "#                            batched_apply_matrix(batched_x), atol=1E-4, rtol=1E-4)\n",
        "print('Manually batched')\n",
        "%timeit batched_apply_matrix(batched_x).block_until_ready()"
      ],
      "metadata": {
        "id": "3OD6PO48YZ1T",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730499566619,
          "user_tz": 420,
          "elapsed": 688,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "3f2f627d-7f6e-4cef-8c43-0f6162ec8841"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Manually batched\n",
            "The slowest run took 10102.79 times longer than the fastest. This could mean that an intermediate result is being cached \n",
            "1 loops, best of 5: 67.2 µs per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from jax import vmap\n",
        "\n",
        "@jit\n",
        "def vmap_batched_apply_matrix(batched_x):\n",
        "  return vmap(apply_matrix)(batched_x)\n",
        "\n",
        "# np.testing.assert_allclose(naively_batched_apply_matrix(batched_x),\n",
        "#                            vmap_batched_apply_matrix(batched_x), atol=1E-4, rtol=1E-4)\n",
        "print('Auto-vectorized with vmap')\n",
        "%timeit vmap_batched_apply_matrix(batched_x).block_until_ready()"
      ],
      "metadata": {
        "id": "cfkeI--5Ynza",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730499577867,
          "user_tz": 420,
          "elapsed": 837,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "2b8bb85d-9a2d-4455-a929-7d1d0c439be6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Auto-vectorized with vmap\n",
            "The slowest run took 12914.65 times longer than the fastest. This could mean that an intermediate result is being cached \n",
            "1 loops, best of 5: 64.2 µs per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Key Concepts\n",
        "\n",
        "https://jax.readthedocs.io/en/latest/key-concepts.html#"
      ],
      "metadata": {
        "id": "ScjjiV6OQ9Ch"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## jax.Array"
      ],
      "metadata": {
        "id": "SVh0OUpnRa4z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# jax.numpy.zeros()\n",
        "# jax.numpy.linspace()\n",
        "# jax.numpy.arange()\n",
        "\n",
        "x = jnp.ones(5)\n",
        "print(x)\n",
        "isinstance(x, jax.Array)\n",
        "# inspect where the contents of the array are stored\n",
        "print(x.devices())\n",
        "print(x.sharding)"
      ],
      "metadata": {
        "id": "RUrdnRvWRDQV",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730497742905,
          "user_tz": 420,
          "elapsed": 52,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "6c6e1e24-eeb6-49c5-8704-c6094c2a8a1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1. 1. 1. 1. 1.]\n",
            "{TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)}\n",
            "SingleDeviceSharding(device=TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0), memory_kind=device)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transformations\n",
        "functions to operate on arrays, e.g., jit, vmap, grad, etc"
      ],
      "metadata": {
        "id": "jDHWa56xvLGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def selu(x, alpha=1.67, lambda_=1.05):\n",
        "  return lambda_ * jnp.where(x > 0, x, alpha * jnp.exp(x) - alpha)\n",
        "\n",
        "selu_jit = jax.jit(selu)\n",
        "print(selu_jit(1.0))\n",
        "\n",
        "# Equal to\n",
        "@jax.jit\n",
        "def selu(x, alpha=1.67, lambda_=1.05):\n",
        "  return lambda_ * jnp.where(x > 0, x, alpha * jnp.exp(x) - alpha)"
      ],
      "metadata": {
        "id": "wLT-xKCDsxEu",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730497686345,
          "user_tz": 420,
          "elapsed": 3,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "6623c2e6-2d34-478b-eade-890d3a914772"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tracing\n",
        "\n",
        "Trace the sequence of operations performed by a Python function"
      ],
      "metadata": {
        "id": "_J9ADHAGvmlt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# see tracing by printing any array value within transformed JAX code\n",
        "@jax.jit\n",
        "def f(x):\n",
        "  print(x)\n",
        "  return x + 1\n",
        "\n",
        "x = jnp.arange(5)\n",
        "result = f(f(x))\n",
        "print(x)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "X0vwl6ymvsV7",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730498838916,
          "user_tz": 420,
          "elapsed": 962,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "006740e8-e839-42bf-8656-e3b64bfeb6b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traced<ShapedArray(int32[5])>with<DynamicJaxprTrace>\n",
            "[0 1 2 3 4]\n",
            "[2 3 4 5 6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Jaxprs\n",
        "\n",
        "- a simple representation of primitive operations\n",
        "- if we have a Python conditional, the jaxpr will only know about the branch we take"
      ],
      "metadata": {
        "id": "WRN-q52hxf43"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def selu(x, alpha=1.67, lambda_=1.05):\n",
        "  return lambda_ * jnp.where(x > 0, x, alpha * jnp.exp(x) - alpha)\n",
        "\n",
        "x = jnp.arange(5.0)\n",
        "jax.make_jaxpr(selu)(x)"
      ],
      "metadata": {
        "id": "AWuadySaxTKK",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730498936220,
          "user_tz": 420,
          "elapsed": 759,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "132b3a94-acb7-406c-86e7-eacf26f08c05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{ \u001b[34m\u001b[22m\u001b[1mlambda \u001b[39m\u001b[22m\u001b[22m; a\u001b[35m:f32[5]\u001b[39m. \u001b[34m\u001b[22m\u001b[1mlet\n",
              "    \u001b[39m\u001b[22m\u001b[22mb\u001b[35m:bool[5]\u001b[39m = gt a 0.0\n",
              "    c\u001b[35m:f32[5]\u001b[39m = exp a\n",
              "    d\u001b[35m:f32[5]\u001b[39m = mul 1.6699999570846558 c\n",
              "    e\u001b[35m:f32[5]\u001b[39m = sub d 1.6699999570846558\n",
              "    f\u001b[35m:f32[5]\u001b[39m = pjit[\n",
              "      name=_where\n",
              "      jaxpr={ \u001b[34m\u001b[22m\u001b[1mlambda \u001b[39m\u001b[22m\u001b[22m; g\u001b[35m:bool[5]\u001b[39m h\u001b[35m:f32[5]\u001b[39m i\u001b[35m:f32[5]\u001b[39m. \u001b[34m\u001b[22m\u001b[1mlet\n",
              "          \u001b[39m\u001b[22m\u001b[22mj\u001b[35m:f32[5]\u001b[39m = select_n g i h\n",
              "        \u001b[34m\u001b[22m\u001b[1min \u001b[39m\u001b[22m\u001b[22m(j,) }\n",
              "    ] b a e\n",
              "    k\u001b[35m:f32[5]\u001b[39m = mul 1.0499999523162842 f\n",
              "  \u001b[34m\u001b[22m\u001b[1min \u001b[39m\u001b[22m\u001b[22m(k,) }"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pytrees\n",
        "\n",
        "- structure for handling collections of params\n",
        "- jax.tree lib"
      ],
      "metadata": {
        "id": "Ph47q0IzzF1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# (nested) list of parameters\n",
        "params = [1, 2, (jnp.arange(3), jnp.ones(2))]\n",
        "print(params)\n",
        "\n",
        "print(jax.tree.structure(params))\n",
        "print(jax.tree.leaves(params))"
      ],
      "metadata": {
        "id": "Qj5GT5zkzJOv",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730499417166,
          "user_tz": 420,
          "elapsed": 1262,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "fdd0438a-9352-46f3-a33f-95dfb56a49fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 2, (Array([0, 1, 2], dtype=int32), Array([1., 1.], dtype=float32))]\n",
            "PyTreeDef([*, *, (*, *)])\n",
            "[1, 2, Array([0, 1, 2], dtype=int32), Array([1., 1.], dtype=float32)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dictionary of parameters\n",
        "params = {'n': 5, 'W': jnp.ones((2, 2)), 'b': jnp.zeros(2)}\n",
        "\n",
        "print(jax.tree.structure(params))\n",
        "print(jax.tree.leaves(params))"
      ],
      "metadata": {
        "id": "feznScrBztVA",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730499505169,
          "user_tz": 420,
          "elapsed": 870,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "78596000-538a-4cfe-cf74-353a0493b603"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTreeDef({'W': *, 'b': *, 'n': *})\n",
            "[Array([[1., 1.],\n",
            "       [1., 1.]], dtype=float32), Array([0., 0.], dtype=float32), 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Named tuple of parameters\n",
        "from typing import NamedTuple\n",
        "\n",
        "class Params(NamedTuple):\n",
        "  a: int\n",
        "  b: float\n",
        "\n",
        "params = Params(1, 5.0)\n",
        "print(jax.tree.structure(params))\n",
        "print(jax.tree.leaves(params))"
      ],
      "metadata": {
        "id": "34NYQ6jY0HCJ",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730499606363,
          "user_tz": 420,
          "elapsed": 53,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "c7e476f9-af3d-4863-b2e2-1430103ca485"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTreeDef(CustomNode(namedtuple[Params], [*, *]))\n",
            "[1, 5.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Just-in-time (jit) Compilation"
      ],
      "metadata": {
        "id": "2zD0yUCz09YR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The code above is sending one operation at a time to the accelerator.\n",
        "# This limits the ability of the XLA compiler to optimize our functions.\n",
        "def selu(x, alpha=1.67, lambda_=1.05):\n",
        "  return lambda_ * jnp.where(x > 0, x, alpha * jnp.exp(x) - alpha)\n",
        "\n",
        "x = jnp.arange(1000000)\n",
        "%timeit selu(x).block_until_ready()"
      ],
      "metadata": {
        "id": "MMll23Cs1mNI",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730503923444,
          "user_tz": 420,
          "elapsed": 3685,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "6d1507d5-b678-4aa3-cde6-596bef27c09b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The slowest run took 10135.66 times longer than the fastest. This could mean that an intermediate result is being cached \n",
            "1 loops, best of 5: 252 µs per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# compiled version of selu\n",
        "selu_jit = jax.jit(selu)\n",
        "\n",
        "# Pre-compile the function before timing...\n",
        "# if we don't have this line, timer will count the pre-compiling time too\n",
        "selu_jit(x).block_until_ready()\n",
        "\n",
        "%timeit selu_jit(x).block_until_ready()"
      ],
      "metadata": {
        "id": "WWygKsMXEj-P",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730504095612,
          "user_tz": 420,
          "elapsed": 4508,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "44f23195-a4b6-44be-e931-90ffe40ce463"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The slowest run took 4.01 times longer than the fastest. This could mean that an intermediate result is being cached \n",
            "10000 loops, best of 5: 71.7 µs per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# While loop conditioned on x and n.\n",
        "\n",
        "def g(x, n):\n",
        "  i = 0\n",
        "  while i < n:\n",
        "    i += 1\n",
        "  return x + i\n",
        "\n",
        "jax.jit(g)(10, 20)  # Raises an error\n",
        "# Traced values within JIT, like x and n here, can only affect control flow via\n",
        "# their static attributes: such as shape or dtype, and not via their values."
      ],
      "metadata": {
        "id": "M5f6SVX-GjCz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@jax.jit\n",
        "def f(x, i):\n",
        "  return x + i\n",
        "\n",
        "def g(x, n):\n",
        "  i = 0\n",
        "  while i < n:\n",
        "    i += 1\n",
        "  return f(x,i)\n",
        "\n",
        "print(g(2, 10))"
      ],
      "metadata": {
        "id": "q1Ozxx_HHYFn",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730504761298,
          "user_tz": 420,
          "elapsed": 338,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "8ce48110-49a5-4ea7-b147-eb5d0b9c83b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Solution 1: Need to split them into two steps\n",
        "# While loop conditioned on x and n with a jitted body.\n",
        "\n",
        "@jax.jit\n",
        "def loop_body(prev_i):\n",
        "  return prev_i + 1\n",
        "\n",
        "def g_inner_jitted(x, n):\n",
        "  i = 0\n",
        "  while i < n:\n",
        "    i = loop_body(i)\n",
        "  return x + i\n",
        "\n",
        "g_inner_jitted(10, 20)"
      ],
      "metadata": {
        "id": "yXe4MPdPGufO",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730504775918,
          "user_tz": 420,
          "elapsed": 2816,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "970de138-7cfd-4a87-c0b8-0d3ed49848a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array(30, dtype=int32, weak_type=True)"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Solution 2: Marking arguments as static\n",
        "# cons: JAX will have to re-compile the function for every new value of the specified static input\n",
        "\n",
        "def f(x):\n",
        "  if x > 0:\n",
        "    return x\n",
        "  else:\n",
        "    return 2 * x\n",
        "\n",
        "f_jit_correct = jax.jit(f, static_argnums=0)\n",
        "print(f_jit_correct(1))\n",
        "\n",
        "def g(x, n):\n",
        "  i = 0\n",
        "  while i < n:\n",
        "    i += 1\n",
        "  return x + i\n",
        "\n",
        "g_jit_correct = jax.jit(g, static_argnames=['n'])\n",
        "print(g_jit_correct(10, 20))\n",
        "\n",
        "from functools import partial\n",
        "\n",
        "@partial(jax.jit, static_argnames=['n'])\n",
        "def g_jit_decorated(x, n):\n",
        "  i = 0\n",
        "  while i < n:\n",
        "    i += 1\n",
        "  return x + i\n",
        "\n",
        "print(g_jit_decorated(10, 20))"
      ],
      "metadata": {
        "id": "3WPJwJF0IQfX",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730505008946,
          "user_tz": 420,
          "elapsed": 1336,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "a7b8a6e9-b66b-4340-aaba-15071abd94e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "30\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Caching\n",
        "\n",
        "from functools import partial\n",
        "\n",
        "def unjitted_loop_body(prev_i):\n",
        "  return prev_i + 1\n",
        "\n",
        "def g_inner_jitted_partial(x, n):\n",
        "  i = 0\n",
        "  while i < n:\n",
        "    # Don't do this! each time the partial returns\n",
        "    # a function with different hash\n",
        "    i = jax.jit(partial(unjitted_loop_body))(i)\n",
        "  return x + i\n",
        "\n",
        "def g_inner_jitted_lambda(x, n):\n",
        "  i = 0\n",
        "  while i < n:\n",
        "    # Don't do this!, lambda will also return\n",
        "    # a function with a different hash\n",
        "    i = jax.jit(lambda x: unjitted_loop_body(x))(i)\n",
        "  return x + i\n",
        "\n",
        "def g_inner_jitted_normal(x, n):\n",
        "  i = 0\n",
        "  while i < n:\n",
        "    # this is OK, since JAX can find the\n",
        "    # cached, compiled function\n",
        "    i = jax.jit(unjitted_loop_body)(i)\n",
        "  return x + i\n",
        "\n",
        "print(\"jit called in a loop with partials:\")\n",
        "%timeit g_inner_jitted_partial(10, 20).block_until_ready()\n",
        "\n",
        "print(\"jit called in a loop with lambdas:\")\n",
        "%timeit g_inner_jitted_lambda(10, 20).block_until_ready()\n",
        "\n",
        "print(\"jit called in a loop with caching:\")\n",
        "%timeit g_inner_jitted_normal(10, 20).block_until_ready()"
      ],
      "metadata": {
        "id": "NmrBNueAJuve",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730505469733,
          "user_tz": 420,
          "elapsed": 186459,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "5d51b0ca-dba4-4b85-8079-966b6847d0ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "jit called in a loop with partials:\n",
            "1 loops, best of 5: 14.9 s per loop\n",
            "jit called in a loop with lambdas:\n",
            "1 loops, best of 5: 10.5 s per loop\n",
            "jit called in a loop with caching:\n",
            "The slowest run took 63.21 times longer than the fastest. This could mean that an intermediate result is being cached \n",
            "1 loops, best of 5: 5.39 ms per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Automatic Vectorization"
      ],
      "metadata": {
        "id": "-K_Pvhsl0xUP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "x = jnp.arange(5)\n",
        "w = jnp.array([2., 3., 4.])\n",
        "\n",
        "def convolve(x, w):\n",
        "  output = []\n",
        "  for i in range(1, len(x)-1):\n",
        "    output.append(jnp.dot(x[i-1:i+2], w))\n",
        "  return jnp.array(output)\n",
        "\n",
        "convolve(x, w)\n",
        "\n",
        "xs = jnp.stack([x, x])\n",
        "ws = jnp.stack([w, w])\n",
        "\n",
        "# not efficient\n",
        "def manually_batched_convolve(xs, ws):\n",
        "  output = []\n",
        "  for i in range(xs.shape[0]):\n",
        "    output.append(convolve(xs[i], ws[i]))\n",
        "  return jnp.stack(output)\n",
        "\n",
        "manually_batched_convolve(xs, ws)\n",
        "\n",
        "# efficient\n",
        "auto_batch_convolve = jax.vmap(convolve)\n",
        "auto_batch_convolve(xs, ws)\n",
        "\n",
        "# If the batch dimension is not the first, you may use the in_axes and out_axes\n",
        "# arguments to specify the location of the batch dimension in inputs and\n",
        "# outputs. These may be an integer if the batch axis is the same for all inputs\n",
        "# and outputs, or lists, otherwise.\n",
        "auto_batch_convolve_v2 = jax.vmap(convolve, in_axes=1, out_axes=1)\n",
        "xst = jnp.transpose(xs)\n",
        "wst = jnp.transpose(ws)\n",
        "auto_batch_convolve_v2(xst, wst)\n",
        "\n",
        "# also supports the case where only one of the arguments is batched\n",
        "batch_convolve_v3 = jax.vmap(convolve, in_axes=[0, None])\n",
        "batch_convolve_v3(xs, w)\n",
        "\n",
        "# jax.jit() and jax.vmap() are designed to be composable\n",
        "jitted_batch_convolve = jax.jit(auto_batch_convolve)\n",
        "jitted_batch_convolve(xs, ws)"
      ],
      "metadata": {
        "id": "wE64PlxNNJif",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730506449772,
          "user_tz": 420,
          "elapsed": 6788,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "b930a4db-d1dd-4884-d7bd-2a1e795090aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([[11., 11.],\n",
              "       [20., 20.],\n",
              "       [29., 29.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Automatic Differentiation"
      ],
      "metadata": {
        "id": "qoXJ2iebO1w_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import grad, jit, vmap\n",
        "from jax import random\n",
        "\n",
        "key = random.key(0)"
      ],
      "metadata": {
        "id": "L_n-b8vx65RO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Taking gradinets with jax.grad\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "from jax import grad\n",
        "\n",
        "grad_tanh = grad(jnp.tanh)\n",
        "print(grad_tanh(2.0))\n",
        "\n",
        "print(grad(grad(jnp.tanh))(2.0))\n",
        "print(grad(grad(grad(jnp.tanh)))(2.0))\n",
        "\n",
        "f = lambda x: x**3 + 2*x**2 - 3*x + 1\n",
        "dfdx = jax.grad(f)\n",
        "d2fdx = jax.grad(dfdx)\n",
        "d3fdx = jax.grad(d2fdx)\n",
        "d4fdx = jax.grad(d3fdx)"
      ],
      "metadata": {
        "id": "hRXCVHMAO8OM",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730506967109,
          "user_tz": 420,
          "elapsed": 9818,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "89c91c11-b0cd-4578-97d3-218bb85c5a74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.07060704\n",
            "-0.13613746\n",
            "0.25251603\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Taking gradients part 2\n",
        "# f: R^n => R^m\n",
        "# jax.jacfwd is more effcient when n << m\n",
        "# jax.jacrev is more efficient when m << n\n",
        "# They give the same answer."
      ],
      "metadata": {
        "id": "IcHITPg26xbr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def hessian(f):\n",
        "  return jax.jacfwd(jax.grad(f))"
      ],
      "metadata": {
        "id": "e0DwbeYXJgui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def f(x):\n",
        "  return jnp.dot(x, x)\n",
        "\n",
        "hessian(f)(jnp.array([1., 2., 3.]))"
      ],
      "metadata": {
        "id": "W7ryQM7eL1KH",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730707163550,
          "user_tz": 480,
          "elapsed": 12209,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "6aa77eaa-de3f-4e37-c1f4-c27da7c03174"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([[2., 0., 0.],\n",
              "       [0., 2., 0.],\n",
              "       [0., 0., 2.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Some meta-learning techniques, such as Model-Agnostic Meta-Learning (MAML),\n",
        "# require differentiating through gradient updates. In other frameworks this can\n",
        "# be quite cumbersome, but in JAX it’s much easier\n",
        "# TODO: ?\n",
        "def meta_loss_fn(params, data):\n",
        "  \"\"\"Computes the loss after one step of SGD.\"\"\"\n",
        "  grads = jax.grad(loss_fn)(params, data)\n",
        "  return loss_fn(params - lr * grads, data)\n",
        "\n",
        "# meta_grads = jax.grad(meta_loss_fn)(params, data)"
      ],
      "metadata": {
        "id": "67F-K2mOMDZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Computing gradients in a linear regression\n",
        "# linear logistic regression model example\n",
        "key = jax.random.key(0)\n",
        "\n",
        "def sigmoid(x):\n",
        "  return 0.5 * (jnp.tanh(x / 2) + 1)\n",
        "\n",
        "# Outputs probability of a label being true.\n",
        "def predict(W, b, inputs):\n",
        "  return sigmoid(jnp.dot(inputs, W) + b)\n",
        "\n",
        "# Build a toy dataset.\n",
        "inputs = jnp.array([[0.52, 1.12,  0.77],\n",
        "                    [0.88, -1.08, 0.15],\n",
        "                    [0.52, 0.06, -1.30],\n",
        "                    [0.74, -2.49, 1.39]])\n",
        "targets = jnp.array([True, True, False, True])\n",
        "\n",
        "# Training loss is the negative log-likelihood of the training examples.\n",
        "def loss(W, b):\n",
        "  preds = predict(W, b, inputs)\n",
        "  label_probs = preds * targets + (1 - preds) * (1 - targets)\n",
        "  return -jnp.sum(jnp.log(label_probs))\n",
        "\n",
        "# Initialize random model coefficients\n",
        "key, W_key, b_key = jax.random.split(key, 3)\n",
        "W = jax.random.normal(W_key, (3,))\n",
        "b = jax.random.normal(b_key, ())\n",
        "\n",
        "# Differentiate `loss` with respect to the first positional argument:\n",
        "W_grad = grad(loss, argnums=0)(W, b)\n",
        "print(f'{W_grad=}')\n",
        "\n",
        "# Since argnums=0 is the default, this does the same thing:\n",
        "W_grad = grad(loss)(W, b)\n",
        "print(f'{W_grad=}')\n",
        "\n",
        "# But you can choose different values too, and drop the keyword:\n",
        "b_grad = grad(loss, 1)(W, b)\n",
        "print(f'{b_grad=}')\n",
        "\n",
        "# Including tuple values\n",
        "W_grad, b_grad = grad(loss, (0, 1))(W, b)\n",
        "print(f'{W_grad=}')\n",
        "print(f'{b_grad=}')"
      ],
      "metadata": {
        "id": "TM7HSYQ3RTCq",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730507799621,
          "user_tz": 420,
          "elapsed": 23994,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "572919a9-d592-4bd0-9801-55c8e065c7f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "W_grad=Array([-0.16966598, -0.8774322 , -1.4901515 ], dtype=float32)\n",
            "W_grad=Array([-0.16966598, -0.8774322 , -1.4901515 ], dtype=float32)\n",
            "b_grad=Array(-0.29228413, dtype=float32)\n",
            "W_grad=Array([-0.16966598, -0.8774322 , -1.4901515 ], dtype=float32)\n",
            "b_grad=Array(-0.29228413, dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Differentiating with respect to nested lists, tuples, and dicts\n",
        "\n",
        "def loss2(params_dict):\n",
        "    preds = predict(params_dict['W'], params_dict['b'], inputs)\n",
        "    label_probs = preds * targets + (1 - preds) * (1 - targets)\n",
        "    return -jnp.sum(jnp.log(label_probs))\n",
        "\n",
        "print(grad(loss2)({'W': W, 'b': b}))"
      ],
      "metadata": {
        "id": "MoLICLpRTk6E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Evaluating a function and its gradient using jax.value_and_grad\n",
        "\n",
        "loss_value, Wb_grad = jax.value_and_grad(loss, (0, 1))(W, b)\n",
        "print('loss value', loss_value)\n",
        "print('loss value', loss(W, b))"
      ],
      "metadata": {
        "id": "MyvrYXtfTsqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Checking against numberical differences (finite differences)\n",
        "\n",
        "# Set a step size for finite differences calculations\n",
        "eps = 1e-4\n",
        "\n",
        "# Check b_grad with scalar finite differences\n",
        "b_grad_numerical = (loss(W, b + eps / 2.) - loss(W, b - eps / 2.)) / eps\n",
        "print('b_grad_numerical', b_grad_numerical)\n",
        "print('b_grad_autodiff', grad(loss, 1)(W, b))\n",
        "\n",
        "# Check W_grad with finite differences in a random direction\n",
        "key, subkey = jax.random.split(key)\n",
        "vec = jax.random.normal(subkey, W.shape)\n",
        "unitvec = vec / jnp.sqrt(jnp.vdot(vec, vec))\n",
        "W_grad_numerical = (loss(W + eps / 2. * unitvec, b) - loss(W - eps / 2. * unitvec, b)) / eps\n",
        "print('W_dirderiv_numerical', W_grad_numerical)\n",
        "print('W_dirderiv_autodiff', jnp.vdot(grad(loss)(W, b), unitvec))\n",
        "\n",
        "\n",
        "# doing the same thing using test util\n",
        "\n",
        "from jax.test_util import check_grads\n",
        "# check_grads(loss, (W, b), order=2)  # check up to 2nd order derivatives"
      ],
      "metadata": {
        "id": "fWXllZ8OUEXj",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730517161663,
          "user_tz": 420,
          "elapsed": 53,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "2ffb0121-b294-434a-a619-6deafa1b6e07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b_grad_numerical -0.26226044\n",
            "b_grad_autodiff -0.29228413\n",
            "W_dirderiv_numerical 0.17166138\n",
            "W_dirderiv_autodiff 0.1800164\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Higher-order optimization"
      ],
      "metadata": {
        "id": "iLK51UWmCcsO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Stopping gradients\n",
        "\n",
        "# Value function and initial parameters\n",
        "# value = theta * state\n",
        "value_fn = lambda theta, state: jnp.dot(theta, state)\n",
        "theta = jnp.array([0.1, -0.1, 0.])\n",
        "\n",
        "# An example transition.\n",
        "s_tm1 = jnp.array([1., 2., -1.])\n",
        "r_t = jnp.array(1.)\n",
        "s_t = jnp.array([2., 1., 0.])\n",
        "\n",
        "# grad(theta) = (r_t + v_theta(s_t)) * grad(v_theta(s_(t-1)))\n",
        "# pseudo loss function\n",
        "# But the gradient computation will include the dependency of target on theta in this implementation.\n",
        "def td_loss(theta, s_tm1, r_t, s_t):\n",
        "  v_tm1 = value_fn(theta, s_tm1)\n",
        "  target = r_t + value_fn(theta, s_t)\n",
        "  return -0.5 * ((target - v_tm1) ** 2)\n",
        "\n",
        "td_update = jax.grad(td_loss)\n",
        "delta_theta = td_update(theta, s_tm1, r_t, s_t)\n",
        "\n",
        "delta_theta\n",
        "\n",
        "# Solution\n",
        "def td_loss(theta, s_tm1, r_t, s_t):\n",
        "  v_tm1 = value_fn(theta, s_tm1)\n",
        "  target = r_t + value_fn(theta, s_t)\n",
        "  #  force JAX to ignore the dependency of the target on theta\n",
        "  return -0.5 * ((jax.lax.stop_gradient(target) - v_tm1) ** 2)\n",
        "\n",
        "td_update = jax.grad(td_loss)\n",
        "delta_theta = td_update(theta, s_tm1, r_t, s_t)\n",
        "\n",
        "delta_theta\n",
        "\n",
        "s_grad = jax.grad(value_fn)(theta, s_tm1)\n",
        "delta_theta_original_calculation = (r_t + value_fn(theta, s_t) - value_fn(theta, s_tm1)) * s_grad\n",
        "\n",
        "delta_theta_original_calculation # [1.2, 2.4, -1.2], same as `delta_theta`"
      ],
      "metadata": {
        "id": "WyPwmYdeMpAj",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730735801099,
          "user_tz": 480,
          "elapsed": 19963,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "fcaf95bb-0fa0-4764-c713-e408d30d2215"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([ 1.2,  2.4, -1.2], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Straight-through estimator using stop_gradient\n",
        "def f(x):\n",
        "  return jnp.round(x)  # non-differentiable\n",
        "\n",
        "def straight_through_f(x):\n",
        "  # Create an exactly-zero expression with Sterbenz lemma that has\n",
        "  # an exactly-one gradient.\n",
        "  zero = x - jax.lax.stop_gradient(x)\n",
        "  return zero + jax.lax.stop_gradient(f(x))\n",
        "\n",
        "print(\"f(x): \", f(3.2))\n",
        "print(\"straight_through_f(x):\", straight_through_f(3.2))\n",
        "\n",
        "print(\"grad(f)(x):\", jax.grad(f)(3.2))\n",
        "print(\"grad(straight_through_f)(x):\", jax.grad(straight_through_f)(3.2))"
      ],
      "metadata": {
        "id": "4kRhjN5x5cMM",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730788165101,
          "user_tz": 480,
          "elapsed": 7342,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "1afec1e9-1f0e-427f-9be8-0d0003e6c939"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "f(x):  3.0\n",
            "straight_through_f(x): 3.0\n",
            "grad(f)(x): 0.0\n",
            "grad(straight_through_f)(x): 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Per-example gradients\n",
        "perex_grads = jax.jit(jax.vmap(jax.grad(td_loss), in_axes=(None, 0, 0, 0)))\n",
        "\n",
        "# Test it:\n",
        "batched_s_tm1 = jnp.stack([s_tm1, s_tm1])\n",
        "batched_r_t = jnp.stack([r_t, r_t])\n",
        "batched_s_t = jnp.stack([s_t, s_t])\n",
        "\n",
        "print(\"per-example grads: \", perex_grads(theta, batched_s_tm1, batched_r_t, batched_s_t))\n",
        "\n",
        "# Step by step\n",
        "# computes the gradient of the loss w.r.t. the parameters on single (unbatched) inputs:\n",
        "dtdloss_dtheta = jax.grad(td_loss)\n",
        "dtdloss_dtheta(theta, s_tm1, r_t, s_t)\n",
        "almost_perex_grads = jax.vmap(dtdloss_dtheta)\n",
        "batched_theta = jnp.stack([theta, theta])\n",
        "print(\"\\nalmost_perex_grads: \", almost_perex_grads(batched_theta, batched_s_tm1, batched_r_t, batched_s_t))\n",
        "# This makes the resulting function add an extra axis only to the other\n",
        "# arguments, leaving theta unbatched, as we want:\n",
        "inefficient_perex_grads = jax.vmap(dtdloss_dtheta, in_axes=(None, 0, 0, 0))\n",
        "print(\"\\ninefficient_perex_grads: \", inefficient_perex_grads(theta, batched_s_tm1, batched_r_t, batched_s_t))\n",
        "# jit to acclerate\n",
        "perex_grads = jax.jit(inefficient_perex_grads)\n",
        "print(\"\\nperex_grads: \", perex_grads(theta, batched_s_tm1, batched_r_t, batched_s_t))\n",
        "\n",
        "%timeit inefficient_perex_grads(theta, batched_s_tm1, batched_r_t, batched_s_t).block_until_ready()\n",
        "%timeit perex_grads(theta, batched_s_tm1, batched_r_t, batched_s_t).block_until_ready()"
      ],
      "metadata": {
        "id": "W13b05Lr6jnW",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730791214951,
          "user_tz": 480,
          "elapsed": 9003,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "4dc892a6-a24f-43ff-8deb-bda903650a91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "per-example grads:  [[ 1.2  2.4 -1.2]\n",
            " [ 1.2  2.4 -1.2]]\n",
            "\n",
            "almost_perex_grads:  [[ 1.2  2.4 -1.2]\n",
            " [ 1.2  2.4 -1.2]]\n",
            "\n",
            "inefficient_perex_grads:  [[ 1.2  2.4 -1.2]\n",
            " [ 1.2  2.4 -1.2]]\n",
            "\n",
            "perex_grads:  [[ 1.2  2.4 -1.2]\n",
            " [ 1.2  2.4 -1.2]]\n",
            "100 loops, best of 5: 4.68 ms per loop\n",
            "The slowest run took 7.97 times longer than the fastest. This could mean that an intermediate result is being cached \n",
            "10000 loops, best of 5: 63.3 µs per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Hessian-vector products with jax.grad-of-jax.grad\n",
        "# d2f/d2x * v\n",
        "def hvp(f, x, v):\n",
        "    return grad(lambda x: jnp.vdot(grad(f)(x), v))(x)"
      ],
      "metadata": {
        "id": "URQ3Qng_MpBI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Jacobians and Hessians using jax.jacfwd and jax.jacrev\n",
        "\n",
        "from jax import jacfwd, jacrev\n",
        "key = random.key(0)\n",
        "\n",
        "# Define a sigmoid function.\n",
        "def sigmoid(x):\n",
        "    return 0.5 * (jnp.tanh(x / 2) + 1)\n",
        "\n",
        "# Outputs probability of a label being true.\n",
        "def predict(W, b, inputs):\n",
        "    return sigmoid(jnp.dot(inputs, W) + b)\n",
        "\n",
        "# Build a toy dataset.\n",
        "inputs = jnp.array([[0.52, 1.12,  0.77],\n",
        "                   [0.88, -1.08, 0.15],\n",
        "                   [0.52, 0.06, -1.30],\n",
        "                   [0.74, -2.49, 1.39]])\n",
        "\n",
        "# Initialize random model coefficients\n",
        "key, W_key, b_key = random.split(key, 3)\n",
        "W = random.normal(W_key, (3,))\n",
        "b = random.normal(b_key, ())\n",
        "\n",
        "# Isolate the function from the weight matrix to the predictions\n",
        "f = lambda W: predict(W, b, inputs)\n",
        "\n",
        "# More efficient for tall Jacobian matrix\n",
        "J = jacfwd(f)(W)\n",
        "print(\"jacfwd result, with shape\", J.shape)\n",
        "print(J)\n",
        "\n",
        "# More efficient for wide jacobian matrix\n",
        "J = jacrev(f)(W)\n",
        "print(\"jacrev result, with shape\", J.shape)\n",
        "print(J)\n",
        "\n",
        "# You can also use jax.jacfwd() and jax.jacrev() with container types:\n",
        "def predict_dict(params, inputs):\n",
        "    return predict(params['W'], params['b'], inputs)\n",
        "\n",
        "J_dict = jacrev(predict_dict)({'W': W, 'b': b}, inputs)\n",
        "for k, v in J_dict.items():\n",
        "    print(\"Jacobian from {} to logits is\".format(k))\n",
        "    print(v)"
      ],
      "metadata": {
        "id": "9OJPcnyPMt3a",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730873552655,
          "user_tz": 480,
          "elapsed": 2988,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "b0c5024d-c8ee-478b-c488-c1937975ad16"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "jacfwd result, with shape (4, 3)\n",
            "[[ 0.05976408  0.12851527  0.08852274]\n",
            " [ 0.04011315 -0.04920547  0.00686381]\n",
            " [ 0.12177273  0.01407711 -0.30397403]\n",
            " [ 0.00140856 -0.00473993  0.00265317]]\n",
            "jacrev result, with shape (4, 3)\n",
            "[[ 0.05986786  0.1287384   0.08867645]\n",
            " [ 0.04012585 -0.04922104  0.00686598]\n",
            " [ 0.12176514  0.01407623 -0.30395508]\n",
            " [ 0.00140816 -0.00473857  0.00265241]]\n",
            "Jacobian from W to logits is\n",
            "[[ 0.05986786  0.1287384   0.08867645]\n",
            " [ 0.04012585 -0.04922104  0.00686598]\n",
            " [ 0.12176514  0.01407623 -0.30395508]\n",
            " [ 0.00140816 -0.00473857  0.00265241]]\n",
            "Jacobian from b to logits is\n",
            "[0.11503464 0.04563986 0.23438963 0.0019079 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def hessian(f):\n",
        "  # most efficient implementation\n",
        "  # f: Rn->Rm (n>>m), jacrev(f) more efficient\n",
        "  # jac(f): mxn\n",
        "  # jac(jac(f)): (mxn) x n  (n << mxn) jacfwd(f) more efficient\n",
        "    return jacfwd(jacrev(f))\n",
        "\n",
        "H = hessian(f)(W)\n",
        "print(\"hessian, with shape\", H.shape)\n",
        "print(H)"
      ],
      "metadata": {
        "id": "rIBXRn7rJNIH",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730874252474,
          "user_tz": 480,
          "elapsed": 16868,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "c6bd3e0f-5090-4a0b-fe5e-e45d527fc11d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hessian, with shape (4, 3, 3)\n",
            "[[[ 0.02283096  0.04895973  0.03373909]\n",
            "  [ 0.04909515  0.10528183  0.07255173]\n",
            "  [ 0.03381729  0.0725193   0.04997444]]\n",
            "\n",
            " [[-0.03197193  0.03905296 -0.00544488]\n",
            "  [ 0.0392189  -0.04790497  0.00667906]\n",
            "  [-0.00547075  0.0066824  -0.00093168]]\n",
            "\n",
            " [[-0.01579142 -0.00183123  0.03957367]\n",
            "  [-0.00182551 -0.00021169  0.00457478]\n",
            "  [ 0.03941917  0.0045712  -0.0987854 ]]\n",
            "\n",
            " [[-0.00103641  0.00349224 -0.00194889]\n",
            "  [ 0.00348759 -0.01175165  0.00655818]\n",
            "  [-0.00195217  0.00657797 -0.00367093]]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## How it’s made: Two foundational autodiff functions"
      ],
      "metadata": {
        "id": "tjkheN2lNn16"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Jacobian-Vector products (JVPs, a.k.a. forward-mode autodiff)\n",
        "\n",
        "from jax import jvp\n",
        "\n",
        "# Isolate the function from the weight matrix to the predictions\n",
        "f = lambda W: predict(W, b, inputs)\n",
        "\n",
        "key, subkey = random.split(key)\n",
        "v = random.normal(subkey, W.shape)\n",
        "\n",
        "# Push forward the vector `v` along `f` evaluated at `W`\n",
        "y, u = jvp(f, (W,), (v,))\n",
        "\n",
        "# the FLOP cost of the jvp-transformed function is about 3x the cost of just evaluating the function\n",
        "# build a column at a time"
      ],
      "metadata": {
        "id": "7KFyF990NUb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Vector-Jacobian products (VJPs, a.k.a. reverse-mode autodiff)\n",
        "# build one row at a time\n",
        "\n",
        "from jax import vjp\n",
        "\n",
        "# Isolate the function from the weight matrix to the predictions\n",
        "f = lambda W: predict(W, b, inputs)\n",
        "\n",
        "y, vjp_fun = vjp(f, W)\n",
        "\n",
        "key, subkey = random.split(key)\n",
        "u = random.normal(subkey, y.shape)\n",
        "\n",
        "# Pull back the covector `u` along `f` evaluated at `W`\n",
        "v = vjp_fun(u)\n",
        "\n"
      ],
      "metadata": {
        "id": "jvObZuvJSjqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Vector-valued gradients with VJPs\n",
        "def vgrad(f, x):\n",
        "  y, vjp_fn = vjp(f, x)\n",
        "  return vjp_fn(jnp.ones(y.shape))[0]\n",
        "\n",
        "print(vgrad(lambda x: 3*x**2, jnp.ones((2, 2))))"
      ],
      "metadata": {
        "id": "V7XrJ0VGT562"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Hessian-vector products using both forward- and reverse-mode\n",
        "\n",
        "def hvp(f, x, v):\n",
        "    return grad(lambda x: jnp.vdot(grad(f)(x), v))(x)\n",
        "\n",
        "\n",
        "# forward-over-reverse\n",
        "def hvp(f, primals, tangents):\n",
        "  return jvp(grad(f), primals, tangents)[1]\n",
        "\n",
        "def f(X):\n",
        "  return jnp.sum(jnp.tanh(X)**2)\n",
        "\n",
        "key, subkey1, subkey2 = random.split(key, 3)\n",
        "X = random.normal(subkey1, (30, 40))\n",
        "V = random.normal(subkey2, (30, 40))\n",
        "\n",
        "ans1 = hvp(f, (X,), (V,))\n",
        "ans2 = jnp.tensordot(hessian(f)(X), V, 2)\n",
        "\n",
        "print(jnp.allclose(ans1, ans2, 1e-4, 1e-4))\n",
        "\n",
        "# Reverse-over-forward\n",
        "def hvp_revfwd(f, primals, tangents):\n",
        "  g = lambda primals: jvp(f, primals, tangents)[1]\n",
        "  return grad(g)(primals)\n",
        "\n",
        "# Reverse-over-reverse, only works for single arguments\n",
        "def hvp_revrev(f, primals, tangents):\n",
        "  x, = primals\n",
        "  v, = tangents\n",
        "  return grad(lambda x: jnp.vdot(grad(f)(x), v))(x)\n",
        "\n",
        "\n",
        "print(\"Forward over reverse\")\n",
        "%timeit -n10 -r3 hvp(f, (X,), (V,))\n",
        "print(\"Reverse over forward\")\n",
        "%timeit -n10 -r3 hvp_revfwd(f, (X,), (V,))\n",
        "print(\"Reverse over reverse\")\n",
        "%timeit -n10 -r3 hvp_revrev(f, (X,), (V,))\n",
        "\n",
        "print(\"Naive full Hessian materialization\")\n",
        "%timeit -n10 -r3 jnp.tensordot(hessian(f)(X), V, 2)"
      ],
      "metadata": {
        "id": "bFMDHGZ_YYu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Composing VJPs, JVPs, and jax.vmap"
      ],
      "metadata": {
        "id": "umk_TLj1YnQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Jacobian-Matrix and Matrix-Jacobian products\n",
        "# Isolate the function from the weight matrix to the predictions\n",
        "f = lambda W: predict(W, b, inputs)\n",
        "\n",
        "# Pull back the covectors `m_i` along `f`, evaluated at `W`, for all `i`.\n",
        "# First, use a list comprehension to loop over rows in the matrix M.\n",
        "def loop_mjp(f, x, M):\n",
        "    y, vjp_fun = vjp(f, x)\n",
        "    return jnp.vstack([vjp_fun(mi) for mi in M])\n",
        "\n",
        "# Now, use vmap to build a computation that does a single fast matrix-matrix\n",
        "# multiply, rather than an outer loop over vector-matrix multiplies.\n",
        "def vmap_mjp(f, x, M):\n",
        "    y, vjp_fun = vjp(f, x)\n",
        "    outs, = vmap(vjp_fun)(M)\n",
        "    return outs\n",
        "\n",
        "key = random.key(0)\n",
        "num_covecs = 128\n",
        "U = random.normal(key, (num_covecs,) + y.shape)\n",
        "\n",
        "loop_vs = loop_mjp(f, W, M=U)\n",
        "print('Non-vmapped Matrix-Jacobian product')\n",
        "%timeit -n10 -r3 loop_mjp(f, W, M=U)\n",
        "\n",
        "print('\\nVmapped Matrix-Jacobian product')\n",
        "vmap_vs = vmap_mjp(f, W, M=U)\n",
        "%timeit -n10 -r3 vmap_mjp(f, W, M=U)\n",
        "\n",
        "# assert jnp.allclose(loop_vs, vmap_vs), 'Vmap and non-vmapped Matrix-Jacobian Products should be identical'\n",
        "\n",
        "def loop_jmp(f, W, M):\n",
        "    # jvp immediately returns the primal and tangent values as a tuple,\n",
        "    # so we'll compute and select the tangents in a list comprehension\n",
        "    return jnp.vstack([jvp(f, (W,), (mi,))[1] for mi in M])\n",
        "\n",
        "def vmap_jmp(f, W, M):\n",
        "    _jvp = lambda s: jvp(f, (W,), (s,))[1]\n",
        "    return vmap(_jvp)(M)\n",
        "\n",
        "num_vecs = 128\n",
        "S = random.normal(key, (num_vecs,) + W.shape)\n",
        "\n",
        "loop_vs = loop_jmp(f, W, M=S)\n",
        "print('Non-vmapped Jacobian-Matrix product')\n",
        "%timeit -n10 -r3 loop_jmp(f, W, M=S)\n",
        "vmap_vs = vmap_jmp(f, W, M=S)\n",
        "print('\\nVmapped Jacobian-Matrix product')\n",
        "%timeit -n10 -r3 vmap_jmp(f, W, M=S)\n",
        "\n",
        "# assert jnp.allclose(loop_vs, vmap_vs), 'Vmap and non-vmapped Jacobian-Matrix products should be identical'"
      ],
      "metadata": {
        "id": "hDm3zh05jy_o",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730908853459,
          "user_tz": 480,
          "elapsed": 25191,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "2ff661f2-2456-45b0-d8a5-dcf2372e4239"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-15-89e5359873b9>:8: DeprecationWarning: vstack requires ndarray or scalar arguments, got <class 'tuple'> at position 0. In a future JAX release this will be an error.\n",
            "  return jnp.vstack([vjp_fun(mi) for mi in M])\n",
            "<ipython-input-15-89e5359873b9>:8: DeprecationWarning: vstack requires ndarray or scalar arguments, got <class 'tuple'> at position 0. In a future JAX release this will be an error.\n",
            "  return jnp.vstack([vjp_fun(mi) for mi in M])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Non-vmapped Matrix-Jacobian product\n",
            "10 loops, best of 3: 147 ms per loop\n",
            "\n",
            "Vmapped Matrix-Jacobian product\n",
            "10 loops, best of 3: 4.06 ms per loop\n",
            "Non-vmapped Jacobian-Matrix product\n",
            "10 loops, best of 3: 186 ms per loop\n",
            "\n",
            "Vmapped Jacobian-Matrix product\n",
            "10 loops, best of 3: 1.95 ms per loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title The implementation of jax.jacfwd and jax.jacrev\n",
        "# TODO: tuidao\n",
        "from jax import jacrev as builtin_jacrev\n",
        "\n",
        "def our_jacrev(f):\n",
        "    def jacfun(x):\n",
        "        y, vjp_fun = vjp(f, x)\n",
        "        # Use vmap to do a matrix-Jacobian product.\n",
        "        # Here, the matrix is the Euclidean basis, so we get all\n",
        "        # entries in the Jacobian at once.\n",
        "        J, = vmap(vjp_fun, in_axes=0)(jnp.eye(len(y)))\n",
        "        return J\n",
        "    return jacfun\n",
        "\n",
        "print(builtin_jacrev(f)(W))\n",
        "print(our_jacrev(f)(W))\n",
        "assert jnp.allclose(builtin_jacrev(f)(W), our_jacrev(f)(W)), 'Incorrect reverse-mode Jacobian results!'\n",
        "\n",
        "from jax import jacfwd as builtin_jacfwd\n",
        "\n",
        "def our_jacfwd(f):\n",
        "    def jacfun(x):\n",
        "        _jvp = lambda s: jvp(f, (x,), (s,))[1]\n",
        "        Jt = vmap(_jvp, in_axes=1)(jnp.eye(len(x)))\n",
        "        return jnp.transpose(Jt)\n",
        "    return jacfun\n",
        "\n",
        "print(builtin_jacfwd(f)(W))\n",
        "print(our_jacfwd(f)(W))\n",
        "\n",
        "assert jnp.allclose(builtin_jacfwd(f)(W), our_jacfwd(f)(W)), 'Incorrect forward-mode Jacobian results!'"
      ],
      "metadata": {
        "id": "Z1VGjHZ_N5b-",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730909291230,
          "user_tz": 480,
          "elapsed": 179,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "bc4c45c9-2471-47c7-fabc-613ad5949f06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.05986786  0.1287384   0.08867645]\n",
            " [ 0.04012585 -0.04922104  0.00686598]\n",
            " [ 0.12176514  0.01407623 -0.30395508]\n",
            " [ 0.00140816 -0.00473857  0.00265241]]\n",
            "[[ 0.05986786  0.1287384   0.08867645]\n",
            " [ 0.04012585 -0.04922104  0.00686598]\n",
            " [ 0.12176514  0.01407623 -0.30395508]\n",
            " [ 0.00140816 -0.00473857  0.00265241]]\n",
            "[[ 0.05976408  0.12851527  0.08852274]\n",
            " [ 0.04011315 -0.04920547  0.00686381]\n",
            " [ 0.12177273  0.01407711 -0.30397403]\n",
            " [ 0.00140856 -0.00473993  0.00265317]]\n",
            "[[ 0.05976408  0.12851527  0.08852274]\n",
            " [ 0.04011315 -0.04920547  0.00686381]\n",
            " [ 0.12177273  0.01407711 -0.30397403]\n",
            " [ 0.00140856 -0.00473993  0.00265317]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def f(x):\n",
        "    try:\n",
        "        if x < 3:\n",
        "            return 2 * x ** 3\n",
        "        else:\n",
        "            raise ValueError\n",
        "    except ValueError:\n",
        "        return jnp.pi * x\n",
        "\n",
        "y, f_vjp = vjp(f, 4.)\n",
        "print(jit(f_vjp)(1.))"
      ],
      "metadata": {
        "id": "ccVjY3EEQv8R",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730909843160,
          "user_tz": 480,
          "elapsed": 3208,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "6340dd57-1d47-4001-b7d5-eedf5237d711"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(Array(3.1415927, dtype=float32, weak_type=True),)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Complex numbers and differentiation\n",
        "\n",
        "TODO"
      ],
      "metadata": {
        "id": "ZfHqAhrrgw-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Custom derivative rules for JAX-transformable Python functions"
      ],
      "metadata": {
        "id": "bAA-gNC5g583"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "from jax import custom_jvp\n",
        "\n",
        "@custom_jvp\n",
        "def f(x, y):\n",
        "  return jnp.sin(x) * y\n",
        "\n",
        "@f.defjvp\n",
        "def f_jvp(primals, tangents):\n",
        "  x, y = primals\n",
        "  x_dot, y_dot = tangents\n",
        "  primal_out = f(x, y)\n",
        "  tangent_out = jnp.cos(x) * x_dot * y + jnp.sin(x) * y_dot\n",
        "  return primal_out, tangent_out\n",
        "\n",
        "print(f(2., 3.))\n",
        "y, y_dot = jvp(f, (2., 3.), (1., 0.))\n",
        "print(y)\n",
        "print(y_dot)\n",
        "print(grad(f)(2., 3.))\n",
        "\n",
        "# Equivalent alternative using the `defjvps` convenience wrapper\n",
        "\n",
        "@custom_jvp\n",
        "def f(x, y):\n",
        "  return jnp.sin(x) * y\n",
        "\n",
        "f.defjvps(lambda x_dot, primal_out, x, y: jnp.cos(x) * x_dot * y,\n",
        "          lambda y_dot, primal_out, x, y: jnp.sin(x) * y_dot)"
      ],
      "metadata": {
        "id": "ovSF-ZiJgn4r",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1731031711944,
          "user_tz": 480,
          "elapsed": 9025,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "64e7773d-3181-45fa-a2fe-c8f90bb1c3c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.7278922\n",
            "2.7278922\n",
            "-1.2484405\n",
            "-1.2484405\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from jax import custom_vjp\n",
        "\n",
        "@custom_vjp\n",
        "def f(x, y):\n",
        "  return jnp.sin(x) * y\n",
        "\n",
        "def f_fwd(x, y):\n",
        "# Returns primal output and residuals to be used in backward pass by `f_bwd`.\n",
        "  return f(x, y), (jnp.cos(x), jnp.sin(x), y)\n",
        "\n",
        "def f_bwd(res, g):\n",
        "  cos_x, sin_x, y = res # Gets residuals computed in `f_fwd`\n",
        "  return (cos_x * g * y, sin_x * g)\n",
        "\n",
        "f.defvjp(f_fwd, f_bwd)\n",
        "\n",
        "print(grad(f)(2., 3.))"
      ],
      "metadata": {
        "id": "f1C_GUe7iYpU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Example problems\n",
        "https://jax.readthedocs.io/en/latest/advanced-autodiff.html#example-problems"
      ],
      "metadata": {
        "id": "tfDsUgO6iqwA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basic usage of jax.custom_jvp and jax.custom_vjp APIs\n",
        "\n",
        "https://jax.readthedocs.io/en/latest/advanced-autodiff.html#basic-usage-of-jax-custom-jvp-and-jax-custom-vjp-apis"
      ],
      "metadata": {
        "id": "inlZv9GJwCp5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## More features and details\n",
        "\n",
        "https://jax.readthedocs.io/en/latest/advanced-autodiff.html#more-features-and-details"
      ],
      "metadata": {
        "id": "e3NbUawgU3Fy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduciton To Debugging"
      ],
      "metadata": {
        "id": "dv2rTt97UsCB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## jax.debug.print"
      ],
      "metadata": {
        "id": "ovhykxXdGWlT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def f(x):\n",
        "  y = jnp.sin(x)\n",
        "  # Use Python print() for static values, such as dtypes and array shapes.\n",
        "  # when transforming a function with jax.jit(), the Python code is executed\n",
        "  # with abstract tracers in place of your arrays. Because of this, the Python\n",
        "  # print() function will only print tracer value.\n",
        "  print(\"print(x) ->\", x)\n",
        "  print(\"print(y) ->\", y)\n",
        "  # Use jax.debug.print() for traced (dynamic) array values with jax.jit(), jax.vmap() and others.\n",
        "  jax.debug.print(\"jax.debug.print(x) -> {x}\", x=x)\n",
        "  jax.debug.print(\"jax.debug.print(y) -> {y}\", y=y)\n",
        "  return y\n",
        "\n",
        "print(\"non jit\")\n",
        "result = f(2.)\n",
        "print(\"\\njit\")\n",
        "result = jax.jit(f)(3.)\n",
        "\n",
        "# the same for jax.vmap\n",
        "\n",
        "def f(x):\n",
        "  jax.debug.print(\"jax.debug.print(x) -> {}\", x)\n",
        "  y = jnp.sin(x)\n",
        "  jax.debug.print(\"jax.debug.print(y) -> {}\", y)\n",
        "  return y\n",
        "\n",
        "print(\"\\nvmap\\n\")\n",
        "xs = jnp.arange(3.)\n",
        "result = jax.vmap(f)(xs)\n",
        "\n",
        "print(\"\\nsequential map\")\n",
        "# a sequential map rather than a vectorization\n",
        "xs = jnp.arange(3.)\n",
        "result = jax.lax.map(f, xs)"
      ],
      "metadata": {
        "id": "FDNqmgDT3_bt",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730520957057,
          "user_tz": 420,
          "elapsed": 1926,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "c7eedbce-f337-486d-c1a9-c13b3b4786bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "non jit\n",
            "print(x) -> 2.0\n",
            "print(y) -> 0.9092974\n",
            "jax.debug.print(x) -> 2.0\n",
            "jax.debug.print(y) -> 0.9092974066734314\n",
            "\n",
            "jit\n",
            "print(x) -> Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace>\n",
            "print(y) -> Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace>\n",
            "\n",
            "vmap\n",
            "\n",
            "jax.debug.print(x) -> 3.0\n",
            "jax.debug.print(y) -> 0.1411200314760208\n",
            "jax.debug.print(x) -> 0.0\n",
            "jax.debug.print(x) -> 1.0\n",
            "jax.debug.print(x) -> 2.0\n",
            "jax.debug.print(y) -> 0.0\n",
            "jax.debug.print(y) -> 0.8414709568023682\n",
            "jax.debug.print(y) -> 0.9092974066734314\n",
            "\n",
            "sequential map\n",
            "jax.debug.print(x) -> 0.0\n",
            "jax.debug.print(y) -> 0.0\n",
            "jax.debug.print(x) -> 1.0\n",
            "jax.debug.print(y) -> 0.8414709568023682\n",
            "jax.debug.print(x) -> 2.0\n",
            "jax.debug.print(y) -> 0.9092974066734314\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def f(x):\n",
        "  print(\"print(x) ->\", x)\n",
        "  jax.debug.print(\"jax.debug.print(x) -> {}\", x)\n",
        "  return x ** 2\n",
        "\n",
        "# only prints the forward pass\n",
        "print(\"\\ngrad\")\n",
        "result = jax.grad(f)(1.)\n",
        "print(\"\\njit grad\")\n",
        "result = jax.jit(jax.grad(f))(1.)"
      ],
      "metadata": {
        "id": "pe6Xas4t77YT",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730521019742,
          "user_tz": 420,
          "elapsed": 1070,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "83f76418-2fff-498e-da97-294c9eb55474"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "grad\n",
            "print(x) -> Traced<ShapedArray(float32[], weak_type=True)>with<JVPTrace> with\n",
            "  primal = 1.0\n",
            "  tangent = Traced<ShapedArray(float32[], weak_type=True)>with<JaxprTrace> with\n",
            "    pval = (ShapedArray(float32[], weak_type=True), None)\n",
            "    recipe = LambdaBinding()\n",
            "jax.debug.print(x) -> 1.0\n",
            "\n",
            "jit grad\n",
            "print(x) -> Traced<ShapedArray(float32[], weak_type=True)>with<JVPTrace> with\n",
            "  primal = Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace>\n",
            "  tangent = Traced<ShapedArray(float32[], weak_type=True)>with<JaxprTrace> with\n",
            "    pval = (ShapedArray(float32[], weak_type=True), None)\n",
            "    recipe = LambdaBinding()\n",
            "jax.debug.print(x) -> 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Sometimes, when the arguments don’t depend on one another, calls to\n",
        "# jax.debug.print() may print them in a different order when staged out with a\n",
        "# JAX transformation. If you need the original order, such as x: ... first and\n",
        "# then y: ... second, add the ordered=True parameter.\n",
        "@jax.jit\n",
        "def f(x, y):\n",
        "  jax.debug.print(\"jax.debug.print(x) -> {}\", x, ordered=True)\n",
        "  jax.debug.print(\"jax.debug.print(y) -> {}\", y, ordered=True)\n",
        "  return x + y\n",
        "\n",
        "f(1, 2)"
      ],
      "metadata": {
        "id": "PeCu7DuXGIIC",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730521113895,
          "user_tz": 420,
          "elapsed": 1068,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "5d9638d3-5d9e-4c65-fb26-b43b1e2cb088"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "jax.debug.print(x) -> 1\n",
            "jax.debug.print(y) -> 2\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array(3, dtype=int32, weak_type=True)"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## jax.debug.breakpoint\n",
        "- Use jax.debug.breakpoint() to pause the execution of your JAX program to inspect values.\n"
      ],
      "metadata": {
        "id": "Tvw9afZPGZ2r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@jax.jit\n",
        "def f(x):\n",
        "  y, z = jnp.sin(x), jnp.cos(x)\n",
        "  jax.debug.breakpoint()\n",
        "  return y * z\n",
        "f(2.) # ==> Pauses during execution"
      ],
      "metadata": {
        "id": "FpyNzcWfGiD6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def breakpoint_if_nonfinite(x):\n",
        "  is_finite = jnp.isfinite(x).all()\n",
        "  def true_fn(x):\n",
        "    pass\n",
        "  def false_fn(x):\n",
        "    jax.debug.breakpoint()\n",
        "  jax.lax.cond(is_finite, true_fn, false_fn, x)\n",
        "\n",
        "@jax.jit\n",
        "def f(x, y):\n",
        "  z = x / y\n",
        "  breakpoint_if_nonfinite(z)\n",
        "  return z\n",
        "\n",
        "f(2., 1.) # ==> No breakpoint\n",
        "f(2., 0.) # ==> Pauses during execution"
      ],
      "metadata": {
        "id": "knmD_xkIIMve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## jax.debug.callback"
      ],
      "metadata": {
        "id": "Y6gZKg3iInpE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: how to use this?\n",
        "\n",
        "import logging\n",
        "\n",
        "def log_value(x):\n",
        "  logging.warning(f'Logged value: {x}')\n",
        "\n",
        "@jax.jit\n",
        "def f(x):\n",
        "  jax.debug.callback(log_value, x)\n",
        "  return x\n",
        "\n",
        "f(1.0)\n",
        "\n"
      ],
      "metadata": {
        "id": "laWmg5waImzH",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730521937555,
          "user_tz": 420,
          "elapsed": 1054,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "86bfae0c-85e9-4bca-dcc1-f55208572948"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array(1., dtype=float32, weak_type=True)"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pseudorandom Numbers"
      ],
      "metadata": {
        "id": "Efx1SbPFJlD-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NumPy\n",
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "print(np.random.uniform())\n",
        "print(np.random.uniform(size=3))\n",
        "\n",
        "\n",
        "# NumPy provides a sequential equivalent guarantee, meaning that sampling N\n",
        "# numbers in a row individually or sampling a vector of N numbers results in the\n",
        "# same pseudo-random sequences.\n",
        "np.random.seed(0)\n",
        "print(\"individually:\", np.stack([np.random.uniform() for _ in range(3)]))\n",
        "np.random.seed(0)\n",
        "print(\"all at once: \", np.random.uniform(size=3))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def bar(): return np.random.uniform()\n",
        "def baz(): return np.random.uniform()\n",
        "\n",
        "def foo(): return bar() + 2 * baz()\n",
        "\n",
        "print(\"generate by order\")\n",
        "print(foo())"
      ],
      "metadata": {
        "id": "jYXRr-cHJuRB",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730525831817,
          "user_tz": 420,
          "elapsed": 53,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "888de2be-4cd9-4db4-a768-52d527e46c20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.5488135039273248\n",
            "[0.71518937 0.60276338 0.54488318]\n",
            "individually: [0.5488135  0.71518937 0.60276338]\n",
            "all at once:  [0.5488135  0.71518937 0.60276338]\n",
            "generate by order\n",
            "1.3921927816747064\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Jax - Explicit random state\n",
        "\n",
        "from jax import random\n",
        "\n",
        "key = random.key(42)\n",
        "print(\"key:\")\n",
        "print(key)\n",
        "\n",
        "# feeding the same key object to a random function will always result in the\n",
        "# same sample being generated.\n",
        "# Re-using the same key, even with different random APIs, can result in\n",
        "# correlated outputs, which is generally undesirable.\n",
        "\n",
        "print(\"\\nsame key\")\n",
        "print(random.normal(key))\n",
        "print(random.normal(key))\n",
        "\n",
        "print(\"\\nnp same seed\")\n",
        "np.random.seed(0)\n",
        "print(np.random.uniform())\n",
        "print(np.random.uniform())\n",
        "np.random.seed(0)\n",
        "print(np.random.uniform())\n",
        "print(np.random.uniform())\n",
        "\n",
        "# The rule of thumb is: never reuse keys (unless you want identical outputs).\n",
        "print(\"\\n\")\n",
        "for i in range(3):\n",
        "  # a deterministic function that converts one key into several independent\n",
        "   # (in the pseudorandomness sense) keys\n",
        "  new_key, subkey = random.split(key)\n",
        "  del key  # The old key is consumed by split() -- we must never use it again.\n",
        "\n",
        "  val = random.normal(subkey)\n",
        "  del subkey  # The subkey is consumed by normal().\n",
        "\n",
        "  print(f\"draw {i}: {val}\")\n",
        "  key = new_key  # new_key is safe to use in the next iteration.\n",
        "\n",
        "# can create as many keys as you need\n",
        "key, *forty_two_subkeys = random.split(key, num=43)\n",
        "\n",
        "# JAX does not provide a sequential equivalence guarantee\n",
        "key = random.key(42)\n",
        "subkeys = random.split(key, 3)\n",
        "sequence = np.stack([random.normal(subkey) for subkey in subkeys])\n",
        "print(\"\\nindividually:\", sequence)\n",
        "\n",
        "key = random.key(42)\n",
        "print(\"all at once: \", random.normal(key, shape=(3,)))\n",
        "\n",
        "# The lack of sequential equivalence gives us freedom to write code more\n",
        "# efficiently; for example, instead of generating sequence above via a\n",
        "# sequential loop, we can use jax.vmap() to compute the same result in a\n",
        "# vectorized manner.\n",
        "print(\"vectorized:\", jax.vmap(random.normal)(subkeys))"
      ],
      "metadata": {
        "id": "KbuZjcbXYP6P",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730527125596,
          "user_tz": 420,
          "elapsed": 1031,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "ddf6fc52-5d3a-4b00-e44a-6e979d38b6d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "key:\n",
            "Array((), dtype=key<fry>) overlaying:\n",
            "[ 0 42]\n",
            "\n",
            "same key\n",
            "-0.18471184\n",
            "-0.18471184\n",
            "\n",
            "np same seed\n",
            "0.5488135039273248\n",
            "0.7151893663724195\n",
            "0.5488135039273248\n",
            "0.7151893663724195\n",
            "\n",
            "\n",
            "draw 0: 1.3694629669189453\n",
            "draw 1: -0.19947032630443573\n",
            "draw 2: -2.298279285430908\n",
            "\n",
            "individually: [-0.04838832  0.10796152 -1.2226636 ]\n",
            "all at once:  [ 0.1869356 -1.2806349 -1.5593112]\n",
            "vectorized: [-0.04838832  0.10796152 -1.2226636 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Working With Pytrees"
      ],
      "metadata": {
        "id": "gjiLWP4BdhT6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title What is pytree?\n",
        "# Classes are considered container-like if they are in the pytree registry,\n",
        "# which by default includes lists, tuples, and dicts. Any object whose type is\n",
        "# not in the pytree container registry will be treated as a leaf node in the\n",
        "# tree.\n",
        "example_trees = [\n",
        "    [1, 'a', object()],\n",
        "    (1, (2, 3), ()),\n",
        "    [1, {'k1': 2, 'k2': (3, 4)}, 5],\n",
        "    {'a': 2, 'b': (2, 3)},\n",
        "    jnp.array([1, 2, 3]),\n",
        "]\n",
        "\n",
        "# Print how many leaves the pytrees have.\n",
        "for pytree in example_trees:\n",
        "  # This `jax.tree.leaves()` method extracts the flattened leaves from the pytrees.\n",
        "  leaves = jax.tree.leaves(pytree)\n",
        "  print(f\"{repr(pytree):<45} has {len(leaves)} leaves: {leaves}\")"
      ],
      "metadata": {
        "id": "FCD6FYT8esy2",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730605150297,
          "user_tz": 420,
          "elapsed": 1082,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "594fb154-cc10-4147-830e-944dd4b9b1d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1, 'a', <object object at 0x13f18cf94870>]   has 3 leaves: [1, 'a', <object object at 0x13f18cf94870>]\n",
            "(1, (2, 3), ())                               has 3 leaves: [1, 2, 3]\n",
            "[1, {'k1': 2, 'k2': (3, 4)}, 5]               has 5 leaves: [1, 2, 3, 4, 5]\n",
            "{'a': 2, 'b': (2, 3)}                         has 3 leaves: [2, 2, 3]\n",
            "Array([1, 2, 3], dtype=int32)                 has 1 leaves: [Array([1, 2, 3], dtype=int32)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Comman pytree functions\n",
        "\n",
        "list_of_lists = [\n",
        "    [1, 2, 3],\n",
        "    [1, 2],\n",
        "    [1, 2, 3, 4]\n",
        "]\n",
        "\n",
        "print(\"jax.tree.map(lambda x: x*2, list_of_lists)\")\n",
        "print(jax.tree.map(lambda x: x*2, list_of_lists))\n",
        "\n",
        "print(\"\\ntwo trees\")\n",
        "another_list_of_lists = list_of_lists\n",
        "print(jax.tree.map(lambda x, y: x+y, list_of_lists, another_list_of_lists))"
      ],
      "metadata": {
        "id": "TApqXz7gjs1K",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730605297508,
          "user_tz": 420,
          "elapsed": 52,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "a8187133-0b6c-49a4-c9a9-5c692d8c2031"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "jax.tree.map(lambda x: x*2, list_of_lists)\n",
            "[[2, 4, 6], [2, 4], [2, 4, 6, 8]]\n",
            "\n",
            "two trees\n",
            "[[2, 4, 6], [2, 4], [2, 4, 6, 8]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML examples"
      ],
      "metadata": {
        "id": "MQS5OXnklibH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def init_mlp_params(layer_widths):\n",
        "  params = []\n",
        "  for n_in, n_out in zip(layer_widths[:-1], layer_widths[1:]):\n",
        "    params.append(\n",
        "        dict(weights=np.random.normal(size=(n_in, n_out)) * np.sqrt(2/n_in),\n",
        "             biases=np.ones(shape=(n_out,))\n",
        "            )\n",
        "    )\n",
        "  return params\n",
        "\n",
        "params = init_mlp_params([1, 128, 128, 1])\n",
        "\n",
        "jax.tree.map(lambda x: x.shape, params)"
      ],
      "metadata": {
        "id": "Yylw15nrkU3v",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730605309185,
          "user_tz": 420,
          "elapsed": 53,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "1c19caab-8484-4341-b4ae-006a4664ad55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'biases': (128,), 'weights': (1, 128)},\n",
              " {'biases': (128,), 'weights': (128, 128)},\n",
              " {'biases': (1,), 'weights': (128, 1)}]"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the forward pass.\n",
        "def forward(params, x):\n",
        "  *hidden, last = params\n",
        "  for layer in hidden:\n",
        "    x = jax.nn.relu(x @ layer['weights'] + layer['biases'])\n",
        "  return x @ last['weights'] + last['biases']\n",
        "\n",
        "# Define the loss function.\n",
        "def loss_fn(params, x, y):\n",
        "  return jnp.mean((forward(params, x) - y) ** 2)\n",
        "\n",
        "# Set the learning rate.\n",
        "LEARNING_RATE = 0.0001\n",
        "\n",
        "# Using the stochastic gradient descent, define the parameter update function.\n",
        "# Apply `@jax.jit` for JIT compilation (speed).\n",
        "@jax.jit\n",
        "def update(params, x, y):\n",
        "  # Calculate the gradients with `jax.grad`.\n",
        "  grads = jax.grad(loss_fn)(params, x, y)\n",
        "  # Note that `grads` is a pytree with the same structure as `params`.\n",
        "  # `jax.grad` is one of many JAX functions that has\n",
        "  # built-in support for pytrees.\n",
        "  # This is useful - you can apply the SGD update using JAX pytree utilities.\n",
        "  return jax.tree.map(\n",
        "      lambda p, g: p - LEARNING_RATE * g, params, grads\n",
        "  )"
      ],
      "metadata": {
        "id": "8XENqtSVll2p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Custom pytree nodes"
      ],
      "metadata": {
        "id": "GVLbN2k_mK2D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Special(object):\n",
        "  def __init__(self, x, y):\n",
        "    self.x = x\n",
        "    self.y = y\n",
        "\n",
        "# treat Special as leaf\n",
        "jax.tree.leaves([\n",
        "    Special(0, 1),\n",
        "    Special(2, 4),\n",
        "])"
      ],
      "metadata": {
        "id": "izyhFd26mL_J",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730590998103,
          "user_tz": 420,
          "elapsed": 166,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "2566169f-1dcd-46e8-b3d1-3a79d8634c07"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<__main__.Special at 0x13f1298ce290>, <__main__.Special at 0x13f1298cfbd0>]"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# will get an error\n",
        "# TypeError: unsupported operand type(s) for +: 'Special' and 'int'\n",
        "jax.tree.map(lambda x: x + 1,\n",
        "  [\n",
        "    Special(0, 1),\n",
        "    Special(2, 4)\n",
        "  ])"
      ],
      "metadata": {
        "id": "1K1vS3c0RFNY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from jax.tree_util import register_pytree_node\n",
        "\n",
        "class RegisteredSpecial(Special):\n",
        "  def __repr__(self):\n",
        "    return \"RegisteredSpecial(x={}, y={})\".format(self.x, self.y)\n",
        "\n",
        "def special_flatten(v):\n",
        "  \"\"\"Specifies a flattening recipe.\n",
        "\n",
        "  Params:\n",
        "    v: The value of the registered type to flatten.\n",
        "  Returns:\n",
        "    A pair of an iterable with the children to be flattened recursively,\n",
        "    and some opaque auxiliary data to pass back to the unflattening recipe.\n",
        "    The auxiliary data is stored in the treedef for use during unflattening.\n",
        "    The auxiliary data could be used, for example, for dictionary keys.\n",
        "  \"\"\"\n",
        "  children = (v.x, v.y)\n",
        "  aux_data = None\n",
        "  return (children, aux_data)\n",
        "\n",
        "def special_unflatten(aux_data, children):\n",
        "  \"\"\"Specifies an unflattening recipe.\n",
        "\n",
        "  Params:\n",
        "    aux_data: The opaque data that was specified during flattening of the\n",
        "      current tree definition.\n",
        "    children: The unflattened children\n",
        "\n",
        "  Returns:\n",
        "    A reconstructed object of the registered type, using the specified\n",
        "    children and auxiliary data.\n",
        "  \"\"\"\n",
        "  return RegisteredSpecial(*children)\n",
        "\n",
        "# Global registration\n",
        "register_pytree_node(\n",
        "    RegisteredSpecial,\n",
        "    special_flatten,    # Instruct JAX what are the children nodes.\n",
        "    special_unflatten   # Instruct JAX how to pack back into a `RegisteredSpecial`.\n",
        ")"
      ],
      "metadata": {
        "id": "EaBMGJrCRU5L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "jax.tree.map(lambda x: x + 1,\n",
        "  [\n",
        "   RegisteredSpecial(0, 1),\n",
        "   RegisteredSpecial(2, 4),\n",
        "  ])"
      ],
      "metadata": {
        "id": "6RcYh4s0UoS-",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730605929532,
          "user_tz": 420,
          "elapsed": 54,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "ad0eb946-ba2d-4403-b704-a62089316d86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[RegisteredSpecial(x=1, y=2), RegisteredSpecial(x=3, y=5)]"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Modern Python comes equipped with helpful tools to make defining containers\n",
        "# easier. Some will work with JAX out-of-the-box, but others require more care.\n",
        "\n",
        "from typing import NamedTuple, Any\n",
        "\n",
        "class MyOtherContainer(NamedTuple):\n",
        "  name: str\n",
        "  a: Any\n",
        "  b: Any\n",
        "  c: Any\n",
        "\n",
        "# NamedTuple subclasses are handled as pytree nodes, so\n",
        "# this will work out-of-the-box.\n",
        "jax.tree.leaves([\n",
        "    MyOtherContainer('Alice', 1, 2, 3),\n",
        "    MyOtherContainer('Bob', 4, 5, 6)\n",
        "])"
      ],
      "metadata": {
        "id": "6bA6DZSXWks1",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730592540790,
          "user_tz": 420,
          "elapsed": 55,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "745947bd-9139-4248-e726-3d072a77d0a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Alice', 1, 2, 3, 'Bob', 4, 5, 6]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Classes decorated with @dataclass are not automatically pytrees. However,\n",
        "# they can be registered as pytrees using the jax.tree_util.register_dataclass()\n",
        "# decorator.\n",
        "\n",
        "from dataclasses import dataclass\n",
        "import functools\n",
        "\n",
        "@functools.partial(jax.tree_util.register_dataclass,\n",
        "                   data_fields=['a', 'b', 'c'],\n",
        "                   meta_fields=['name'])\n",
        "@dataclass\n",
        "class MyDataclassContainer(object):\n",
        "  name: str\n",
        "  a: Any\n",
        "  b: Any\n",
        "  c: Any\n",
        "\n",
        "# MyDataclassContainer is now a pytree node.\n",
        "print(jax.tree.leaves([\n",
        "  MyDataclassContainer('apple', 5.3, 1.2, jnp.zeros([4])),\n",
        "  MyDataclassContainer('banana', jnp.array([3, 4]), -1., 0.)\n",
        "]))"
      ],
      "metadata": {
        "id": "hOsquRc0XVgR",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730606335247,
          "user_tz": 420,
          "elapsed": 196,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "a1877c1d-2f83-40ce-ac93-1b50d4d73a67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5.3, 1.2, Array([0., 0., 0., 0.], dtype=float32), Array([3, 4], dtype=int32), -1.0, 0.0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@jax.jit\n",
        "# x  can be either of type MyDataclassContainer or MyOtherContainer\n",
        "def f(x: MyDataclassContainer | MyOtherContainer):\n",
        "  return x.a + x.b\n",
        "\n",
        "# Notice that the name field does not appear as a leaf. This is because we\n",
        "# included it in the meta_fields argument to jax.tree_util.register_dataclass(),\n",
        "# indicating that it should be treated as metadata/auxiliary data, just like\n",
        "# aux_data in RegisteredSpecial above. Now instances of MyDataclassContainer can\n",
        "# be passed into JIT-ed functions, and name will be treated as static\n",
        "# (see Marking arguments as static for more information on static args).\n",
        "\n",
        "# Works fine! `mdc.name` is static.\n",
        "print(\"MyDataclassContainer\")\n",
        "mdc = MyDataclassContainer('mdc', 1, 2, 3)\n",
        "print(\"mdb = \", mdc)\n",
        "y = f(mdc)\n",
        "print(y)\n",
        "\n",
        "#\n",
        "print(\"\\nMyOtherContainer\")\n",
        "moc = MyOtherContainer('moc', 1, 2, 3)\n",
        "print(\"moc = \", moc)\n",
        "# Since the name field is a pytree leaf, JIT expects it to be convertible to\n",
        "# jax.Array, and the following raises an error.\n",
        "# y = f(moc)\n",
        "\n",
        "# works for non jit function\n",
        "def g(x: MyDataclassContainer | MyOtherContainer):\n",
        "  return x.a + x.b\n",
        "g(moc)"
      ],
      "metadata": {
        "id": "EKkxqkZeLS-V",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730607399632,
          "user_tz": 420,
          "elapsed": 1055,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "bb712f34-cd78-47e2-f82b-bcad2c60127e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MyDataclassContainer\n",
            "mdb =  MyDataclassContainer(name='mdc', a=1, b=2, c=3)\n",
            "3\n",
            "\n",
            "MyOtherContainer\n",
            "moc =  MyOtherContainer(name='moc', a=1, b=2, c=3)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pytrees and JAX transformations"
      ],
      "metadata": {
        "id": "CoHYeyaaePHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# jax.vmap(f, in_axes=(a1, {\"k1\": a2, \"k2\": a3}))\n",
        "# only the k2 argument is mapped\n",
        "jax.vmap(f, in_axes=(None, {\"k1\": None, \"k2\": 0}))\n",
        "jax.vmap(f, in_axes=(None, 0))  # equivalent to (None, {\"k1\": 0, \"k2\": 0})\n",
        "\n",
        "# also supports the case where only one of the arguments is batched\n",
        "# batch_convolve_v3 = jax.vmap(convolve, in_axes=[0, None])\n",
        "# batch_convolve_v3(xs, w)\n",
        "\n",
        "# TODO: application example, and what map does here?\n",
        "\n",
        "x1 = [\"x1\", {\"k1\": 1, \"k2\": 2}]\n",
        "x2 = [\"x2\", {\"k2\": 3, \"k2\": 4}]\n",
        "x = [x1, x2]\n",
        "f_vmap = jax.vmap(f, in_axes=(None, {\"k1\": None, \"k2\": 0}))\n",
        "f_vmap(x)"
      ],
      "metadata": {
        "id": "SM1QBIARayuf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explict key paths"
      ],
      "metadata": {
        "id": "_Yq_Ik1-eSnz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "\n",
        "ATuple = collections.namedtuple(\"ATuple\", ('name'))\n",
        "\n",
        "tree = [1, {'k1': 2, 'k2': (3, 4)}, ATuple('foo')]\n",
        "flattened, _ = jax.tree_util.tree_flatten_with_path(tree)\n",
        "\n",
        "print(\"flattened\")\n",
        "print(flattened)\n",
        "\n",
        "print(\"\\nkeystr\")\n",
        "for key_path, value in flattened:\n",
        "  print(f'Value of tree{jax.tree_util.keystr(key_path)}: {value}')"
      ],
      "metadata": {
        "id": "zms2yidGdz7U",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730608813749,
          "user_tz": 420,
          "elapsed": 54,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "299ebf97-09d3-4e3d-f752-d152a33707fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "flattened\n",
            "[((SequenceKey(idx=0),), 1), ((SequenceKey(idx=1), DictKey(key='k1')), 2), ((SequenceKey(idx=1), DictKey(key='k2'), SequenceKey(idx=0)), 3), ((SequenceKey(idx=1), DictKey(key='k2'), SequenceKey(idx=1)), 4), ((SequenceKey(idx=2), GetAttrKey(name='name')), 'foo')]\n",
            "\n",
            "keystr\n",
            "Value of tree[0]: 1\n",
            "Value of tree[1]['k1']: 2\n",
            "Value of tree[1]['k2'][0]: 3\n",
            "Value of tree[1]['k2'][1]: 4\n",
            "Value of tree[2].name: foo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Common pytree gotchas"
      ],
      "metadata": {
        "id": "qiTJ_T3lfqI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# accidentally introducing tree nodes instead of leaves\n",
        "\n",
        "a_tree = [jnp.zeros((2, 3)), jnp.zeros((3, 4))]\n",
        "\n",
        "# Try to make another pytree with ones instead of zeros.\n",
        "shapes = jax.tree.map(lambda x: x.shape, a_tree)\n",
        "print(shapes)\n",
        "# Thus, in the map, instead of calling jnp.ones on e.g. (2, 3), it’s called on 2 and 3.\n",
        "print(jax.tree.map(jnp.ones, shapes))\n",
        "\n",
        "# Soluitons:\n",
        "# 1. Rewrite the code to avoid the intermediate jax.tree.map().\n",
        "# 2. Convert the tuple into a NumPy array (np.array) or a JAX NumPy array\n",
        "#    (jnp.array), which makes the entire sequence a leaf."
      ],
      "metadata": {
        "id": "LHwnrJxyfrBy",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730608858810,
          "user_tz": 420,
          "elapsed": 54,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "446180dc-e559-4a39-d2b5-0d0addf782e2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[(2, 3), (3, 4)]\n",
            "[(Array([1., 1.], dtype=float32), Array([1., 1., 1.], dtype=float32)), (Array([1., 1., 1.], dtype=float32), Array([1., 1., 1., 1.], dtype=float32))]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# jax.tree_util functions treat None as the absence of a pytree node, not as a leaf:\n",
        "print(jax.tree.leaves([None, None, None]))\n",
        "\n",
        "# Solution\n",
        "print(jax.tree.leaves([None, None, None], is_leaf=lambda x: x is None))"
      ],
      "metadata": {
        "id": "p-uUOnkfggdF",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730608974938,
          "user_tz": 420,
          "elapsed": 53,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "69cd2163-c68a-4703-a84d-1559ec8ca798"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[]\n",
            "[None, None, None]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom pytrees and initialization with unexpected values\n",
        "# so that any input validation done at initialization may fail.\n",
        "\n",
        "class MyTree:\n",
        "  def __init__(self, a):\n",
        "    self.a = jnp.asarray(a)\n",
        "\n",
        "register_pytree_node(MyTree, lambda tree: ((tree.a,), None),\n",
        "    lambda _, args: MyTree(*args))\n",
        "\n",
        "tree = MyTree(jnp.arange(5.0))\n",
        "\n",
        "# TODO: when object() is passed to MyTree?\n",
        "# jax.vmap(lambda x: x)(tree)      # Error because object() is passed to `MyTree`.\n"
      ],
      "metadata": {
        "id": "6oz21aDrhC2m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# jax.jacobian(lambda x: x)(tree)  # Error because MyTree(...) is passed to `MyTree`."
      ],
      "metadata": {
        "id": "PwEPG1q7haCF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Soluiton 1\n",
        "\n",
        "class MyTree:\n",
        "  def __init__(self, a):\n",
        "    if not (type(a) is object or a is None or isinstance(a, MyTree)):\n",
        "      a = jnp.asarray(a)\n",
        "    self.a = a\n",
        "\n",
        "# Soluiton 2\n",
        "def tree_unflatten(aux_data, children):\n",
        "  del aux_data  # Unused in this class.\n",
        "  obj = object.__new__(MyTree)\n",
        "  obj.a = a\n",
        "  return obj"
      ],
      "metadata": {
        "id": "1kqQXw7ohs2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Common pytree patterns"
      ],
      "metadata": {
        "id": "rNYehVBIdhU5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transpose a pytree: turn a list of trees into a tree of lists\n",
        "\n",
        "\n",
        "def tree_transpose(list_of_trees):\n",
        "  \"\"\"\n",
        "  Converts a list of trees of identical structure into a single tree of lists.\n",
        "  \"\"\"\n",
        "  # The *xs syntax allows the lambda function to accept any number of arguments\n",
        "  # and packs them into a tuple called xs.\n",
        "  # *list_of_trees: unpacks the list_of_trees so that each tree in the list is\n",
        "  # passed as a separate argument to jax.tree.map.\n",
        "  # jax.tree.map iterates through the leaves of multiple trees simultaneously.\n",
        "  # At each step, it gathers the corresponding leaves from each tree and passes\n",
        "  # them to the lambda function.\n",
        "  return jax.tree.map(lambda *xs: list(xs), *list_of_trees)\n",
        "\n",
        "# Convert a dataset from row-major to column-major.\n",
        "episode_steps = [dict(t=1, obs=3), dict(t=2, obs=4)]\n",
        "print(episode_steps)\n",
        "print(*episode_steps)\n",
        "tree_transpose(episode_steps)"
      ],
      "metadata": {
        "id": "8b-w5ZOGiH9u",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730610724207,
          "user_tz": 420,
          "elapsed": 2,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "0d2913b4-e435-413b-cbcd-9ad8ed51fe99"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'t': 1, 'obs': 3}, {'t': 2, 'obs': 4}]\n",
            "{'t': 1, 'obs': 3} {'t': 2, 'obs': 4}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'obs': [3, 4], 't': [1, 2]}"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def f(*xs):\n",
        "  return list(xs)\n",
        "\n",
        "print(*episode_steps)\n",
        "f(*episode_steps)"
      ],
      "metadata": {
        "id": "_KK7uBzeb0BJ",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730610781777,
          "user_tz": 420,
          "elapsed": 53,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "d4312295-9381-4681-922b-bf92a04ffc46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'t': 1, 'obs': 3} {'t': 2, 'obs': 4}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'obs': 3, 't': 1}, {'obs': 4, 't': 2}]"
            ]
          },
          "metadata": {},
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "jax.tree.transpose(\n",
        "  outer_treedef = jax.tree.structure([0 for e in episode_steps]),\n",
        "  inner_treedef = jax.tree.structure(episode_steps[0]),\n",
        "  pytree_to_transpose = episode_steps\n",
        ")"
      ],
      "metadata": {
        "id": "GSmU8nV3YBIn",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730609683108,
          "user_tz": 420,
          "elapsed": 55,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "f225cf35-1620-4422-d1cf-1a32051c6887"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'obs': [3, 4], 't': [1, 2]}"
            ]
          },
          "metadata": {},
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction To Parallel Programming"
      ],
      "metadata": {
        "id": "DY2xb-QcioG1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(jax.devices())\n",
        "\n",
        "arr = jnp.arange(32.0).reshape(4, 8)\n",
        "print(\"\\nArray devices \", arr.devices())\n",
        "print(\"\\n Array sarding\", arr.sharding)\n",
        "\n",
        "jax.debug.visualize_array_sharding(arr)"
      ],
      "metadata": {
        "id": "U-tuEbUSjyWZ",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730613077068,
          "user_tz": 420,
          "elapsed": 199,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "00b43498-4630-4355-e380-3cc3cfb5d2d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0), TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1), TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0), TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1), TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0), TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1), TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0), TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]\n",
            "\n",
            "Array devices  {TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)}\n",
            "\n",
            " Array sarding SingleDeviceSharding(device=TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0), memory_kind=device)\n",
            "┌────────────────────────────────────────────────┐\n",
            "│                                                │\n",
            "│                                                │\n",
            "│                                                │\n",
            "│                                                │\n",
            "│                     TPU 0                      │\n",
            "│                                                │\n",
            "│                                                │\n",
            "│                                                │\n",
            "│                                                │\n",
            "└────────────────────────────────────────────────┘\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from jax.sharding import PartitionSpec as P\n",
        "\n",
        "mesh = jax.make_mesh((2, 4), ('x', 'y'))\n",
        "# NamedSharding specifies an N-dimensional grid of devices with named axes,\n",
        "# where jax.sharding.Mesh allows for precise device placement\n",
        "sharding = jax.sharding.NamedSharding(mesh, P('x', 'y'))\n",
        "print(sharding)\n",
        "\n",
        "arr_sharded = jax.device_put(arr, sharding)\n",
        "\n",
        "print(\"\\narr_sharded\", arr_sharded)\n",
        "jax.debug.visualize_array_sharding(arr_sharded)\n",
        "\n",
        "# TODO: ?\n",
        "# The device numbers here are not in numerical order, because the mesh reflects\n",
        "# the underlying toroidal topology of the device."
      ],
      "metadata": {
        "id": "6xM_w_CnlC37",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730613189308,
          "user_tz": 420,
          "elapsed": 1017,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "8cecfa46-c2a9-4218-87b1-7566171bef40"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NamedSharding(mesh=Mesh('x': 2, 'y': 4), spec=PartitionSpec('x', 'y'), memory_kind=device)\n",
            "\n",
            "arr_sharded [[ 0.  1.  2.  3.  4.  5.  6.  7.]\n",
            " [ 8.  9. 10. 11. 12. 13. 14. 15.]\n",
            " [16. 17. 18. 19. 20. 21. 22. 23.]\n",
            " [24. 25. 26. 27. 28. 29. 30. 31.]]\n",
            "┌──────────┬──────────┬──────────┬──────────┐\n",
            "│          │          │          │          │\n",
            "│  TPU 0   │  TPU 1   │  TPU 2   │  TPU 3   │\n",
            "│          │          │          │          │\n",
            "│          │          │          │          │\n",
            "├──────────┼──────────┼──────────┼──────────┤\n",
            "│          │          │          │          │\n",
            "│  TPU 6   │  TPU 7   │  TPU 4   │  TPU 5   │\n",
            "│          │          │          │          │\n",
            "│          │          │          │          │\n",
            "└──────────┴──────────┴──────────┴──────────┘\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Automatic parallelism via jit"
      ],
      "metadata": {
        "id": "eqKGdR2InhzD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@jax.jit\n",
        "def f_elementwise(x):\n",
        "  return 2 * jnp.sin(x) + 1\n",
        "\n",
        "result = f_elementwise(arr_sharded)\n",
        "\n",
        "print(\"shardings match:\", result.sharding == arr_sharded.sharding)\n",
        "jax.debug.visualize_array_sharding(result)"
      ],
      "metadata": {
        "id": "yKSkZg3inlHz",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730617259304,
          "user_tz": 420,
          "elapsed": 1093,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "50ccd535-4426-4b91-a1f8-bd4b7c1fd15b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shardings match: True\n",
            "┌──────────┬──────────┬──────────┬──────────┐\n",
            "│          │          │          │          │\n",
            "│  TPU 0   │  TPU 1   │  TPU 2   │  TPU 3   │\n",
            "│          │          │          │          │\n",
            "│          │          │          │          │\n",
            "├──────────┼──────────┼──────────┼──────────┤\n",
            "│          │          │          │          │\n",
            "│  TPU 6   │  TPU 7   │  TPU 4   │  TPU 5   │\n",
            "│          │          │          │          │\n",
            "│          │          │          │          │\n",
            "└──────────┴──────────┴──────────┴──────────┘\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@jax.jit\n",
        "def f_contract(x):\n",
        "  return x.sum(axis=0)\n",
        "\n",
        "result = f_contract(arr_sharded)\n",
        "jax.debug.visualize_array_sharding(result)\n",
        "print(result)\n",
        "\n",
        "# The result is partially replicated: that is, the first two elements of the\n",
        "# array are replicated on devices 0 and 6, the second on 1 and 7, and so on."
      ],
      "metadata": {
        "id": "3UHvNz0S0_4m",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730617280979,
          "user_tz": 420,
          "elapsed": 1120,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "4e20f4b3-c954-4102-faf7-3e28d3c2efce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "┌───────┬───────┬───────┬───────┐\n",
            "│TPU 0,6│TPU 1,7│TPU 2,4│TPU 3,5│\n",
            "└───────┴───────┴───────┴───────┘\n",
            "[48. 52. 56. 60. 64. 68. 72. 76.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Semi-automated sharding with constraints"
      ],
      "metadata": {
        "id": "Ko-1hb4T1P_D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@jax.jit\n",
        "def f_contract_2(x):\n",
        "  out = x.sum(axis=0)\n",
        "  mesh = jax.make_mesh((8,), ('x',))\n",
        "  sharding = jax.sharding.NamedSharding(mesh, P('x'))\n",
        "  return jax.lax.with_sharding_constraint(out, sharding)\n",
        "\n",
        "result = f_contract_2(arr_sharded)\n",
        "jax.debug.visualize_array_sharding(result)\n",
        "print(result)\n",
        "\n",
        "# the output not to be partially-replicated, but rather to be fully sharded across the eight devices"
      ],
      "metadata": {
        "id": "zPjndoIt1RXV",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730617391767,
          "user_tz": 420,
          "elapsed": 1122,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "56f09c76-f899-4de8-db55-491283fc1347"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐\n",
            "│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 6 │ TPU 7 │ TPU 4 │ TPU 5 │\n",
            "└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘\n",
            "[48. 52. 56. 60. 64. 68. 72. 76.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Manual parallelism with shard_map"
      ],
      "metadata": {
        "id": "qJdGya5S2gl_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from jax.experimental.shard_map import shard_map\n",
        "mesh = jax.make_mesh((8,), ('x',))\n",
        "\n",
        "f_elementwise_sharded = shard_map(\n",
        "    f_elementwise,\n",
        "    mesh=mesh,\n",
        "    in_specs=P('x'),\n",
        "    out_specs=P('x'))\n",
        "\n",
        "arr = jnp.arange(32)\n",
        "result = f_elementwise_sharded(arr)\n",
        "jax.debug.visualize_array_sharding(arr)\n",
        "jax.debug.visualize_array_sharding(result)"
      ],
      "metadata": {
        "id": "gsj85s1z2htS",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730617860959,
          "user_tz": 420,
          "elapsed": 2985,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "c2350dec-f735-48b1-8e29-d1cf2fa73a00"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "┌───────┐\n",
            "│ TPU 0 │\n",
            "└───────┘\n",
            "┌───────┬───────┬───────┬───────┬───────┬───────┬───────┬───────┐\n",
            "│ TPU 0 │ TPU 1 │ TPU 2 │ TPU 3 │ TPU 6 │ TPU 7 │ TPU 4 │ TPU 5 │\n",
            "└───────┴───────┴───────┴───────┴───────┴───────┴───────┴───────┘\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x = jnp.arange(32)\n",
        "print(f\"global shape: {x.shape=}\")\n",
        "\n",
        "def f(x):\n",
        "  print(f\"device local shape: {x.shape=}\")\n",
        "  return x * 2\n",
        "\n",
        "y = shard_map(f, mesh=mesh, in_specs=P('x'), out_specs=P('x'))(x)\n",
        "\n",
        "# The function you write only “sees” a single batch of the data"
      ],
      "metadata": {
        "id": "9-Z7_4KO3WOg",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730617900303,
          "user_tz": 420,
          "elapsed": 4171,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "47114629-5c92-4abb-d018-4baa39100140"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "global shape: x.shape=(32,)\n",
            "device local shape: x.shape=(4,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# operates separately on each shard, and the resulting summation reflects this.\n",
        "def f(x):\n",
        "  return jnp.sum(x, keepdims=True)\n",
        "\n",
        "shard_map(f, mesh=mesh, in_specs=P('x'), out_specs=P('x'))(x)"
      ],
      "metadata": {
        "id": "T8w9YnWB3nLn",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730617968579,
          "user_tz": 420,
          "elapsed": 3275,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "816e196a-a836-4122-a680-c422bb710065"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([  6,  22,  38,  54,  70,  86, 102, 118], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# sum across shards using jax.lax.psum()\n",
        "def f(x):\n",
        "  sum_in_shard = x.sum()\n",
        "  return jax.lax.psum(sum_in_shard, 'x')\n",
        "\n",
        "shard_map(f, mesh=mesh, in_specs=P('x'), out_specs=P())(x)"
      ],
      "metadata": {
        "id": "6HlkGyHr3tjG",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730618038162,
          "user_tz": 420,
          "elapsed": 4345,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "a283c7d6-5dc2-4b44-ed58-f5fb26e9edc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array(496, dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Comparing the three approaches"
      ],
      "metadata": {
        "id": "mpf-b6PR4KcN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@jax.jit\n",
        "def layer(x, weights, bias):\n",
        "  return jax.nn.sigmoid(x @ weights + bias)\n",
        "\n",
        "# jit\n",
        "import numpy as np\n",
        "rng = np.random.default_rng(0)\n",
        "\n",
        "x = rng.normal(size=(32,))\n",
        "weights = rng.normal(size=(32, 4))\n",
        "bias = rng.normal(size=(4,))\n",
        "\n",
        "result=layer(x, weights, bias)\n",
        "print(\"jit \", result)\n",
        "jax.debug.visualize_array_sharding(result)\n",
        "\n",
        "mesh = jax.make_mesh((8,), ('x',))\n",
        "sharding = jax.sharding.NamedSharding(mesh, P('x'))\n",
        "\n",
        "x_sharded = jax.device_put(x, sharding)\n",
        "weights_sharded = jax.device_put(weights, sharding)\n",
        "\n",
        "result = layer(x_sharded, weights_sharded, bias)\n",
        "print(\"\\nsemi 1\", result)\n",
        "jax.debug.visualize_array_sharding(result)\n",
        "\n",
        "@jax.jit\n",
        "def layer_auto(x, weights, bias):\n",
        "  x = jax.lax.with_sharding_constraint(x, sharding)\n",
        "  weights = jax.lax.with_sharding_constraint(weights, sharding)\n",
        "  return layer(x, weights, bias)\n",
        "result=layer_auto(x, weights, bias)\n",
        "print(\"\\nsemi2\", result)  # pass in unsharded inputs\n",
        "jax.debug.visualize_array_sharding(result)\n",
        "\n",
        "\n",
        "from functools import partial\n",
        "\n",
        "@jax.jit\n",
        "@partial(shard_map, mesh=mesh,\n",
        "         in_specs=(P('x'), P('x', None), P(None)),\n",
        "         out_specs=P(None))\n",
        "def layer_sharded(x, weights, bias):\n",
        "  return jax.nn.sigmoid(jax.lax.psum(x @ weights, 'x') + bias)\n",
        "\n",
        "result = layer_sharded(x, weights, bias)\n",
        "print(\"\\nmanual\", result)\n",
        "jax.debug.visualize_array_sharding(result)"
      ],
      "metadata": {
        "id": "d6dvSkR14Mmv",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730618397096,
          "user_tz": 420,
          "elapsed": 4492,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "441ad22b-d12b-465a-c333-384e3c6b467b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "jit  [0.02138912 0.893112   0.59892005 0.97742504]\n",
            "┌───────┐\n",
            "│ TPU 0 │\n",
            "└───────┘\n",
            "\n",
            "semi 1 [0.02138914 0.89311206 0.5989201  0.97742504]\n",
            "┌───────────────────┐\n",
            "│TPU 0,1,2,3,4,5,6,7│\n",
            "└───────────────────┘\n",
            "\n",
            "semi2 [0.02138914 0.89311206 0.5989201  0.97742504]\n",
            "┌───────────────────┐\n",
            "│TPU 0,1,2,3,4,5,6,7│\n",
            "└───────────────────┘\n",
            "\n",
            "manual [0.02138914 0.89311206 0.5989201  0.97742504]\n",
            "┌───────────────────┐\n",
            "│TPU 0,1,2,3,4,5,6,7│\n",
            "└───────────────────┘\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stateful Computations"
      ],
      "metadata": {
        "id": "ZtWL5RD15XCJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Counter example"
      ],
      "metadata": {
        "id": "E5-Rt89c76M7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "class Counter:\n",
        "  \"\"\"A simple counter.\"\"\"\n",
        "\n",
        "  def __init__(self):\n",
        "    self.n = 0\n",
        "\n",
        "  def count(self) -> int:\n",
        "    \"\"\"Increments the counter and returns the new value.\"\"\"\n",
        "    self.n += 1\n",
        "    return self.n\n",
        "\n",
        "  def reset(self):\n",
        "    \"\"\"Resets the counter to zero.\"\"\"\n",
        "    self.n = 0\n",
        "\n",
        "\n",
        "counter = Counter()\n",
        "\n",
        "for _ in range(3):\n",
        "  print(counter.count())"
      ],
      "metadata": {
        "id": "HznbiTCo6T-E",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730618673435,
          "user_tz": 420,
          "elapsed": 53,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "a23c47db-3fd5-4557-da9d-cae3e5798de1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "2\n",
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The counter’s n attribute maintains the counter’s state between successive\n",
        "# calls of count. It is modified as a side effect of calling count.\n",
        "\n",
        "# in count involves a side effect: it modifies the input counter in-place, and\n",
        "# so this function is not supported by jit. Such side effects are executed only\n",
        "# once when the function is first traced, and subsequent calls will not repeat\n",
        "# the side effect.\n",
        "# TODO: why?\n",
        "\n",
        "counter.reset()\n",
        "fast_count = jax.jit(counter.count)\n",
        "\n",
        "for _ in range(3):\n",
        "  print(fast_count())"
      ],
      "metadata": {
        "id": "_MN6Gwtd6mX3",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730618750305,
          "user_tz": 420,
          "elapsed": 1068,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "bce773dc-00cd-4eba-bf2b-e8961466f4ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "1\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Solution\n",
        "\n",
        "CounterState = int\n",
        "\n",
        "class CounterV2:\n",
        "\n",
        "  def count(self, n: CounterState) -> tuple[int, CounterState]:\n",
        "    # You could just return n+1, but here we separate its role as\n",
        "    # the output and as the counter state for didactic purposes.\n",
        "    return n+1, n+1\n",
        "\n",
        "  def reset(self) -> CounterState:\n",
        "    return 0\n",
        "\n",
        "counter = CounterV2()\n",
        "state = counter.reset()\n",
        "\n",
        "for _ in range(3):\n",
        "  value, state = counter.count(state)\n",
        "  print(value)"
      ],
      "metadata": {
        "id": "dDerU4fH7NNX",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730618952864,
          "user_tz": 420,
          "elapsed": 53,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "badabd44-e9c5-47f3-8883-e4240cf45fcf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1\n",
            "2\n",
            "3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A general strategy"
      ],
      "metadata": {
        "id": "YrRVl0Da79ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class StatefulClass\n",
        "\n",
        "  state: State\n",
        "\n",
        "  def stateful_method(*args, **kwargs) -> Output:\n",
        "    return self.state\n",
        "\n",
        "class StatelessClass\n",
        "\n",
        "  def stateless_method(state: State, *args, **kwargs) -> (Output, State):\n",
        "    return state, state\n",
        "\n",
        "#  JAX pseudo-randomness API, jax.random, is an exmaple"
      ],
      "metadata": {
        "id": "c4s7jBj48O5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Simple worked example: Linear Regression"
      ],
      "metadata": {
        "id": "q1ZWvPrq-WEI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import NamedTuple\n",
        "\n",
        "class Params(NamedTuple):\n",
        "  weight: jnp.ndarray\n",
        "  bias: jnp.ndarray\n",
        "\n",
        "\n",
        "def init(rng) -> Params:\n",
        "  \"\"\"Returns the initial model params.\"\"\"\n",
        "  weights_key, bias_key = jax.random.split(rng)\n",
        "  weight = jax.random.normal(weights_key, ())\n",
        "  bias = jax.random.normal(bias_key, ())\n",
        "  return Params(weight, bias)\n",
        "\n",
        "\n",
        "def loss(params: Params, x: jnp.ndarray, y: jnp.ndarray) -> jnp.ndarray:\n",
        "  \"\"\"Computes the least squares error of the model's predictions on x against y.\"\"\"\n",
        "  pred = params.weight * x + params.bias\n",
        "  return jnp.mean((pred - y) ** 2)\n",
        "\n",
        "\n",
        "LEARNING_RATE = 0.005\n",
        "\n",
        "@jax.jit\n",
        "# Notice that we manually pipe the params in and out of the update function.\n",
        "def update(params: Params, x: jnp.ndarray, y: jnp.ndarray) -> Params:\n",
        "  \"\"\"Performs one SGD update step on params using the given data.\"\"\"\n",
        "  grad = jax.grad(loss)(params, x, y)\n",
        "\n",
        "  # If we were using Adam or another stateful optimizer,\n",
        "  # we would also do something like\n",
        "  #\n",
        "  #   updates, new_optimizer_state = optimizer(grad, optimizer_state)\n",
        "  #\n",
        "  # and then use `updates` instead of `grad` to actually update the params.\n",
        "  # (And we'd include `new_optimizer_state` in the output, naturally.)\n",
        "\n",
        "  new_params = jax.tree.map(\n",
        "      lambda param, g: param - g * LEARNING_RATE, params, grad)\n",
        "\n",
        "  return new_params"
      ],
      "metadata": {
        "id": "-t1Jcdl5958v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "rng = jax.random.key(42)\n",
        "\n",
        "# Generate true data from y = w*x + b + noise\n",
        "true_w, true_b = 2, -1\n",
        "x_rng, noise_rng = jax.random.split(rng)\n",
        "xs = jax.random.normal(x_rng, (128, 1))\n",
        "noise = jax.random.normal(noise_rng, (128, 1)) * 0.5\n",
        "ys = xs * true_w + true_b + noise\n",
        "\n",
        "# Fit regression\n",
        "params = init(rng)\n",
        "for _ in range(1000):\n",
        "  params = update(params, xs, ys)\n",
        "\n",
        "plt.scatter(xs, ys)\n",
        "plt.plot(xs, params.weight * xs + params.bias, c='red', label='Model Prediction')\n",
        "plt.legend();"
      ],
      "metadata": {
        "colab": {
          "height": 265
        },
        "id": "LS9TXUgN_R3O",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1730620014765,
          "user_tz": 420,
          "elapsed": 6562,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "5e2f839a-a017-4f70-e0bd-c3a8f801f7f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAuUAAAHwCAYAAAAByRFLAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90\nbGliIHZlcnNpb24zLjkuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/TGe4hAAAACXBIWXMAABYl\nAAAWJQFJUiTwAABxQ0lEQVR4nO3deViVZf7H8c/BDRdcEMU9SMClMpOUFC2TsmVqLC0tdzOzVatp\nRqspcyrTfjWprZpmLi1UmmU1pZJaoiGpuaUCiguWKJKKmivn9wcdBHkO5zn7Ad6v6+qa6dzPcqNZ\nH26+9/e2WK1WqwAAAAD4TZC/JwAAAABUdIRyAAAAwM8I5QAAAICfEcoBAAAAPyOUAwAAAH5GKAcA\nAAD8jFAOAAAA+BmhHAAAAPAzQjkAAADgZ4RyAAAAwM8I5QAAAICfEcoBAAAAP6vs7wn4QkREhI4e\nParIyEh/TwUAAADlWGZmpmrXrq1du3Y5dV+FCOVHjx7VyZMn/T0NAAAAlHOuZs4KEcptK+Rr1671\n80wAAABQnsXGxrp0HzXlAAAAgJ8RygEAAAA/I5QDAAAAfkYoBwAAAPyMUA4AAAD4GaEcAAAA8DNC\nOQAAAOBnFaJPuTPy8/OVm5urvLw8nTp1Slar1d9TAnABi8WiatWqKSQkRKGhoQoKYn0BAFC2EcqL\nyM/P1969e3XixAl/TwVAKaxWq06ePKmTJ0/q+PHjat68OcEcAFCmEcqLyM3N1YkTJ1S5cmU1atRI\nNWvW5D/0QADKz8/X8ePHtX//fp04cUK5ubkKCwvz97QAAHAZobyIvLw8SVKjRo0UEhLi59kAsCco\nKKjwz2hWVpby8vII5QAASVJadp6SM3J07ORZ1QqurPioMMWEB36uI5QXcerUKUlSzZo1/TwTAGbY\n/qza/uwCACqu5IwcTUlK15rM3BJjnSJDNTohWvFRgbuAQ21GEbZNnZSsAGWDxWKRJDZkA0AFl5i6\nR4NmphgGcklak5mrQTNT9EnqXh/PzDzSJ4AyyxbKAQAVV3JGjp5csEn5DtZn8q3S2AUblZyR45uJ\nOYlQDgAAgDJrSlK6w0Buk2+Vpiale3dCLiKUAwAAoExKy86zW7JiT0pmrtKy87w0I9cRylEmLF++\nXBaLRc8995xbz3n//fdlsVj0/vvve2Re/mTva4mIiFBERITX3vvcc8/JYrFo+fLlXnsHAABmuFqK\nEoglLIRyGLJYLLJYLAoKCtKOHTvsXnfttdcWXlsegu6FbMG36F/VqlVTZGSkhg4dql9//dXfU/S4\n8vSNCwCgfDt28qxP7/MmWiLCrsqVK+vs2bOaOXOmJkyYUGI8PT1dK1asKLyuPLv88st12223SZKO\nHDmi5cuXa/bs2frkk0/0/fff66qrrvLvBItISkry6vMffvhh3XXXXWrRooVX3wMAgCO1gl2Lsq7e\n502BNyMEjPDwcDVu3FizZs3Sf/7zH1WuXPwflxkzZshqteqWW27RwoUL/TNJH2nfvn2x0hmr1aph\nw4Zp9uzZevLJJ7Vs2TL/Te4CLVu29Orzw8LCOKgHABAQXO07Hoj9yilfQalGjBih/fv366uvvir2\n+ZkzZzR79mx16dJFl1xyid3709PTNXjwYDVt2lRVq1ZVkyZNNHjwYKWnG+98zs7O1vDhwxUeHq7q\n1aurffv2mj17dqlzzM3N1ZNPPqk2bdqoevXqqlOnjhISErR48WLnv2CTLBaLHnzwQUnSmjVrCj+3\n1XMfPXpUjz/+uCIiIlSlSpVigX7btm0aOnSomjdvrmrVqik8PFz9+/fX9u3bDd+VkZGhO++8U/Xq\n1VPNmjXVpUsXff3113bnVlpNeWJiohISEhQaGqrg4GBFRETo7rvv1s8//yxJ6t69u4YNGyZJGjZs\nWLGynV27dkkqvaY8KSlJN954Y+HzY2JiNHbsWB05cqTEtd27d5fFYtHZs2c1YcIERUdHq1q1amre\nvLnGjBmj06dP2/0aAQCQpJjwEHWKDHXqnrjI0IA84ZOVcpTq7rvv1uOPP64ZM2YUlm9I0pdffqns\n7GxNnDhRGRkZhvempqbquuuuU15env7+97+rbdu22rZtmz744AN98cUXSkpK0pVXXll4/aFDh9Sl\nSxft3LlTXbt2VdeuXfX777/r/vvvV8+ePQ3fsXv3bnXv3l27du1St27ddOONN+r48eP66quvdOON\nN2ratGkaMWKER39NbGwH1lzYK/v06dPq0aOHcnNz1bNnT9WuXVuRkZGSpG+//Va9e/fWmTNndOut\ntyoqKkpZWVlasGCBvv76ay1btkwdOnQofFZ6ero6d+6sQ4cO6aabblL79u2VkZGh2267TTfddJNT\nc7Wt7IeFhal3795q0KCBsrKytGzZMrVq1UpXXnmlhg4dqrp16+qLL75Qr1691L59+8Jn1K1bt9R3\nTJs2TQ888IBq1qypO++8Uw0bNtTy5cs1adIkLVq0SMnJyYbP6N+/v3788UfddNNNql27tr755hu9\n/PLLOnDggGbNmmX6awQAVEyjE6I1aGaKqbaIQRZpVEK09yflCmsF0KFDB2uHDh0cXvfrr79af/31\nVx/MKPBJsjZt2tRqtVqtw4cPt1aqVMm6d+/ewvEbbrjBWrt2bevx48etTz/9tFWSddasWYXj+fn5\n1tatW1slWefNm1fs2R9//LFVkrVVq1bWc+fOFX4+YsQIqyTro48+Wuz61NRUa+XKla2SrOPGjSs2\nds0111gtFov1o48+Kvb5H3/8Yb388sutwcHB1v379xd+PmvWrBJzLY3t+iFDhhT7PD8/3zp48GCr\nJGuPHj0KP7/ooouskqwJCQnWY8eOFbsnNzfXWrduXWv9+vWtW7ZsKTa2efNma82aNa1XXHFFsc+v\nv/56qyTr5MmTi32+cOFCqyTDr+Wiiy6yXnTRRcU+mzZtmlWStWPHjtbDhw8XGzt79qz1t99+K/E1\n2/s1GjdunFWSddmyZYWf7dq1y1q1alVrSEiIdevWrcWuf+CBB6ySrCNGjCj2+TXXXGOVZO3QoYP1\n0KFDhZ8fO3bM2rJlS2tQUJD1999/N5xDUfy5BQB8vGa3NXLsV9aLxtj/K3LsV9bENXu8PhezufNC\nlK84w2IpO3950IgRI3Tu3Dm99957kgpWp5csWaIBAwaoRo0ahvesWrVK27ZtU+fOnTVgwIBiY/36\n9VPXrl21fft2rVy5UlJBOcwHH3ygkJCQEm0Pr7zyyhLPkKQNGzZoxYoV6tOnj+66665iY3Xr1tX4\n8eN18uRJzZ8/39UvvdAvv/yi5557Ts8995wee+wxdejQQXPmzFH16tX14osvlrj+1VdfVc2aNYt9\nNmfOHB0+fFjjx49X27Zti41dcsklGjFihNavX1/Y0SUrK0tLlixRZGSkHn744WLX9+rVS9dcc43p\n+b/++uuSClaz69SpU2ysUqVKaty4selnGZk3b55Onz6thx9+WK1bty429uKLLyokJERz587VqVOn\nStw7adIkhYae/9FjzZo1NWDAAOXn5xeW1QAAUJp+HVto7vA4xdkpZYmLDNXc4XHq27G5j2dmHuUr\ncCguLk6XXXaZ3nvvPf373//WjBkzlJ+fX2pZyLp16yRJPXr0MBzv0aOHVq5cqfXr1+vqq6/Wtm3b\ndOLECXXr1q1EaJQK6o8vrC1fvXq1pIJuKEb9yw8ePChJ2rp1q6mvszQbNmzQhg0bJElVqlRR48aN\nNWjQII0dO7ZEwA4ODla7du1KPMM23w0bNhjONy0trXC+bdu21fr16yVJXbt2VaVKlUpc3717d61Y\nscLh3I8fP67NmzcrPDxcV1xxhcPrXVHa73e9evV0xRVX6IcfftC2bdt0+eWXFxsvWsJk07x5wb80\n//jjDy/MFgBQHsVHhSk+Kkxp2XlKzsjRsZNnVSu4suKjwgKyhvxChHKYMmLECI0aNUrffvutZs2a\npdjY2FIDnm1jn70VWNvnhw8fLnZ9eHi44fWNGjUq8dmhQ4ckSUuWLNGSJUvszuXYsWN2x8waMmSI\n6b7dDRs2LFFnLp2f77vvvlvq/bb5uvJrYsT2a9y0aVNT17vC2d/voozqzG2dfs6dO+eZCQIAKoyY\n8JAyEcIvRPmKM6zWsvOXhw0aNEjVq1fXyJEjtW/fPt13332lXm9b7d6/f7/h+O+//17sOtv/Zmdn\nG15v9BzbPVOmTJHVarX7l683CxoFcun8fDds2FDqfIcMGVLsemd+TYzYQu++ffuc+TKc4uzvNwAA\nKI5QDlPq1q2rO+64Q1lZWapZs6buvvvuUq+3raLbO4rd9rmt00jr1q1Vo0YN/fLLL4bt84yeYzuw\n58cffzT5VfiXs/O1/RquXLnScMXY7DH3NWvW1KWXXqrs7OzCkpjS2EplnFmlLu33+/Dhw/rll18U\nHBysNm3amH4mAAAVCaEcpr3wwgv6/PPP9d133ykkpPQfC8XHx6tVq1ZauXKlPvvss2Jjn332mX74\n4QfFxMSoa9eukgrqtAcMGKC8vLwS9dY///yzPvjggxLvuPLKK9WtWzctWLCgcBPqhTZt2qQDBw44\n8VV6z7Bhwwo3oBbtbW6Tn59fLNQ2a9ZM119/vTIzM/XGG28Uu/aLL74wVU9uM2rUKEnSyJEjS3zT\nk5+fX7iSLUn169eXJO3Zs8f08wcOHKgqVaro9ddfL9Ei85lnntHRo0c1cOBAVatWzfQzAQCoSKgp\nh2ktWrQwfbS6xWLR7Nmzdf3116tfv37q1auXWrdure3bt2vhwoUKCQnRnDlzFBR0/vvCCRMmKCkp\nSZMnT9bPP/9c2Kc8MTFRN998s7788ssS7/nwww/Vo0cPDR8+XFOnTlVcXJzq1q2rrKwsbdy4UZs3\nb9bq1avVsGFDj/06uKp+/fr67LPPdPvtt+uqq65SQkKCLrnkEgUFBWnPnj1avXq1Dh06pJMnTxbe\n8+abb6pz58569NFHtXjxYl1++eXKyMjQ559/rltvvVWLFi0y9e57771XK1eu1Jw5cxQdHa1evXqp\nQYMG+u233/T999/rnnvuKfxmqHPnzqpRo4YmT56s3Nzcwpr2Rx55xG75SUREhCZPnqyHHnpIHTp0\nUN++fdWgQQOtWLFCq1evVuvWrTVp0iT3fgEBACjHCOXwmri4OKWmpuqFF17Q0qVLtWjRIoWFhenu\nu+/WM888o1atWhW7PiwsTMnJyXrqqae0aNEi/fzzz2rVqpXefvttRUREGIbyZs2aae3atXr99dc1\nf/58ffDBBzp37pwaNWqktm3b6pFHHtFll13mqy/ZoYSEBG3cuFGvvPKKvvvuO/3444+FJ5326NFD\nffr0KXZ9dHS0fvrpJ40dO1ZLly7V8uXL1a5dOy1cuFAHDx40Hcpt3yT17NlT06dP1yeffKJTp06p\ncePG6tatm/7+978XXluvXj3Nnz9f48eP16xZs3T8+HFJBavhpdWEP/jgg4qKitIrr7yi+fPn68SJ\nE2revLn++c9/6qmnnnJ4+BAAABWZxWr1wq7AABMbGytJWrt2banX2VrnUfcKlB38uQUABBKzufNC\n1JQDAAAAfkYoBwAAAPyMUA4AAAD4GaEcAAAA8DNCOQAAAOBnhHIAAADAzwjlAMqsCtDRFQBQQRDK\ni7BYLJIKjh0HEPhsodz2ZxcAgLKKUF5EtWrVJKnwBEMAgc32Z9X2ZxcAgLKKUF5ESEiIJGn//v3K\ny8tTfn4+Px4HAozValV+fr7y8vK0f/9+Sef/7AIAUFZV9vcEAkloaKiOHz+uEydOKCsry9/TAWBC\njRo1FBoa6u9pAADgFkJ5EUFBQWrevLlyc3OVl5enU6dOsVIOBCCLxaJq1aopJCREoaGhCgrih34A\ngLItYEP53LlzNXjwYEnSu+++q3vvvdcn7w0KClJYWJjCwsJ88j4AAAAgIEP53r179cgjj6hWrVo6\nduyYv6cDAACAAJeWnafkjBwd+/OMWm5fr0sj6qvF367z97RMC7hQbrVaNWzYMNWvX1+9e/fWK6+8\n4u8pAQAAIEAlZ+RoSlK61mTm6oa0VZr2+YTCsf8b+ZK6PHGv4qMCvwIi4EL51KlT9f3332v58uX6\n/vvv/T0dAAAABKjE1D16csEmNTpyQLvevqfEeO6ufRo0M0UTe7dT347N/TBD8wJqd9TWrVs1duxY\njR49WldffbW/pwMAAIAAlZyRo39/9os+nfOEVhkEcklKbHe98q3S2AUblZyR4+MZOidgVsrPnj2r\nQYMGqUWLFpowYYLjGwzExsYafr5161a1adPGnekBAAAggGQ8+bzSP5tqOPb43x7TgksTCv8+3ypN\nTUoP6DKWgAnl//nPf7R+/XqtXLlS1atX9/d0AAAAEIjWrpWuvFJDDIa+iemiB297UrJYSoylZOYq\nLTtPMeGBeeBcQITyNWvWaMKECfrHP/6hzp07u/yctWvXGn5ubwUdAAAAZURenhQZKR06ZDjc4ZEP\nlFujTqmPSM7ICdhQ7veaclvZSkxMjJ5//nl/TwcAAACB5sEHpdq1DQP53XdNUMSYrxwGckk6dvKs\nN2bnEX5fKT927JjS0tIkScHBwYbXjBgxQiNGjNDo0aM1efJkH84OAAAAfvP119IttxgOvXXVHXr5\nmqFOPa5WsN+jr11+n1m1atU0fPhww7F169Zp/fr16tq1q1q1auVWaQsAAADKiH37pGbNjMdatlT6\n0lV6+Z1Upx/LRs9SVK9eXTNmzDAce+6557R+/XoNGTJE9957r49nBgAAAJ86d07q2VOyd1bNli1S\n27aKltQpMlRrMnNNPzouMjRg68mlAKgpBwAAAPTWW1LlysaB/N13JatVatu28KPRCdEKKtlkxVCQ\nRRqVEO2hiXoHoRwAAAD+s3FjQQvDhx4qOfa3vxWsnhtUTMRHheml3pc5DOZBFmli73YBXboiSRar\n1Wr19yS8zdYS0V7LRAAAAPjY8eNSTIz022/G47//LjVq5PAxyRk5mpqUrhSDUpa4yFCNSoj2aSB3\nNXf6vaYcAAAAFczjj0uvvWY89t13BXXlJsVHhSk+Kkxp2XlKzsjRsZNnVSu4suKjwgK6hvxChHIA\nAAD4xuLF0g03GI89+qj9oG5CTHhImQrhFyKUAwAAwC0OV6n375caNza+uWlTaft2qWZN30w2QBHK\nAQAA4JLkjBxNSUo3bE3YKTJUo69tqfjHhknffGP8gA0bpHbtvDzLsoHuKwAAAHBaYuoeDZqZYrdX\neOTCjxTfKtw4kL/xRkGLQwJ5IVbKAQAA4JTkjBw9uWCT8g16+EXl7NHSmQ8a35iQULCRs1Il706w\nDCKUAwAAwClTktJLBPJqZ05p8XsP6aLD+41vysoqqB+HIUI5AAAATEvLzitRsvLPFbP10E+fGl5/\nT59n9X1UJy2uXFsxvphgGUUoBwAAgGnJGTmF//+qPRv18UdPGV4394qb9UzP82UsyRk5ZbplobcR\nygEAAGDasZNnVe/EEa1/fYDheG712uo2coaOV6tR4j7YRygHAACAOVarbn3+ET2y4jvD4VuGTNbm\nRlGGY7WCiZ2l4VcHAAAAjs2dKw0erAiDoeevHa6ZnW4v9fb4qDCvTKu8IJQDAADAvrQ0qVUrw6HU\npm11V/+XdC6o9BaHcZGh1JM7QCgHAABASadOSbGx0pYthsNdH3hPWbUbOnxMkEUalRDt6dmVO5zo\nCQAAgOLGj5eCg40D+fz5ktWqR4YlKMhS+mOCLNLE3u0oXTGBlXIAAAAUSE6WunY1Hhs2TJo5U7IU\nJPF+HVuoWb0ampqUrpQL+pZLBSUroxKiCeQmEcoBAAAqutxcKTxcOmvQtrBmzYLTOOvWLTEUHxWm\n+KgwpWXnKTkjR8dOnlWt4MqKjwqjhtxJhHIAAICKymqVBg6UPvzQeHz1aumqqxw+JiY8hBDuJmrK\nAQAAKqLERCkoyDiQT5hQENhNBHJ4BivlAAAAFcnOnVLLlsZjHToUrI5XrerbOYFQDgAA4Ct+rb0+\nc0bq3Flau9Z4fMcO6eKLfTMXlEAoBwAA8LLkjBxNSUrXGoMuJZ0iQzXa211KJk2Sxo41HvvoI+mu\nu7z3bphCTTkAAIAXJabu0aCZKYaBXJLWZOZq0MwUfZK61/MvX7OmoIWhUSC/+24pP59AHiBYKQcA\nAPCS5IwcPblgk/KtpV+Xb5XGLtiopvWqe2bF/MgRqVkz6dixkmNBQdLBg1JoqPvvgcewUg4AAOAl\nU5LSHQZym3yrNDUp3b0XWq3SvfcW9BQ3CuQ//CCdO0cgD0CslAMAAHhBWnae3ZIVe1Iyc5WWnefa\n5s/PP5d69zYeGzdOeu4555/pBg4Ucg6hHAAAwAuSM3Jcvs+p8Lpnj3TRRcZjbdtK69ZJ1aq5NBdX\n+H1TaxlF+QoAAIAXHDtpcGS9J+87e1bq2tV+IN++XdqyxaeB3K+bWss4QjkAAIAX1Ap2rSDB1H2T\nJ0tVqkjJySXHZs8uqC2PiXHp/a5ydlOrqz9JKK8oXwEAAPACV0s0Sr1v3TopNtZ47Pbbpc8+K+iu\n4geubGqljOU8VsoBAAC8ICY8RJ0inetyEhcZalxPnpcnNWhgP5AfOCAtWOC3QO7OplYUIJQDAAB4\nyeiEaAVZzF0bZJFGJUSXHHj4Yal2bSnHoNwjKamgVKVBA/cm6iZ3NrWiAKEcAADAS+KjwvRS78sc\nBvMgizSxd7vi5Rxff11wGuebb5a8YcyYgjDeo4dnJ+wir29qrQCoKQcAAPCifh1bqFm9GpqalK4U\ngxKPuMhQjSraJvC336SmTY0fdvHF0ubNUvXqXpyx87y6qbWC4FcCAADAy+KjwhQfFVb6gTrnzkk3\n3FBQkmJky5aCvuMByCubWisYQjkAAICPxISHGG/kfPtt6cEHjW+aPl0aMcK7E3OTbVOrM5s97W5q\nraCoKQcAAPCXTZsK6saNAvlNNxWsngd4ILfxyKbWCoxQDgAA4GsnTkgtWkjt2hmP//679M03fmtx\n6Aq3NrWCUA4AAOBTTzwh1awp7TU4av5//yvoqtKoke/n5QH9OrbQ3OFxirPTnz0uMlRzh8epb8fm\nPp5Z4KOmHAAAwBeWLJF69jQeGz1amjzZp9PxFlObWlECoRwAAMCbsrPtr3w3aSKlpRWsnJczdje1\nwhChHAAAwA63Vnvz86W//73gECAjv/wiXX65x+aKso1QDgAAcIHkjBxNSUo3bPHXKTJUo4se9mPk\nvfek4cONx954Q3roIQ/NFOUFoRwAAKCIxNQ9enLBJuVbjcfXZOZq0MwUTezdruSGxa1b7R/wc+21\nBXXllSp5dsIoFwjlAAAAf0nOyCk1kNvkW6WxCzaqab3qBSvmJ08WtDdMTze+Ye9eqVkzz08Y5QYt\nEQEAAP4yJSndYSC3ybdKU5PSpaeflqpXNw7kX35Z0OKQQA4HWCkHAABQwaZOZ46Jv2rPRn086Snj\nwfvvl956q+C0TsAEQjkAAIAKSlfMqHfiiNa/PsDOYD1p1y6pdm3PTQwVAqEcAABA0rGTZ0u/wGrV\nWwtf0s1pq4zHf/5Zio31/MRQIVBTDgAAIKlWsP21ytu2LNOul281DORrHn6qoG6cQA43sFIOAAAg\nGfYdj8jdp+XvjjS8fm2T1uo7YJL+949rvT01VACEcgAAABUcC98pMlRrMnNV5dwZfTn7MbU5uMvw\n2vj739O+Og0VFxnKUfLwCEI5AAAByq0j3uGS0QnRSrnnUT3+4weG4/ff9qS+bRUvSQqySKMSon05\nPZRjhHIAAAKM20e8e1m5/WZh1SrFx8cr3mDo00uv0z9vHl3Y4jDIIk3s3c6vvw8oXwjlAAAEELeO\nePeyQP9mwWV//CGFh0tnzpQY+rNyNcU9NFtHg2sVfhYXGapRZfVrRcAilAMAECBcPuLdBwL5mwWX\nWa3SoEHSB8alKlq9WnsjL9Fj5fGnAgg4hHIAAAKEK0e8+yKUB/I3Cy775BOpXz/jsRdflJ4qOKkz\nRiKEwycCok/5oUOHNGPGDN1+++2KiopS9erVVadOHXXt2lUzZ85Ufn6+v6cIAIBXOXvEuySlZOYq\nLTvPSzM6z5VvFgJWZmZBXbhRIL/iCunUqcJADvhSQITyTz/9VCNGjFBKSori4uL06KOPqk+fPtq8\nebPuvfde9e3bV1aryX8bAABQBpk94t1T95kVyN8sOOXMGalTJ+nii43HMzKkdeukqlV9Oy/gLwER\nymNiYvTll18qKytLH3zwgV566SW999572rZtm5o3b6758+drwYIF/p4mAABe4/CIdw/fZ1agfrPg\nlJdfLgjbqaklxz78sKC2vGVL388LKCIgQnmPHj106623Kiio+HQaNWqk+++/X5K0fPlyP8wMAADf\nKO2Id2/cZ1agfrNgSmpqQanKmDElx+66S8rPl+6+2/fzAgwE/EbPKlWqSJIqV3Y81djYWMPPt27d\nqjZt2nh0XgAAeJKrGyO9vaEyUL9ZKNWRI1Lz5lKeQQmNxSIdPCjVr+/7eQGlCIiVcnvOnj2rOXPm\nSJJuvPFGP88GAADvsR3x7gxfHPHui28W0rLzNCs5U68npWtWcqbr9ehWqzRihFS3rnEg/+GHgtVx\nAjkCUECvlI8dO1abN2/WzTffrBtuuMHh9WvXrjX83N4KOgAAgWR0QrQGzUwx1enEV0e8275ZcGaz\np9lvFjxxGJHtdNGGSf/T38Y9ZHzRM89I//mPqbkD/hKwK+VTp07Vq6++qtatW2vu3Ln+ng4AAF4X\nHxWml3pfpiBL6df5+oj30QnRDudkY/abhcTUPRo0M8Vu2LcdRvRJ6l7D8eSMHPWdtlpDxs/XsK4X\nGwfyNm2kkycJ5CgTAjKUv/nmmxo9erTatm2rZcuWKTTUuR/nAQBQVvXr2EJzh8cpzk4pS1xkqOYO\nj/PpqZme/mbB2cOILuzkkpi6R0PfXaV/vDBCq98eZnhvwoh39Mns76Rq1Up/CRAgAq58ZfLkyXrs\nscd06aWXKikpSQ0bNvT3lAAA8Kn4qDDFR4UVlmYEwhHv/Tq2ULN6NTQ1KV0pBqvbcZGhGmVQbmL0\nNbhzcmlyRo62PfWi0pdON7z+iZsf1WeXXSepDJ0uCijAQvmkSZM0duxYtW/fXkuWLFFYGH+IAAAV\nV0x4SEAd8e7MNwul1Ys7y3YYUcxvGYrv0EHxBtd8F32V7r/9KVkt54sALgz0QCALmFD+/PPP69ln\nn1VsbKwWL15MyQoAAAHK0TcLial7TJWnmFXj9J9q3jpSOnzIcDz24Xk6VLOu4VhhoA+gb24AIwER\nymfPnq1nn31WlSpVUrdu3TR16tQS10RERGjo0KG+nxwAAOWcJ8tkzNaLm/Xcknc0dN1XhmP9+72g\nVRHtTc2JUI5AFxChPDMzU5J07tw5TZ482fCaa665hlAOAIAHeaIl4YWcqRcvTfcdqXr/s/GGY+/E\n9dHE7sYbPI1s2XfU/QkBXmaxWq0e+l42cNn6lNvrYw4AQEVjpsTE1k3FbKeXtOw89XztB7fm1TDv\nkNa8NcRwLK9xc11592SdquJcRxWLpEl9zH8dgDtczZ0B2RIRAAB4j7stCUt7rquC8s9p7sf/thvI\ntXmzfl+/xelALklWOfd1AP5AKAcAoIJxpSWhGcdOnnVpPgPWf6Od/9dL3Xb/UmLsyRse1k2vrZAu\nuaTwdFFXOPN1AP4QEDXlAACgdJ7ajJmWned0m0KzHUxqBTsXK2IO7tLi9x42HFseGathd44raHG4\nP6/w/aMTojVoZopLdet0YkEgI5QDABDAPL0Z09USDjMdTMzOI/jMSSW9+4Ca5h00HO/40BwdrFV8\nRdz2ftvpomPnb5Irm+LoxIJARfkKAAABKjF1jwbNTLG7sr0mM1eDZqbok9S9pp/paomJmfvMlJc8\nuew9bfvvHYaBfMid4xUx5qsSgfzC9/fr2EJ9YpuZmHVJrn79gLcRygEACEDe2ozpbImJs/eNTohW\nkKXk5/G7ftGuSbdo5JoFJcZmxd6qiDFfacXFsabff0mT2qbm4+g5QKDgn0wAAAKQK5sxzZSPuHrk\nvNn7bOUltm8owo7/oZ/fGGR4bXatUHUfMV1/Vg12+v3e/joAXyOUAwAQYLy5GdNWYuLM8+MiQ52q\nw+7XsYWa1QlW8B29Fbsp2fCam4dO1a/hF7v8fl98HYAvEcoBAAgw3tyMKcmpDiZBFmlUQrThWNGO\nMCfOnJVkUY0qldQ+6XN1e2mM4T0HXnhZ6f2GadvMFJnZqVna+z31dQCBgFAOAECA8eZmTKlkiYk9\nthM9Lyz5sNcRpuWhvUqa8YDxw665RkpKUsNKldRQcuv9nvo6gEBCKAcAIMB4ezOm9FeJSb0ampqU\nrhSDEpC4yFCNMmi3mJi6p0QIrnb2tL6Z9Yha5u4zfNeiRSm69ZZOHnm/p74OINAQygEACDC+2sQY\nHxWm+Kgw0wcTGXWEefyHuRq1OtHw+ff2fkZLo+MUlHxQoa1zDDdrOvN+T30dQCAilAMAEGB8vYkx\nJjzE1L1FO8LE7dmkxI+eNLzug/Y36umeD0mWgt6IjrrDmH2/I556DuAPhHIAAAKQs5sYe3doqlnJ\nmV5bJbZ1hKl34ojWvz7A8Joj1Woq/oFZOlatRokxjrgHSkcoBwAgQFxYfjH6umhNWVp6v3KLpIvq\n19SY+ZtKjHWKDNVoD9VTJ6cf1BsLJ+qW7SsNx28d/Jo2NS69u8kb32do6t1XuD0XoDwilAMA4Gf2\nuplIUutGBSvL2/bnlRiLDKupXYeOKzPnuOFz12TmatDMFE3s3U59OzaXVDL4m1pR//BDDRtgvDr+\nYvd79G5c79Lv/8uiDb+pX8fmbLoEDBDKAQDwI6NuJkVt25+nIIv0+PUxCgmuXBima1StpCcXbJLV\nQXlLvlUau2Cj/jhxWknbDhgGf7sr6hkZUrTx6vf6xq1054BJOlvJfJSwyvzJo0BFY7FaHf1xLvti\nY2MlSWvXrvXzTAAAOC85I8epuvG5w+MKA23faaudPvXT0fMLV9RPn5Y6dpQ2bjS8tuv9M5VVJ9zl\ndy1+7Gqna8vprIKywtXcyUo5AAB+UrSbiSNFO5jYNl16km1FPe7Dt3TR5ImG17w64gW9Htre7XeZ\nPXnUdq290h5P1swD/hbk7wkAAFARuRKsbR1MkjNyPD6fDllbtXPiLcaBfMgQKT9fV/1rpIIs7r/L\n7Mmjial7NGhmit1fJ1vN/Cepe92fFOBnrJQDAFAKb5VNuBqsbXPxlNonjyn1jYGqds7gmcHB0m+/\nSfXqSTJ/rL0jZk4eNTqoyIhthb9pveqsmKNMI5QDAGDA22UTrgbrXTnHFRFW0+X3FrJa9erX/1Wf\nLcuMx1etUtrFlyr51xwdO5lT+A2Jo2PtzTDz6+ZqaQ9QVhHKAQC4gKOOKEatBp1lZrXYyOzVu3Vp\n09ou3Wtz07aVevsL47rxV7oN1P6Hn9CejSe05osfSoy3CK2hTpGhuvHSRhreNVIvfL1Ve3JPmH63\nmZNH3SntYfMnyipCOQAARfiqbMKdVd3N+466dF+zI9la+c5ww7FfG0aq1+D/6kylKrKsy5K9L39P\n7gntyT2hz/5qLNG6UYgskt3riwqySKMSSj9gSHKvtIdQjrKKUA4AQBG+KpuICQ9Rp8hQj3dRMVL5\n3Fl99sG/1P73NMPxa+6brt31mhT+vTPl4tv258kiOQzmtpaLZn6tXC3t8WStPeBrdF8BAOAv7pRN\nuGJ0QrRHupmU5r6U+cp45TbDQD7q1icUMearYoHcFVZJspw/ffRCcZGhmjs8znSpj6ulPa7eBwQC\n/ukFAOAvvi6b8FQ3k3bN6mhj1pHin/2epi/nPG54/aLW3fTI3/8lWTz3HYHVKtWpXkWLH7va7W41\nrpb2sNETZRmhHACAv/ijbMIT3Uxuv6KpXrnzciVn5OjUocO6p//Vqnr8mOG17Ud9qMPVS24UNVsX\nXhrb/IfFR7r1HFdKe8xsIAUCGeUrAAD8xV9lE/FRYUoc2VlDulzk0v3HTp5VTMNaGjb7Jd3fq4Nh\nIN/0wRfq984qw0AeFxmqPrHNXHr3hTx1sJEzpT1mN5ACgYyVcgAA/uLvsomI+q71Hw9d+j/putHG\ng//+t/T887pMUqLsH4Y0KzmzsKOKOzy12dJsaY8zG0iBQEYoBwDgL74qm7AXjJ0Nlo2O5uint4ca\njp2OitbH077UEWsl1UrOLHyH7a+ic5mVnKldh4479W57nPmpgaPTUh2V9sRFhmqUm4c4AYGCUA4A\nQBGjE6I1aGaKqY2XzpZNmDkl1Mw3BZXyz+nDj59W3N7NhuOPPvehFv5ZW1q8w/Ad8VFhpc7FHWYC\nsjOnpcZHhSk+KsxhgAfKOovVanV3X0fAi42NlSStXeuBn8sBAMo9Ryd6SufLJsy2+TP7zOFdIzVz\nZabd64asXaTxS6cZjk3sO0bTL+7m8B29OzTVgnX73Or4YiQuMlSJIzuXeo03fm2BQOJq7mSlHABQ\n7ri7qurpsglnTgmduTJTw7tGasbKTBVdNmubvVPfvD/K8L4lUXG6r/fTslqCHLZQybdKn63dZ2re\nzjDzUwNfnZYKlEWEcgBAueFMWYQjniybcPaU0I1ZRzQwroXm/rRHNU7/qRXTR6jB8cOG18c+PE+H\natZ1aj6eZnazpa9OSwXKIkI5AKBccFQWsSYzV4NmpjhdFnHhxsiizAR2V08Jbd0oROOWTtOwtYsM\nrxnQ7wUlR7R36rmuur5NuNIO5Gn3oRMlxsz+1MCd01KpHUdFQCgHAJR5vi6LcGZF3pW+3d13/Kzx\nk24xHJvWqbdeuvYep5/pji5R9fXukCvd+qmBr09LBcoaQjkAoMzzZVmEsyvyzvTtbnAsV6lvDjYc\ny6rdUNfd+5ZOVgl2ZdpusX0Npf3UwOwzfHUfUNYQygEAZZovyyJcWZE307fbYs3X+58+p2sy1xmO\n97znDaU1iHBqrp7k7oml7jzDE+8GygL+SQcAlGm+LItwZUX++dsuLfW6/r/8TxO+e9NwLOM/r6jn\nidYeb13oLE9stvT3aalAoAvy9wQAAHCHr8oiXF2RlwrqzC8Uc3CXdk26xTCQb2jbSTp7VlHP/EMv\n9b5MQZbS3xNkkSLDajo1N7NcObHUiO20VH+8GygLCOUAgDLNV2URrq7Iz129S6MToguDdfCZk1r5\n9j1a/N7DhtfHPTxHx774WqpUSVJBz/S5w+MUZyfQxkWGau7wOL1w26UOw7uznD2x1JGivw6+fjcQ\n6ChfAQCUab4qi3B1RX7eT3t0WdO6eqn3ZTr8yGMambLA8LqhdzynH6KuNOz3bbZn+ku9LzN1WqaZ\nEz3N9h53RnxUmOk5evrdQKAjlAMAyjRbWYQzpSWulEW4uiJvlfTlf+do3sf/NhyfFXurxl83smDF\n20G/b0fdT5w5ifT2K5p57MRSZ3j6tFSgvCCUAwDKvNEJ0Ro0M8XUhkhXyyJcCYn1jx/W2jcGGo79\nWa++Zs9Zqmr16mixC6eE2mN2Vd2TJ5Z6a45ARWKxWq1+3tPtfbGxsZKktWvX+nkmAABvcdQ/XDpf\nFuHMiZ5F9Z222tSKvMWar2mfT1DP9J+ML1i3TrriCpfmACCwuZo7WSkHAAQkZ1dRfVEWYWZF/o5N\nS/XKN5MNx34a/ayumjze5fcDKL8I5QCAgOLMEfYX8kVZxEX1ayoz53iJzy8+lKXvZ9xveE9K80vV\n/64X9egNbXSVR2YBoLwhlAMAAoazR9jb485x8M7OrdrZ0/pm1ii1zM0yvK/zA7P0e+0GkjidEoB9\n/NsBABAQkjNyNGb+JofXFT3C3lcdOpIzcgwD+eM/zNWo1YmG94zo/W8tiS6+Lk5HEQD2EMoBAH43\nbcUOvfztNtPX246w91XInZKUXiyQd9q7WZ98ONbw2g8vv1FP3fCQZCl+Sg6nUwIoDaEcAOBX//jk\nF81ft8/p+1Iyc5WWnef1oJuWnVdY3173z6P6ZWp/w+uOVq2hLg++r2PVapQY43RKAI4QygEAfjNt\nxQ6XArlNckaO10N5ckaOZLXq9S9f1q3bfjS85u+D/6uNjWMMxzidEoAZhHIAgN/8d0maW/cfO3nW\nQzM578LOLSELPtGu/xqXqkzoPkzT4/rYfRanUwIwi1AOAPCLKUvTdOpsvlvP8GQ3kwtbMbb443f9\nMH2E4bW/NI7RHQNe1tlK9t//QPeWGnNj68K/5/RKAKUhlAMA/OL9VbvcfoanVqCLtjuscu6Mvpjz\nuNoeyDS8tuv9M5VVJ9zhM2+/oqkk9/quA6g4COUAAJ9Ly87THyfOuPUMT3UzKdru8KFVifrnj3MN\nr3uw11h907qrU3PzVN91AOVfkL8nUFRWVpbuueceNWnSRNWqVVNERIQeffRR/fHHH/6eGgDAg5Iz\ncty63yLPdTOZkpSu9llbtWvSLYaBfP6lPRTxr0WmA7mt04q93uYXsvVdd/fXBEDZFjAr5Tt27FCX\nLl104MAB9erVS61bt9aaNWs0ZcoUffvtt0pOTlb9+vX9PU0AgAe4u0Hz+rbhHin5yEjbqzkPd1fw\n2dMlxk5VqqyOD8/T0eBapp9XtNNK32mrHQZyG1/3Xae+HQg8ARPKH3zwQR04cEBTp07VI488Uvj5\n448/rtdee01PP/203nnnHT/OEADgKe5u0Ozc0s1FGqtVGjZMUbNnGw73HvB/WtesjVOPLNpppWhv\nc7N80Xed+nYgcFmsVqvJ7+O9Z+fOnWrZsqUiIiK0Y8cOBQWdr6rJy8tT48aNZbVadeDAAdWsWdPp\n58fGxkqS1q5d67E5AwBcl5adp56v/eDy/Ysfu9r18PrZZ9KddxoOvdp1gF6Pv9vhI4Z0vkgRYTXt\nrjTPSs7U+EW/Oj21cbe21bD4SKfvM8NRfbt0fqWf+nbAda7mzoBYKf/+++8lST179iwWyCUpJCRE\n8fHxWrx4sX766SclJCTYfY7tF+FCW7duVZs2zq14AAC8JyY8RJ0iQ51eTZakkODKOph3yvlQvnu3\nFBFhOLS1QYT+PuQ1nalUxdSjIsJqlhqeXS3P8UbfdUlO17c3rVedFXPAxwJio+f27dslSTExxqeh\nRUcXbOZJS3PvkAkAgO+kZedpVnKmXk9K16zkTKVl5xUbH50QrSCL88/NO3lWg2am6JPUveZuOHNG\nuuoqu4H8mvum66Z73jAdyCXHrRhdLc/xZN/1oqYkpTtd3w7AtwJipfzIkSOSpDp16hiO2z4/fPhw\nqc+x92MCeyvoAADPM1u3HB8Vppd6X2ZqBfdCpld0X31VeuIJ47F586QBAxQ+bbV2O7Fib6YVo6ur\nzN5YnQ7U+nYAxQXESrkjtrJ3i8WFJRUAgMNVa09JTN2jQTNT7IZAW19u2yp3v44tNHd4nOIiQ51+\nV6kruj//LFksxoG8b18pP18aMECScyv2tnaHjtjKc5zhqb7rF3K11SItGgHfCoiVcttKuG3F/EJH\njx4tdh0AwBxfdttwtW7Z9tfiLft131znNkaVWNE9elS66CLJ3k9WDx6Uwop/vWZX7Iu2OzRjdEK0\nBs1MMfVTALNh3xWBVt8OwFhArJS3atVKkv2a8fT0gpUQezXnAFCR2VsFd3bV2l3u1i3vO/ynS+9N\nzsgpaHF4//1SnTrGgXzZsoJrwowDtaMV+7jIUM0dHudUVxJb2He0Cu9s2HdWoNW3AzAWEH/irr32\nWknS4sWLlZ+fX6IlYnJysqpXr66rrrrKX1MEgIBT2ip460Yh2r4/T44ysqe6bXiibtnVldmwZYul\nrvcbDz79tPTCC6aeY1ux9+TBOv06tlCzejU0NSldKQa/PkV7m3tLINW3A7AvIEJ5y5Yt1bNnTy1e\nvFhvvvlmscODxo0bp+PHj2vkyJEu9SgHgPLIUc/pbfvN14x74jRJd+qWbYHX2ZXZRkdz9NPbQ40H\nY2KkDRuk4GCn5xQTHuLR2m5vhH1nuNJ+0lv17QDsC4hQLklvvfWWunTpolGjRikpKUlt2rRRSkqK\nli1bppiYGL344ov+niIABASztdvOcLfbhifqlpvWrW7qnqD8c/rw46d11d7Nxhds3Sq1bu3SfLzJ\n02HfGYFS3w7AvoCoKZcKVst//vlnDR06VCkpKXr11Ve1Y8cOjRo1SqtXr1b9+m4eqQwA5YQztdvO\ncKfbhifqls3UlA9eu0g7/6+XcSB/772CuvEADOT+Fij17QDsC5iVcklq3ry5Zs2a5e9pAEDAcqV2\n2yx3um14om65tPe3ObBT/5s1ynBsZ+ceunjlEikoYNaZAlIg1LcDsC+gQjkAoHTe7B3tTrcNV+qW\n2zQqXs5h9P7qp09qxfQRanj8D8NnXPnwXD10V7wuJpCb4u/6dgD2EcoBoAzxZu/oA3mnStSVOxPe\nnKlblgo2o36SurewzeCFK7TPLp2ue9Z+aXjvwL7Pa2XkFYb3wTF/1rcDMEYoB4AyxJu9o99evkNv\nL9+hTpGhSmjdUEnbDjh16JCtbnns/E0OWzFKklXF2zHaVtuDk5ZozqfjDO+Z3vF2TegxvPDv6RIC\noLwglANAGeKLVeE1mbmllqHYDh2a2LtdicN0mtWrYSqQ2+RbpYn/26pFj3ST9u/XJ/d3Mbwuq3YD\nXXfv2zpZ5XyLQ7qEAChPKMIDgDLEtprsb7ZDhy6scX/pf1udftbmrMNa2/YqqXFjw/Eb7nlDXR+Y\nVSKQ0yUEQHlCKAeAMmZ0QrTD1nY2FkvB6Z7eYDt0yCYtO0+b9x116hl3/fKtMl/+u2K3ppQYe7f/\nPxUx5ittbxBR7HNXjrwHgEBH+QoAlDG22m1HBwjZVpPbt6irnq/94JW5FD10yJnOMNEHd2vJew8Z\njv0R3131VizViEqVdA1dQgBUEIRyACiDnOk5PSs506tzSc7IUUx4iKnOMNXOnNLSmQ+q+ZFsw/FO\nD85WZLtoJVaqJIkuIQAqDkI5AJRRZntOe7ONYtHnO+oMM3b5LN2fMt9wbOgd47S8ZUdJ0oEiq+8A\nUFEQygGgjHO0muzNNopFn29v02Xn3Rv00cdPG47N7vA3jbv+gRKf21bfAaCiIJQDQDnn7Q4ltudf\neKpn6IkjWvf6AMN7cmrU0dUjZ+hE1eqG495e3QeAQEMoB4By7sKw7EntmtVRckaOvtu8X7WCK6tP\nh6Zam5mjtxZM0A3pPxne87ehU7QlvGWpz/X26j4ABBr+rQcAAcKZI+2dNTohWoNmppTarcUVG7OO\naGPWkcK/77MpSTu+ec3w2vEJIzTryl6mnkv/cQAVDaEcAPwsOSNHU5LSnTrS3llm2yi66uJDWfp+\nxv2GYynNLlH/uyfoXFAlU8+Kiwz1Sj25N7/pAQB3EcoBwI8SU/eUGpRLO9LeWY7aKNrERYaqR+uG\n+n7bgVKvk6SqZ8/om1mPKCo3y3D8y0WrlXq2pvJ/2mNqjkEWaVRCtKlrzfLFNz0A4C6L1Wr1wppJ\nYImNjZUkrV271s8zAYDzkjNyTJeUBFmkucPjPBYebavGuw4dV/aRkwqvHayIsJolVo8vXF2evy6r\n8NTOR1d+oEeTPzJ8/n23P63FMZ0VFxmqxJGdHX7zYfsaPfHNR1H+ei+AisvV3MlKOQD4yZSkdNOl\nJLYj7T0Vys0eylP0urTsPI1f9Ks67t2sTz8ca3j9R+166skbH5EsFknnT/x05rAjT0nOyDFVrpNv\nlcYu2Kim9aqzYg7AbwjlAOAHadl5TndDSfHzoTqp6zK0c9KtClLJlJtXtbq6PPi+8qrVLDFm6zlu\n9rAjT/HnNz0A4CxCOQD4QXJGjkv3vfTNVl0d08C3mxStVql/fw34+GPD4V6DXtWGJq3s3n5hz3Gz\nq/TuWLxlf5n7pgdAxUYoBwA/cPVwnGXbD2rZ9oOSfLRJ8aOPpP79DYde6j5U0+LucPgIX/YcL21T\np9n7CeUA/IFQDgB+4Img6snOLDa20pKgnTs1ZPB1htf80jhadwz4P52tZO5r8FVJiJlNnY5wkigA\nfyGUA6jQ/NW72lNB1VObFG0rzOszsrVwzj90yYGdhtc99MJn+jov2PRzvdVz/EJmN3U6wkmiAPyF\nf/sAqJBc6V3tyQAfEx6iTpGhLpdZFOXuJkXbCvP9qz7RJz/MMbzm4V5jdPXTD6l/ver6nxNtHD3d\nc9weZzZ1loaNngD8hVAOoMJx9sAebx0+MzohWgNmpDh9nxFHmxTtfUORnJGjxNc/1c65Txjet+CS\na/X43x6XLBZ9s2Cj5g6PM3UyqK33ty9CriudbIz4alUfAIwQygFUKM72rk4/kKeZKzO9cuJmg5Bq\nTl3viNEmxdK+oegeXkXvjO2lBadPlhg7HVRZVz4yT0eDaxV+ZluRTxzZ2ec9x0vjaiebony5qg8A\nRgjlACoUZ3tXv/tjpqnrXKnr9kSYLOrCTYp2fyJgter/vpmiOzcvNXxOnwEva22ztoZjthV5X/cc\nL427mzN9uaoPAPYQygFUGJ4qczDiSl23pzt91AquXBiSt+w7qs/WZZW4pmXOXi2Z+aDhAUD/7TpA\nU+PvdvieQGsb6M7mTF+v6gOAPYRyABWGp1emL+Ts4TOe7vQxf12Wxi/61XCs9sljGp38kQav+6pE\nIN/aIEK9Br+m05WrmHrPsZNnvVZn74qmdau7dN/0QbHqeUkjD88GAFxDKAdQYfiiB7Uzq8ieDq2b\n9x0t8ZnFmq87Ny7Vv36YrbATR0qMdx8xTbtCmzr1nrQDefrvkjSDtfYCazJzNXBGiib1sV9n74my\nF3cOCoqLDCWQAwgohHIAFYYvelA7E/w92RbRSId9WzVu6XRdvj+92OcpzS/Vc9fdp60NL3bpuYs2\n/O7wGqukMfNL1tl7aoXdnYOC2NQJIBAF+XsCAOArviincDb4j06IVpDFs3NomHdIr371qhbM+2ex\nQL4vpIEe+vsY9bv7JZcDefUq5v+zYZX0/Ffny2kSU/do0MwUu9+E2DrZfJK6t9TnunNQEJs6AQQq\nVsoBVBjeXpmWnA/+8VFhpvp+G2nXrI42Zp0vSal69ozu+fkLPbw6UbVO/1n4+alKVfRO3B16+6o+\nOlnF/GmcF7JI+vNMvlP3bNufp7TsPB3MO+VUK8rSOtm4elAQmzoBBDJCOYAKZXRCtAaZPJHSWa4e\nPtOvYwuHfb97d2iqE6fPFavBTs7IKQzl1+5I1bNJ0xX5R/HSkv/FdNGL196jrLru1U8HWaSrYxpo\n+faDTt/7+fosrd192KlWlPY62bjaQYdNnQACHaEcQIVidmU6yCIN7xpZ6sFBF17vTp2yK32/v9u8\nXxcfytIz37+ra3euLTa2PayFxifcp1UR7U29v3ZwZbVpXLvUw4Bm/LjT6a9Lktbu+kNrdv3h1D32\nOtm42kFn3+E/HV8EAH5EKAdQ4ZhZmbaVOUQ1rOXTI+VjwkPMrbYfParuM/9PIxNnqWr++c2lR6rV\n1GtdB2jeFTfrbCXz/4o/evKsRiVE6/ips1qwLkt5J88qJLiyendoVrjC7Goo/+PEaZfuM+pk42oH\nHV903gEAdxDKAVRIZlemnQnwPpGfL82dK40Zo8uys89/LIs+vvwGvXL1IOXWqOPSo++ft1Z5F4TX\nb7dkF3ZFad24tpa5UL5Sr2ZVScedvs8oSLvaQccXnXcAwB38WwpAhWZmZTpgjpRPTZUeeURKSSn2\n8c9N22jcdSO1pVGUW4+/MJDb2LqiPHpdjEvPjb2ontZkOle+IhkHaVe/+WFzJ4BARygHAJNMl5Z4\nWna29OST0qxZxT9v0kTb//Gs+h5oqnx5uK/iBfKt0mtL0tSodrD2Hz1p+r42jUJ0+xXN9PZy50tf\njIK0Kx10XN2ACwC+RJ9yAAhUp09Lr74qxcQUD+RVqxaE9O3b1erxkXqpTzuP9zo3YpWcCuQWSf++\npW1hkHZGaUHamd7uHBQEoKwglANAIPr2W6ldO+mJJ6SjR89//ve/S7/+Kk2YINWqJamg7n3u8DjF\nORl8vclikSb1Ob/51ZNB2tZBx9HzOCgIQFlC+QoASP6vF/9L5k+/qNITT6hFclLxgVatpClTpBtu\nMLzvwrr3XYeOa/aq3T6YcUlGm1+daUVpJkgH3AZcAHAToRxAhZackaMpSemGNcq2riO+CHarN+zS\n/jHP6ualH6naufMbLo9WraHP/z5c0ePHqkvbJg6fU7Tufevvrh2044zLmtZW7w7NTH0z4+kgHTAb\ncAHAAyxWq9UL59oFltjYWEnS2rVrHVwJoCJJTN1jeuW2b8fm3pmE1aqfXpiqiJf/o0bHigfVxMuu\n1/9dM1g5Nes5PY+07DzNXb1b837aLW//S37xY1c7HYIJ0gDKK1dzJyvlACqk5Iwch4FcKug6MnbB\nRjWtV93zK+br1unofQ/oqrVrin28vnErPXfdfdrQpJXT8yht5d9bjA75ccRvnWwAIEARygFUSFOS\n0h0Gcpt8qzQ1Kd1zofzgQenpp6UZM1S7yA8rD9Ssp4ndh+rzS66V1VJyH76jeUxJStPkJelOrYyH\nBFe225/cLE7LBAD3EcoBVDhp2c7XWqdk5iotO8+91d0zZ6S33pLGjZOOHCn8+HRQZb135d/1Rpe7\ndKxaDafnkZyRo+e/+lXb9ueZmoZF0sCrWmhQ5wgdzDulQTNTTH+DYoTTMgHAfbREBFDhJGfk+PQ+\nSdLSpVL79tKjjxYL5N9ffKVuGP6mJl57j8NAbjSPxNQ9GjQzxXQglwr6jadlH1NMeIjp9oKlocMJ\nALiP5Q0AFY6r5Ra7Dh3XrORM5zYnZmZK//iH9PnnxT+PjtaiIU/okbymTs/DNn+zdfFGiq64O+qK\nUhpOywQAzyCUAyhTPNG1w9VyC6O+33bbJh4/Lk2aJL38snTqVJGX15KeeUYaPVo5P/8mLfrV6XnY\n5u9MXbyRohs0be0FE1P3aOz8Tabq0jktEwA8h1AOoEzwZD9xT5ZbrMnM1aCZKefbFVqt0iefFJzE\nmZVV/OLBg6WJE6XGjd2ah603t7sdVrbsO1ris34dW0iSxw75AQCYQ005gIBnq5u2F0JtwfiT1L2m\nnhcTHqJOHjyS3taucP2i5VL37tJddxUP5LGx0qpV0uzZhYFcktbv+cPpd9nKRdyqb/9L6m7jX89+\nHVto7vA4xdn5NYqLDNXc4XHe690OABUQK+UAApq3+on36dBUqZm5HjlYp+6fR/WPH+fp8knfStb8\n8wMNGhSsjA8dKgUVXwOxfV3OKFou4ok2hLsPnbDbUYbTMgHAtwjlAAKap/uJe/JwnUr553T3L9/q\niR/nqu7JY+cHKleWHnlEevZZqW5dw3tdqQcvWi7iqTaEjg7+4ZAfAPANQjmAgOXpfuKJqXtc7lZy\noav2bNS4pdPV5uCuYp/v69RNTd+fJrVpY/deV+vB27eoW/j/PVXLbVRXDgDwPWrKAQQsT/YTd6Z9\noEVS54vrG441OXpAbyycqI8/eqpYIN9dt5FG9P63Frw4o9RAbm9+ZhS9z1N18fbqygEAvsVKOYCA\n5WrdtNF9zpSLWCWt3nmo2GfVzpzSyDUL9MBPn6n62fMtDk9UqaY3OvfTzI636VTlqupSvYpL8zPj\nwvtGJ0S7fRpnaXXlAADf8ftKeXp6uiZNmqQePXqoefPmqlq1qsLDw9WrVy8tW7bM39MD4Eeu1k1f\neJ9b7QOtVt24PVlJMx7Q4ys/KBbIF7a9Rj3unaa3OvfVqcpVJZkrK/HU1+WJ0zglN08qBQB4hN9X\nyp955hklJiaqbdu2uvnmmxUaGqrt27fryy+/1JdffqkpU6Zo1KhR/p4mAD9wp493Ua6GzpiDuzQu\nabrid28s9vmWhhdr3PUj9XOzS4p9bvZ0S099XZLcOo3TxhOdXAAA7vF7KL/xxhs1ZswYXXHFFcU+\nX7Fiha6//nr985//1J133qnGRXr7AqgYbHXTzqxyGwVjZ0Nn7ZPH9NjKDzRo3deqXKTFYW712nrl\n6kH6uF1P5QdVKnaPM6dbeurrsrG1L5z07Va9vXyn6WfaeKqTCwDAdX4vXxk6dGiJQC5J11xzjbp3\n767Tp09r1apVfpgZgEAwOiHadHmGRcbB2GzoDPqrxeHy6fdp2NpFhYH8rCVIs2JvVff7puvD9jeV\nCOQWF063dObrMhv4b7+imen3F8WpnADgf34P5aWpUqVgw1TlyqziABWVM3XTVhVs6LywXMVM6Lwy\na4u+nPO4XvruDYX+eb5NYPJF7XTzsKkaf91IHQ2uZfxeq/TZuiynymTMfl3OHGfvSkcWsyU3AADv\nslitVk8caOdxu3fvVqtWrVSpUiVlZWWpXr16Du+JjY01/Hzr1q1q06aN1q5d6+lpAvCR5Iwc03XT\ntiBb9Bj4vtNWG5aLNDqaoyeXz1KvrSuKfZ5Vu6Fe6DFc38Z0KVgKN8HovY6U9nXFRYZqVEK0UyvZ\nyRk5pjuyBFmkucPjWCkHAA+y5VFnc2dALkGfOnVKAwYM0KlTp/Tyyy+bCuQAyjdbcBw4I0WO8ma+\nVRq7YKOa1qteeN+F7QOrnT2t4akL9fDqRNU4c76jyp+Vq+ntq+7QtE69dapKNafmaPReM1+XJ4+z\nt63AO+rJ7swKPADA+zyyUh4REaHdu3ebvn7AgAGaN2+e4di5c+d0991369NPP1W/fv300UcfyWJy\nlcoeV79jAeAdrgbQa19Zrsyc46bfExcZqsSRnQv/PjF1j56cv1EJ6Sn69/czdNHh/cWu/6p1N024\ndph+q93Q/Bdj4r3+4OkVeACAOX5dKW/ZsqWCg4NNX9+kSRPDz8+dO6eBAwfq008/Vd++fTVv3jy3\nAzkA37MXupMzcjQlKd2wjKRTZKhGlxIUpySlORXIJSklM7fYwTj9ah1Xz+RXVW/l8mLXbW0QofHX\n3aefWrRz6vlm3+sPnl6BBwB4l0dCeVJSktvPOHv2rPr3769PP/1U/fv315w5c1SpUiXHNwIIGKWF\n7siwmtp16Ljs/WxuTWauBs1MKVGTXdozzc4pJjhfGj9eev111Tt7vj3iqZA6Wj10tHb1Gajba1TT\nGgclH06/NwDCb0x4SEDMAwBQuoCoKT99+rT69u2rL774QoMHD9asWbMUFBTQjWEAXCAxdU+pdcxm\nVrkvrMl29ExHLNZ8Nf/8Y6n3ZOnAgfMDQUHSyJGq9vzz6l6/frF73HlfURzIAwBwht9D+alTp9S7\nd2998803Gj58uKZPn04gB8qY5Iwcj4XZfKs0NSldknsBucO+rRq3dLou359efODqq6WpU6XLLy9x\njydOx7ThQB4AgDP8/l+N+++/X998843CwsLUtGlT/ec//ylxTffu3dW9e3ffTw6AKf/56lePlX1I\nBTXZL/1vq0vPbHAsV2NXvK8+m78vPtCsmfTKK1LfvqW2OCxaiz139W7N/cn8JvYLnwMAgFl+D+WZ\nmZmSpJycHMNAbkMoBwLTlKQ0bd+f5/Hnbt531PFFRVQ9e0bD1n6hR1YlqtbpP88PVKsm/etf0pgx\nUs2app8XEx6i7dmufV0cyAMAcJbfQ/ny5cv9PQUALkrOyNHkJemOL/Sya3ek6tmk6Yr84/din++9\n9kY1n/mWFBnp9DPTsvNc2lxqsUijEqILn0HnEwCAGX4P5QDKrilJ6Q4P8vGmyNx9eibpXfXY+XOx\nz9Pqt9C03qP06vR/uPzs5Iwcl+67tV1jSfZPEHXU+hEAUDERygG4xNWVZE+oeeqEHlmdqHtSv1DV\n/PNdTo5Wq6nXuvbX3Cv+ptkj4916h6vdU06eyS/1mHt7rR8BABUboRyAS1xdSTajXbM62ph1pMTn\nFmu+bt+yTGOXv6+Gx/8o/DxfFn18eU+9cvVg5daoI08cOeZq95Qlv2Y7/OnBha0fAQAglANwibf6\ncAdZpDE3ti5xYFC739M0fsk0XfH79mLX/9y0jZ67bqQ2N4oq/MyqgraK7gReV+81W85ja/1IKAcA\nSIRyAAZK26BoG1u35w8HT3FekEWa2LtdYVAdNDNFocf+0D9XzFG/TUuKXbu/Vqhe6j5MX7Ttbtji\n0N2j7mPCQ9QpMtSrJTruzhEAUH4QygEUKu1I+1aNQmSRtM3N9ofN6lVX1h9/lvg8LjJUo4psgIxv\nUVvz//xJUe/+VyGnThRed6pSZc3oeLve7NxXJ6pWd/j1uBN4RydEl1ofXpRF5lfJi3J3jgCA8oFQ\nDkCSHB5p74le5G0aheh/j17tuFXgd99Jjz6qK7ZtK3b/kqg4vdBjuHbXa2LqffZKbMy2KoyPCtNL\nvS9zeLJokEW6vk24vvs129S8zMwRAFCxEMoBKDkjx60j7c0Iskj/vqWtpILSEMPV4R07pMcfl778\nstjHh1tcrNFxQ7Ti4lin3rnltyPFykNK+0mAvVaF/Tq2ULN6NTQ1KV0pBvfZVvjTsvNcCuWubigF\nAJQv/NcAgKYkpXs9kBetFS/h2DHppZekV16RTp8+/3lIiDRunA70HaoVb/7k9Hu/3ZKtb7dkq1Nk\nqC5vVkczV2a61KowPipM8VFhpa6wNwip5vT8bM8GAIBQDlRw3u43fmGteDFWq/TRR9I//yn99lvx\nsWHDpAkTpEaNFCO5telyTWauqXsdtSq0u8Iv1zaGxkWGUk8OAJAkBfl7AgD8y1v9xi2SJvW5TIkj\nOxsH8vXrpW7dpAEDigfyuDgpJUV67z2pUaPCj0cnRCvIEw3IHbC1KnSFM3MMskijEqJdeg8AoPwh\nlAMVnLc2GlolLVi3r+TAwYPSyJFSbKyUnHz+8/Bw6f33pVWrpE6dStxm23Tpi2Bua1XoLLNzdFjO\nAwCocAjlQAXnzY2GxcLtmTPS1KlSTIw0fXpB6YokValSUL6SliYNGSIF2f/XUr+OLTR3eJziIkO9\nNmcbV3+C4GiOcZGhmjs8rkTdOgCgYqOmHKjgvL1am5yRo5jNa6TRo6UtW4oP3nSTNHlyQVA3qeim\ny/8u3q5vtzjf8cQMd36CYGZjKAAARRHKgQrOmydXNjuSrW5jp0orFxcfiIoqCON/+5vLz44JD9El\nTep4LZR74icIpW0MBQCgKMpXAHh8E2XwmZN67McPtHTGA4oqGshr1ZImTZI2b3YrkBc+zoulN9R7\nAwB8iZVyAKZPrnTIatXftq3UU8veU9O8g8XHBg2SJk6Umpg7jdMMbwVnWz34rORMSk8AAD5BKAcg\nyfHJlY60PpCp55ZO01V7NxcfiI2VXn9d6tzZQzM9zxulNxZJR/48o56v/VBizN6pnwAAuIvyFQCF\n4qPClDiysxY/drUe6N7S1D11/zyq/yx+W1+/P7pYIM+pUUfpE16T1qzxSiC38WTpje0x2/Ybt0O0\nnfr5Sepez7wQAIC/EMoBlBATHqIxN7ZWp1JaD1bKP6eB677WsukjNXj916pkzZcknQmqpBlX9tKP\ni1Yq+slHlXbwuGYlZ+r1pHTNSs50qf93aZzpDT6iW6TdVoWtG4VIloL+6qWxnfrprUOXAAAVE+Ur\nAOwanRCtQTNTStSZd8tcp6eWvac2B3cV+/yHiCv0n4QR2hHWQmMrVVffaasNS0s8XQbiqPQmLjJU\no4q8z6hV4b8Xbi5sne6I7dRPylgAAJ5isVrN/meo7IqNjZUkrV271s8zAfzH1Z7Zial7CjeAxmb9\nqvkf/KvENXvqhOv5hBFaEhUnWczVkthOtfT0ITqufJ1p2XmGNeSOLH7sajZ/AgCKcTV3slIOlHPJ\nGTmakpTu8op1v44tVOXoEd3Ss4Oq5hc/UOdElWp686q+mtHpdp2qXNWpednKQJrWq+7RFWdXeoO7\nWoqSnJFDKAcAeAQ15UA5lpi6R4NmptjtTuJw46LVKg0cqN7XtSsRyCWpx73T9GaXfk4HchtbGYi/\nuXp6pzunfgIAUBShHCinkjNyTPUdt7tx8ZNPpKAg6YMPStzz8tWDFTHmK+2v7f4Kd0pmrhZv2e/2\nc9zh6iFE3jy8CABQsfBfFKCcmpKUbvogoGIbFzMzpYsvNrxuc3hL3T7oFZ2pVMWDM5Xum7vWrz3A\nXX0nGz0BAJ7CSjlQDqVl5zl9oM7ajAM62b6D3UB+9X3v6pahUzweyG382QPcdgiRM+IiQ6knBwB4\nDKEcKIec3bg4MuUzZbxym4I3rC85+OGHktWqRh0u8dDs7PNnD3BnDiEKskijEqK9OyEAQIVCKAfK\nIbMbEC//bbt2TbpFTy5/v+Tg3XdL+fkF/6uC0OqhgzNL5a/Nn84cQjSxdztKVwAAHkVNOVAOOdqA\nGHLquFa9NVQhp/8sOWixSAcPSvXrF35ka6voq0MNUjJzlZad5/PyEGcPIQIAwFMI5UA5ZDc0Wq16\n6dvXdffGxYbDexd+q+a9bij2WdHDgxyJiwxVj9YNNenbbaY3mdrjrx7g8VFhio8Kc/mwJQAAXEEo\nB8oh28bFops9e6at1vTPXzS8fkqXu/TF7SP1fa/uxT4321ZRKlhgt60i161RxfR99vi7B7grhxAB\nAOAqasqBcsq2cbHx0YPaNekWw0CeXr+5Yv7xuV7rNlA7c47r2leWF9tk6UxbRWuRWvB+HVto7vA4\nxTnZ0aSoA3mnXL4XAICyhpVyoJyKj6irlP+NV4MNqYbjPe59RzvrNyv2WWbOcQ2ckaJJfdqpfYu6\nTrdVLFoLbisDWbxlv+6bu9bp+f+y9w+n7wEAoKxipRwoZ9Ky85Qy+lmpShXDQP6Pmx9TxJivSgRy\nG6sK2hLOXb3bpfdf2M6w5yWNdFnTOk4/Z9O+o0rLznNpDgAAlDWslAPlRHJGjr6Y9ZVenjDMcPzb\nmM564LYnZbU4/l483yot23bApXkY1YK3b15Hm/YdcfpZ/trsCQCArxHKgXJg/oqtuubmznr5hHHw\njX14ng7VrOvUM7MOG7RLNMGoHWPDkGCXnuXvzZ4AAPgKoRwo434bfK/6zJ1pONa/3wtaFdHep/Mx\nasfoqG+6Pa7eBwBAWcN/8YCy6ptvpL/9TU0Mht6J66OJ3Y3LWJzRrG51p1bM4yJDDctNXD1sh0N6\nAAAVBaEcKGt++01q2tRwaE+dcF0//C2dqlLNI6+6tnUDfZCyx1RbxKC/+pQbMeqb7oi9gA8AQHlE\n9xWgrDh3Trr+eruB/Pp73tTV98/0WCCXpEGdI/RS78sUZCn9uiCLNLF3u1JXtm19080oLeADAFAe\nEcqBsuDtt6XKlaWlS0sMPXnDw4oY85XSG1zk0VfaVqodHQQUFxmqucPj1Ldj81KfFx8V5rGADwBA\neUP5ChDINm2S2rUzHrvpJs168nV99PU2j7/2wpVq20FAadl5Ss7I0bGTZ1UruLLio8KcKjHp17GF\nmtWroalJ6UoxKGWJiwzVqIRoAjkAoMIhlAOB6MQJqXVrae9e4/HffpMaN1Z8dp7k4VBe2kp1THiI\n23Xengr4AACUJ4Ry4C/uhkSPhcx//lN65RXDoS8nvKtDVycoPqiWYuTaBsqLw2rqXL5Vu3NPlBjz\n5Uq1JwI+AADlBaEcFV5yRo6mJKUbBttOkaEa7SCkunt/oaVLCzZyGpgVe6vGXzdSOiJp0a/Fnj06\nIVqDZqaY7pDy/G2XslINAECAsVitVhP/KS/bYmNjJUlr167180wQaBJT9+jJBZtKDbS2cg6jjYzu\n3i9Jys6WGjUyHqoVqu4jpuvPqsYnYtqebZXV/XkAAAC3uZo76b6CCis5I8dhkJWkfKs0dsFGJWfk\nePR+5edLt95qN5D/bdhUxT00x24gL/rsZvVqeKRDCgAA8A/KV1BhTUlKN1XyIRWE36lJ6cXKUNy6\n/733pOHDjS+eOlV9q16pLSbrxG3PThzZmbIUAADKKEI5KqS07DynNkdKUkpmrtKy8xQTHuLy/bt+\n/FkRV3c0vqB7d2npUqXlnNCa135weW5soAQAoOyhfAUVUolSEifvc/b+amdP6/vp99kP5Hv3SsuW\nSZUquT03AABQ9hDKUSEdO3nWrfucuf/xH+Zq+6u9dfEfv5Uc/OILyWqVmjXz2NwAAEDZQ/kKKqRa\nwa79o2+7z8z9cXs2KfGjJ40HR46U3n5bspQ8c97duVFTDgBA2UMoR4Xk6uE4tvtKu7/eiSNa//oA\nw7Fzdeqq0p7dUu3aHp9bjaqV1Hfaavf7pQMAAJ+jfAUVUkx4iC5taj8YG4mLDC1ccbadpFmM1ao3\nFk60G8iffHKmKh3+o9RAbvfZDkSG1dSTCzbZ3Xy6JjNXg2am6JPUvU49FwAA+AahHBVOckaO+k5b\nrc37jpq+J8gijUqILvbZ6IRoBf1VfdJryzLtevlW3bJ9ZYl7X+x+jy4e+5Vuuefvpt9X9NmOWCTt\nOnTc9X7pAADA7yhfQYUyJSlNk5eky5ljbG0nYV5Y+hEfFabXr6ylv93R3fC+dU1aqW//ScqvXNnw\n/tLER4Xppd6XmTql86L6NZWZc9zUc436rQMAAP8LyFA+fPhwvffee5Kk9PR0RUVF+XlGKOuSM3L0\n/Fe/atv+PKfui4sM1SijWuzTp6Urr9TfNm0yvK/r/TOVVSfc/v0m9OvYQs3q1dDUpHSlGJSlxEWG\nqneHphoz33gO9hTtaQ4AAAJDwIXyRYsW6b333lOtWrV07Ngxf08H5UBi6h6HK85G2jWro8SRnUsO\nvPCC9Mwzhvd8//wb2tK5p4Z7qOtJfFRYqad0zkrOdOm5yRk5hHIAAAJIQIXygwcPasSIEerXr5/2\n79+vFStW+HtKKOOSM3JcCuSStDHrSPEV5VWrpPh444uHDJFmzVIPi0U9XJ+uXfZO6aSnOQAA5UNA\nbfS87777JElvvvmmn2eC8mJKUrpLgdwmOSNH+uMPqVo140AeHCzl5krvv2/Yc9zb3O1pDgAAAkPA\nhPL3339fCxcu1DvvvKP69ev7ezooB9Ky8+y2CDTFalXn5x6TQkMLasgvtGqV9OefUr16rr/DTe72\nWwcAAIEhIJbLdu/erdGjR2vgwIG67bbbXH5ObGys4edbt25VmzZtXH4uyiZ3Wv/dvG2l3vpiovHg\nCy9ITz/t8rM9ydbT3JlvPor2WwcAAIHB76E8Pz9fQ4YMUa1atTR16lR/TwfliCt1082OZGvlO8ON\nB9u3l1JSpKpV3ZuYh41OiNagmSmmynSM+q0DAAD/80goj4iI0O7du01fP2DAAM2bN0+S9Nprr2nF\nihX6+uuvVc/NMoC1a9cafm5vBR3lh1F3EmfqpiufO6vPPvin2v+ebnxBRobSajVUcuq+Eh1Q/M2Z\nnubO9ksHAAC+4ZFQ3rJlSwUHB5u+vkmTJpIKepA//fTTGjZsmG6++WZPTAUVTHJGjqYkpRuWb1zW\ntI6pZ9yXMl9PLZ9lPPjBB0ru1FNTlqZrTea2EsOdIkM12sU+5J5kpqe5q/3SAQCA91msVqsbvSnc\ns3DhQt1+++2mrv38889drje3rZTbW0lH2eRq/3Gbdr+n6cs5jxuOpcZdr46rv1Piz3tNr0D37djc\ntYl4mL2e5gAAwPtczZ1+rSmPiIjQ8OHG9btff/219u/frzvvvFO1a9dWRESEbyeHgOZO//Fap05o\n1VtDVfv0CcPx9qM+1Oi+V+n0jkOm3pFvlcYu2Kim9aoHxEq0vZ7mAAAgcPk1lLdv314zZswwHOve\nvbv279+vCRMmKCoqysczQ6Bzqf+41aoJ372h/hu+Mxzu23+i1jS/VFJBnfa/F242/Y58qzQ1KT0g\nQjkAACh7/N59BXCWK/3Hr0//Se8ueMFwbEqXu/Rat4GFfx8XGSpJTr8jJTO3+AmgAAAAJhHKUeY4\n03+88dGDWv32MMOxjNBm+tuwqTpV+XyLQ1vLQFd7nCdn5BDKAQCA0wI2lC9fvtzfU0CAMtN/vFL+\nOX340VOKy9piON7j3ne0s36zYp8VbRm4bvcfXpsbAADAhQI2lAP2OOo/PmTtIo1fOs1w7O3BT2lS\n4y4lPr+wZaAzPc6dmRsAAIAREgTKHHubKdtm79Q3748yHFsSFaeLfvhODzSuowQTLQNd3bDJRk8A\nAOAKQjm8yhs9s2PCQ9QpMrRwI2aN039qxfQRanD8sOH1sQ/PU9SlFyuxcZ3C+x3N4cJ3mBEXGUo9\nOQAAcAmhHF5R2kmbnjgFc3RCtAbNTNEzS6Zp2NpFhtcM6PeCkiPaF27edPUdZtoiuvoOAAAASQry\n9wRQ/iSm7tGgmSl2V5nXZOZq0MwUfZK61+V3xKet0c6JtxgG8nc69VbEmK8KA7lt86bT74gK00u9\nL1OQpfTr3HkHAACAxEo5PMzsSZsun4L5++9SkyaGQ1m1G+q6e9/SySrBkqSL6tfQg91bqm/H5uaf\nf4F+HVuoWb0ampqUrhSDbzIu3CAKAADgCkI5PMqZkzadOgXz3DnpppukJUsMh7/9aLEm7Kmsk7kn\nCj/bfeiExszfpPnr9rlVLhMfFab4qDCv1McDAABIhHJ4kCsnbZo6BXPaNOn++43H3nlHiR1u+mt1\n/rThJbZymYm927m1am5mgygAAIArqCmHx7hzCqahzZsli8U4kPfsKZ09q+SEPk6Vy7g6RwAAAG8i\nlMNjXD3NssR9J05IERHSZZcZ37Bvn/Tdd1KlSi6VywAAAAQaQjk8xiOnYP7rX1LNmtLu3SUv/OYb\nyWot3OjpTrkMAABAICGUw2PcOgUzKamgVOX//q/kBY88UhDGb7qp2MceL5cBAADwEzZ6wmNcOQXz\n+vpSTKPaxoPh4VJGhlSrluGwx8plAAAA/IyVcnjU6IRoh4ftSJLFmq8Z8/+jd/91i/EF69dL+/fb\nDeSSh8plAAAAAgDpxIcqQp9r2ymYpXVEuWPTUr3yzWTjwSlTpFGjTL/L1TkCAAAEEkK5DyRn5GhK\nUrphWUenyFC3DrYJRPZOwbz4UJa+n2Gn3/jVVxfUlVc2/4+kK+UycZGh5e4bIQAAUPYRyr0sMXVP\nqavGnjrYJtAUPQXzpy37dOvQm1Vvb6bxxXv2SM1d+9pHJ0Rr0MwUU20RgyzSqIRol94DAADgTdSU\ne1FyRk6FP9gm5o2XNTihjXEgX7iwoKuKi4FcOl8u46iOPcgiTezdrlz9RAIAAJQfrJR7kSsH25Sb\n0PjDD9I11xiP3Xef9M47BS0QPcBeuYxNXGSoRpWzEiEAAFC+EMq9xJ2DbRzVPAf0htFDh6QwO+G3\nTp2CUpXadloguqFouUzA/toAAADYQSj3EncOtrEXIgN6w6jVKt11l/TJJ8bja9ZIHTt6fRox4SGE\ncAAAUOZQU+4lnj7YJjF1jwbNTLG7+m7bMPpJ6l6X3uuWDz+UgoKMA/nLLxcEdh8EcgAAgLKKlXIv\n8eTBNs5uGG1ar7pvVsx37JCioozHOnWSVq6UqlTx/jwAAADKOFbKvcSTB9u4smHUq06fltq3tx/I\nd+6UUlII5AAAACYRyr3EdrCNM4wOtnFnw6hXvPiiVK2atGFDybFPPikoVYmM9M67AQAAyinKV7zI\nEwfbeGPDqEtWr5a6dDEeGzRImj3bYYtDOqMAAAAYI5R7ke1gG0f14KUdbOPpDaNOO3xYatRIOnWq\n5Fi1atLvv0v16pX6iIDuGgMAABAAKF/xsn4dW2ju8DjF2SlliYsM1dzhcerb0fhUS09uGHWK1SoN\nGVIQuI0CeXKydPKkw0Ae0F1jAAAAAgQr5T7gzsE2ntwwatpnn0l33mk89vzz0r//beoxAds1BgAA\nIMAQyn3IlYNtbBtGndnsabRh1JTdu6WICOOxdu2k1FSpalXTj3OlawyhHAAAVESUr5QBoxOiFVT6\nHspC9jaMlurMGemqq+wH8vT0gm4rTgTygOsaAwAAEMAI5WWAbcOoo2Be2oZRu159tSBsp6SUHJs3\nr6C23F4/8lK40zUGAACgoqF8pYzo17GFmtWroalJ6UoxWIGOiwzVKGe6mPz8s9Sxo/HYnXdKiYkO\nWxyWxu9dYwAAAMoQQnkZ4s6G0UJHj0otWkhHjhiPHzwohblf1+23rjEAAABlEAmoDHJlw6isVun+\n+6Xp043Hly+XrrnG7bnZ+KVrDAAAQBlFTXlF8OWXUlCQcSB/+umCwO7BQC6d7xrjDJe7xgAAAJRx\nrJSXZ1lZUnPjQ4nUqpX0yy9ScLDXXj86IVqDZqaYaovoUtcYAACAcoKV8vLo3Dmpe3f7gfzXX6Vt\n27wayCUvd40BAAAoRwjl5c0bb0iVK0srVpQcmzmzoFSlTRufTadfxxaaOzxOcXZKWeIiQzV3eJz6\ndrTzDQQAAEAFQPlKebFhg9S+vfHYrbdKCxcW1JX7gUe6xgAAAJRjhPKy7tgxKTpa2r/feHz/fik8\n3LdzssOlrjEAAAAVAOUrZdno0VJIiHEgX7y4oFQlQAI5AAAA7COUl0Xffltw2ubUqSXHnniiIIxf\nf73v5wUAAACXUL5SluzfLzVubDzWvHlBR5UaNXw7JwAAALiNlfKyID9fuukm+4F840Zpzx4COQAA\nQBlFKA90774rVapUULJyobfeKihVuewy388LAAAAHkP5SqDaskW69FLjseuvl/73v4KwDgAAgDKP\nUB5o/vxTattW2rXLeHzfPqlJE59OCQAAAN5F+UogGTOmoC7cKJB//XVBqQqBHAAAoNxhpTwQJCVJ\n111nPPbww9Lrr/t2PgAAAPApQrk/HTwoNWxoPNaggbRzp1Srlm/nBAAAAJ8jlPtDfr7Up4+0cKHx\n+Lp10hVXeOx1adl5Ss7I0bGTZ1UruLLio8I47h4AACCAEMp9bfZsaehQ47HXXpMefbQgRCdnuh2i\nkzNyNCUpXWsyc0uMdYoM1eiEaMVHhTn9XAAAAHgWodxXtm+XWrc2HuvWTfr+eyXvOqwp01Z7JEQn\npu7Rkws2Kd9qPL4mM1eDZqZoYu926tuxudmvAgAAAF5A9xVvO3lSatPGfiDfvVv64Qclrv9Ng2am\nGAZy6XyI/iR1r8NXJmfklBrIbfKt0tgFG5WckePwmQAAAPAeQrk3zZ0rVa8ubdtWcuzzzwtaHLZo\n4fEQPSUp3eGzij5zalK6uYsBAADgFYRybzl+XLr33pKfjxhRsNHzttsKP/JkiE7LzrO72m5PSmau\n0rLznLoHAAAAnhMwodxqtWr27Nnq3r27QkNDVb16dUVGRqpv375KS0vz9/ScV61awV82tWtLhw9L\n06dLFkvhx54O0a6WolDCAgAA4D8BsdHz5MmTuvPOO/XVV1+pVatW6t+/v0JCQvTbb7/pxx9/VFpa\nmmJiYvw9TedUriytWSOtWiVdfrkUG2t4mTsh2qgjy7GTZ116nqv3AQAAwH0BEcr/8Y9/6KuvvtKT\nTz6pF154QUFBxRfwz5w546eZual1a/sbPP/i6RBdK9i131JX7wMAAID7/J7EduzYoXfeeUcdO3bU\niy++KEuR0g6bKlWq+GFm7jF7YI+nQ7SrfcfpVw4AAOA/fg/lH330kfLz8zVkyBAdPXpUixYt0t69\ne1W/fn316NFDUVFR/p6iU5w9sMfTITomPESdIkOdqlOPiwzlhE8AAAA/8nsoT01NlSQdOXJELVu2\n1KFDhwrHLBaLHnjgAU2dOlWVKlVy+KxYO3XbW7duVZs2bTwz4VK4cmCPN0L06IRoDZqZYqqjS5BF\nGpUQbfrdAAAA8Dy/d185cOCAJOnZZ5/VlVdeqU2bNikvL09JSUlq2bKl3nrrLT3//PN+nqVj7vQa\nT2jd0Kl39XBwfXxUmF7qfZmCSlYCFRNkkSb2bkfpCgAAgJ95JJRHRETIYrGY/mvgwIGF9547d06S\n1LhxY33++ee69NJLVatWLfXo0UOfffaZgoKC9N///lenT592OI+1a9ca/uWLVXJ3eo0nbTvg1Lu+\nN3F9v44tNHd4nOIiQw3H4yJDNXd4XOGKPQAAAPzHI+UrLVu2VHBwsOnrmzRpUvj/69WrJ0m68cYb\nVb169WLXXX755YqMjNSOHTu0detWXX755Z6Yrse522vc1Xsd1YHHR4UpPirM9KZTAAAA+IdHQnlS\nUpLL97Zq1UqLFy9W3bp1Dcdtof3PP/90+R3e5o8De+z1KTcSEx5CCAcAAAhgfq8pT0hIkCRt3ry5\nxNipU6eUnl5Q5hEREeHLaTnFnV7jHPYDAAAAv4fym266SRdffLG+++47LVmypNjY888/ryNHjuia\na65Ro0aN/DRDx9zpNc5hPwAAAPB7sqtatapmz56tnj176qabbtLtt9+uiy66SKmpqfrhhx/UoEED\nTZ8+3d/TLJU/DuyhYwoAAED54feVcknq2rWrfv75Z/Xp00crVqzQ1KlTtXPnTt13331at26dYmJi\n/D3FUtl6jTvD1mvcnXsBAABQPvh9pdymbdu2SkxM9Pc0XObOgT0c9gMAAFCxBcRKeXngzoE9HPYD\nAABQsQXMSnl50K9jCzWrV0NTk9KVYtB7PC4yVKMSog1DtTv3AgAAoGwjlHuYOwf2cNgPAABAxUQo\n9xJ3DuzhsB8AAICKhZpyAAAAwM8I5QAAAICfEcoBAAAAPyOUAwAAAH5GKAcAAAD8jFAOAAAA+Bmh\nHAAAAPAzQjkAAADgZ4RyAAAAwM8I5QAAAICfWaxWq9Xfk/C20NBQnTx5Um3atPH3VAAAAFCObd26\nVcHBwcrNzXXqvspemk9AqV27tr+nUC5t3bpVkvhmpxzj97hi4Pe5/OP3uPzj9zhwBAcHu5Q9K8RK\nObwjNjZWkrR27Vo/zwTewu9xxcDvc/nH73H5x+9x2UdNOQAAAOBnhHIAAADAzwjlAAAAgJ8RygEA\nAAA/I5QDAAAAfkb3FQAAAMDPWCkHAAAA/IxQDgAAAPgZoRwAAADwM0I5AAAA4GeEcgAAAMDPCOUA\nAACAnxHKAQAAAD8jlMMj0tPTNWnSJPXo0UPNmzdX1apVFR4erl69emnZsmX+nh484MyZM5oyZYqG\nDRum9u3bq2rVqrJYLJoxY4a/pwYXZWVl6Z577lGTJk1UrVo1RURE6NFHH9Uff/zh76nBAz777DM9\n8sgj6tatm2rXri2LxaKBAwf6e1rwoEOHDmnGjBm6/fbbFRUVperVq6tOnTrq2rWrZs6cqfz8fH9P\nEU7g8CB4xF133aXExES1bdtWXbt2VWhoqLZv364vv/xS586d05QpUzRq1Ch/TxNuOHz4sOrVqydJ\nCg8PV9WqVbV37169++67uvfee/08Ozhrx44d6tKliw4cOKBevXqpdevWWrNmjZYtW6ZWrVopOTlZ\n9evX9/c04Yb27dtrw4YNqlWrlpo1a6Zt27ZpwIABmjdvnr+nBg9555139MADD6hx48a69tpr1aJF\nC2VnZ2vBggU6cuSI+vTpo08//VQWi8XfU4UZVsADZs2aZV23bl2Jz5cvX26tUqWKtWrVqtbffvvN\nDzODp5w6dcr6zTffFP4+jhs3zirJ+u677/p5ZnBFz549rZKsU6dOLfb5Y489ZpVkHTlypJ9mBk/5\n/vvvrWlpadb8/HzrsmXLrJKsAwYM8Pe04EFJSUnWL7/80nru3Llin//+++/W5s2bWyVZP/vsMz/N\nDs6ifAUeMXToUF1xxRUlPr/mmmvUvXt3nT59WqtWrfLDzOApVatW1U033aTGjRv7eypw086dO7V4\n8WJFRETooYceKjY2fvx41axZU3PnztXx48f9NEN4wrXXXqvo6GhWScuxHj166NZbb1VQUPE416hR\nI91///2SpOXLl/thZnAFoRxeV6VKFUlS5cqV/TwTAJL0/fffS5J69uxZ4j/mISEhio+P14kTJ/TT\nTz/5Y3oAPID/9pY9hHJ41e7du5WUlKQaNWro6quv9vd0AEjavn27JCkmJsZwPDo6WpKUlpbmszkB\n8JyzZ89qzpw5kqQbb7zRz7OBWXz7BK85deqUBgwYoFOnTunll18u3CQIwL+OHDkiSapTp47huO3z\nw4cP+2pKADxo7Nix2rx5s26++WbdcMMN/p4OTGKlHIUiIiJksVhM/1Vaa61z585p0KBBSk5OVr9+\n/fTEE0/48CuBPZ78PUb5Zf2rKRe1yEDZM3XqVL366qtq3bq15s6d6+/pwAmslKNQy5YtFRwcbPr6\nJk2aGH5+7tw5DRw4UJ9++qn69u2refPm8R/3AOGp32OUbbaVcNuK+YWOHj1a7DoAZcObb76p0aNH\nq23btkpKSlJoaKi/pwQnEMpRKCkpye1nnD17Vv3799enn36q/v37a86cOapUqZIHZgdP8MTvMcq+\nVq1aSbJfM56eni7Jfs05gMAzefJkPfbYY7r00kuVlJSkhg0b+ntKcBLlK/CY06dP64477tCnn36q\nwYMHa+7cuQRyIABde+21kqTFixeXOPEvLy9PycnJql69uq666ip/TA+AkyZNmqTHHntM7du317Jl\nywjkZRShHB5x6tQp3X777friiy80fPhwzZo1q0SrNQCBoWXLlurZs6d27dqlN998s9jYuHHjdPz4\ncQ0ePFg1a9b00wwBmPX8889r7Nixio2NVVJSksLCwvw9JbjIYrXt6AHcMGzYML3//vsKCwvTgw8+\naFhD3r17d3Xv3t33k4PHTJw4Udu2bZMk/fLLL9qwYYO6dOlS2EKva9euuvfee/05RZi0Y8cOdenS\nRQcOHFCvXr3Upk0bpaSkaNmyZYqJidGqVatUv359f08Tbli4cKEWLlwoSdq/f7++++47XXzxxerW\nrZskKSwsTK+88oofZwh3zZ49W0OHDlWlSpX0yCOPGO4DiYiI0NChQ30/OTiNUA6P6N69u1asWFHq\nNePGjdNzzz3nmwnBKxz9Pg8ZMkTvv/++7yYEt+zdu1fPPvusvv32Wx06dEiNGzfWbbfdpnHjxrFB\nrBx47rnnNH78eLvjF110kXbt2uW7CcHjHP0eSwUna3OqZ9lAKAcAAAD8jKJfAAAAwM8I5QAAAICf\nEcoBAAAAPyOUAwAAAH5GKAcAAAD8jFAOAAAA+BmhHAAAAPAzQjkAAADgZ4RyAAAAwM8I5QAAAICf\nEcoBAAAAPyOUAwAAAH5GKAcAAAD8jFAOAAAA+BmhHAAAAPAzQjkAAADgZ4RyAAAAwM/+H2UdHvcm\n3+mQAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 600x400 with 1 Axes>"
            ]
          },
          "metadata": {
            "image/png": {
              "height": 248,
              "width": 370
            }
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# External Callbacks\n",
        "\n",
        "\n",
        "- jax.pure_callback(): appropriate for pure functions: i.e. functions with no side effects.\n",
        "\n",
        "- jax.experimental.io_callback(): appropriate for impure functions: e.g. functions which read or write data to disk.\n",
        "\n",
        "- jax.debug.callback(): appropriate for functions that should reflect the execution behavior of the compiler."
      ],
      "metadata": {
        "id": "N7QWngXKXHXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Exploring pure_callback\n",
        "\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "\n",
        "def f_host(x):\n",
        "  # call a numpy (not jax.numpy) operation:\n",
        "  return np.sin(x).astype(x.dtype)\n",
        "\n",
        "def f(x):\n",
        "  result_shape = jax.ShapeDtypeStruct(x.shape, x.dtype)\n",
        "  return jax.pure_callback(f_host, result_shape, x)\n",
        "\n",
        "x = jnp.arange(5.0)\n",
        "print(f(x))\n",
        "print(jax.jit(f)(x))\n",
        "print(jax.vmap(f)(x))\n",
        "def body_fun(_, x):\n",
        "  return _, f(x)\n",
        "jax.lax.scan(body_fun, None, jnp.arange(5.0))[1]\n",
        "# Pure callbacks do not support JVP\n",
        "# jax.grad(f)(x)"
      ],
      "metadata": {
        "id": "5SZMlW3Wda14",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1731047834613,
          "user_tz": 480,
          "elapsed": 2436,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "e150e775-293a-4332-c4e3-04aaa97991c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0.          0.84147096  0.9092974   0.14112    -0.7568025 ]\n",
            "[ 0.          0.84147096  0.9092974   0.14112    -0.7568025 ]\n",
            "[ 0.          0.84147096  0.9092974   0.14112    -0.7568025 ]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-17-6c0ef8dea82e>:11: DeprecationWarning: The default behavior of pure_callback under vmap will soon change. Currently, the default behavior is to generate a sequential vmap (i.e. a loop), but in the future the default will be to raise an error. To keep the current default, set vmap_method='sequential'.\n",
            "  return jax.pure_callback(f_host, result_shape, x)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array([ 0.        ,  0.84147096,  0.9092974 ,  0.14112   , -0.7568025 ],      dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def print_something():\n",
        "  print('printing something')\n",
        "  return np.int32(0)\n",
        "\n",
        "@jax.jit\n",
        "def f1():\n",
        "  return jax.pure_callback(print_something, np.int32(0))\n",
        "f1();"
      ],
      "metadata": {
        "id": "w7SUVLM-e-AT",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1731047977541,
          "user_tz": 480,
          "elapsed": 749,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "ae38cd04-e40a-4bac-eb26-ff42714f559d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "printing something\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@jax.jit\n",
        "def f2():\n",
        "  # the output of the callback is unused, and so the compiler notices this and eliminates the function call.\n",
        "  jax.pure_callback(print_something, np.int32(0))\n",
        "  return 1.0\n",
        "f2();"
      ],
      "metadata": {
        "id": "TOxhw9OsgFlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Exploring io_callback\n",
        "# functions that do have side-effects\n",
        "from jax.experimental import io_callback\n",
        "from functools import partial\n",
        "\n",
        "global_rng = np.random.default_rng(0)\n",
        "\n",
        "def host_side_random_like(x):\n",
        "  \"\"\"Generate a random array like x using the global_rng state\"\"\"\n",
        "  # We have two side-effects here:\n",
        "  # - printing the shape and dtype\n",
        "  # - calling global_rng, thus updating its state\n",
        "  # a side-effect of generating a random number in numpy is that the random state is updated\n",
        "  print(f'generating {x.dtype}{list(x.shape)}')\n",
        "  return global_rng.uniform(size=x.shape).astype(x.dtype)\n",
        "\n",
        "@jax.jit\n",
        "def numpy_random_like(x):\n",
        "  # a callback to a global host-side numpy random generator\n",
        "  return io_callback(host_side_random_like, x, x)\n",
        "\n",
        "x = jnp.zeros(5)\n",
        "print(numpy_random_like(x))\n",
        "\n",
        "print(\"\\nvmap\\n\", jax.vmap(numpy_random_like)(x))\n",
        "\n",
        "\n",
        "@jax.jit\n",
        "def numpy_random_like_ordered(x):\n",
        "  return io_callback(host_side_random_like, x, x, ordered=True)\n",
        "# Cannot `vmap` ordered IO callback.\n",
        "# jax.vmap(numpy_random_like_ordered)(x)\n",
        "\n",
        "def body_fun(_, x):\n",
        "  return _, numpy_random_like_ordered(x)\n",
        "print(\"\\nscan\\n\", jax.lax.scan(body_fun, None, jnp.arange(5.0))[1])\n",
        "\n",
        "# ValueError: IO callbacks do not support JVP.\n",
        "# jax.grad(numpy_random_like)(x)\n",
        "\n",
        "# if the callback is not dependent on a differentiated variable, it will execute:\n",
        "# Unlike pure_callback, the compiler will not remove the callback execution in\n",
        "# this case, even though the output of the callback is unused in the subsequent\n",
        "# computation.\n",
        "@jax.jit\n",
        "def f(x):\n",
        "  io_callback(lambda: print('hello'), None)\n",
        "  return x\n",
        "\n",
        "jax.grad(f)(1.0);\n"
      ],
      "metadata": {
        "id": "8iNHdNqEgdxA",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1731048886967,
          "user_tz": 480,
          "elapsed": 2570,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "71f75840-2cdd-4435-b6e5-9115da277eb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "generating float32[5]\n",
            "[0.6369617  0.26978672 0.04097353 0.01652764 0.8132702 ]\n",
            "\n",
            "vmap\n",
            "generating float32[]\n",
            " generating float32[]\n",
            "generating float32[]\n",
            "generating float32[]\n",
            "generating float32[]\n",
            "[0.91275555 0.60663575 0.72949654 0.543625   0.9350724 ]\n",
            "\n",
            "scan\n",
            " generating float32[]\n",
            "generating float32[]\n",
            "generating float32[]\n",
            "generating float32[]\n",
            "generating float32[]\n",
            "[0.81585354 0.0027385  0.8574043  0.03358557 0.72965544]\n",
            "hello\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Exploring debug.callback\n",
        "\n",
        "from jax import debug\n",
        "\n",
        "def log_value(x):\n",
        "  # This could be an actual logging call; we'll use\n",
        "  # print() for demonstration\n",
        "  print(\"log:\", x)\n",
        "\n",
        "@jax.jit\n",
        "def f(x):\n",
        "  debug.callback(log_value, x)\n",
        "  return x\n",
        "\n",
        "f(1.0);\n",
        "\n",
        "x = jnp.arange(5.0)\n",
        "jax.vmap(f)(x);\n",
        "\n",
        "jax.grad(f)(1.0);"
      ],
      "metadata": {
        "id": "ve68cmg7joTd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Example: pure_callback with custom_jvp"
      ],
      "metadata": {
        "id": "i3Jpk6skmTPz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gradient checkpointing with jax.checkpoint (jax.remat)\n",
        "\n",
        "Use the jax.checkpoint() decorator (aliased as jax.remat()) with jax.grad() to control which intermediates are saved on the forward pass versus the recomputed intermediates on the backward pass, trading off memory and FLOPs."
      ],
      "metadata": {
        "id": "6lc10LmamjxK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If you don’t use jax.checkpoint(), the jax.grad(f)(x) forward pass stores\n",
        "# Jacobian coefficients and other intermediates to use during the backward pass.\n",
        "# These saved values are called residuals.\n",
        "import jax\n",
        "import jax.numpy as jnp\n",
        "\n",
        "def g(W, x):\n",
        "  y = jnp.dot(W, x)\n",
        "  return jnp.sin(y)\n",
        "\n",
        "def f(W1, W2, W3, x):\n",
        "  x = g(W1, x)\n",
        "  x = g(W2, x)\n",
        "  x = g(W3, x)\n",
        "  return x\n",
        "\n",
        "W1 = jnp.ones((5, 4))\n",
        "W2 = jnp.ones((6, 5))\n",
        "W3 = jnp.ones((7, 6))\n",
        "x = jnp.ones(4)\n",
        "\n",
        "# Inspect the 'residual' values to be saved on the forward pass\n",
        "# if you were to evaluate `jax.grad(f)(W1, W2, W3, x)`\n",
        "from jax.ad_checkpoint import print_saved_residuals\n",
        "jax.ad_checkpoint.print_saved_residuals(f, W1, W2, W3, x)"
      ],
      "metadata": {
        "id": "9PLDJv6GmiKc",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1731049856840,
          "user_tz": 480,
          "elapsed": 3074,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "bbd26007-51ff-4b05-ffb1-d37d2639a493"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "f32[5,4] from the argument W1\n",
            "f32[6,5] from the argument W2\n",
            "f32[7,6] from the argument W3\n",
            "f32[4] from the argument x\n",
            "f32[5] output of sin from <ipython-input-27-9b1bc8963ecf>:10:9 (g)\n",
            "f32[5] output of cos from <ipython-input-27-9b1bc8963ecf>:10:9 (g)\n",
            "f32[6] output of sin from <ipython-input-27-9b1bc8963ecf>:10:9 (g)\n",
            "f32[6] output of cos from <ipython-input-27-9b1bc8963ecf>:10:9 (g)\n",
            "f32[7] output of cos from <ipython-input-27-9b1bc8963ecf>:10:9 (g)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#  only the inputs of a jax.checkpoint()-decorated function might be saved\n",
        "def f2(W1, W2, W3, x):\n",
        "  x = jax.checkpoint(g)(W1, x)\n",
        "  x = jax.checkpoint(g)(W2, x)\n",
        "  x = jax.checkpoint(g)(W3, x)\n",
        "  return x\n",
        "\n",
        "jax.ad_checkpoint.print_saved_residuals(f2, W1, W2, W3, x)\n",
        "\n",
        "#  inputs to a jax.checkpoint()-decorated function may be saved, e.g. sin"
      ],
      "metadata": {
        "id": "Pbu6qDmCnhur",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1731049957778,
          "user_tz": 480,
          "elapsed": 53,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "638747ee-760f-4c14-a4e6-916a6ff254c6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "f32[5,4] from the argument W1\n",
            "f32[6,5] from the argument W2\n",
            "f32[7,6] from the argument W3\n",
            "f32[4] from the argument x\n",
            "f32[5] output of sin from <ipython-input-27-9b1bc8963ecf>:10:9 (g)\n",
            "f32[6] output of sin from <ipython-input-27-9b1bc8963ecf>:10:9 (g)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# saves only the results of dot operations with no batch dimensions (since they\n",
        "# are often FLOP-bound, and hence worth saving rather than recomputing)\n",
        "f3 = jax.checkpoint(f, policy=jax.checkpoint_policies.dots_with_no_batch_dims_saveable)\n",
        "jax.ad_checkpoint.print_saved_residuals(f3, W1, W2, W3, x)"
      ],
      "metadata": {
        "id": "EckU_pftn7sQ",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1731050208344,
          "user_tz": 480,
          "elapsed": 52,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "4b07c7c0-ddac-4bea-f289-a91040dc3c30"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "f32[5,4] from the argument W1\n",
            "f32[6,5] from the argument W2\n",
            "f32[7,6] from the argument W3\n",
            "f32[4] from the argument x\n",
            "f32[5] output of reduce_precision from <ipython-input-27-9b1bc8963ecf>:9:6 (g)\n",
            "f32[6] output of reduce_precision from <ipython-input-27-9b1bc8963ecf>:9:6 (g)\n",
            "f32[7] output of reduce_precision from <ipython-input-27-9b1bc8963ecf>:9:6 (g)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from jax.ad_checkpoint import checkpoint_name\n",
        "\n",
        "def f4(W1, W2, W3, x):\n",
        "  x = checkpoint_name(g(W1, x), name='a')\n",
        "  x = checkpoint_name(g(W2, x), name='b')\n",
        "  x = checkpoint_name(g(W3, x), name='c')\n",
        "  return x\n",
        "\n",
        "f4 = jax.checkpoint(f4, policy=jax.checkpoint_policies.save_only_these_names('a'))\n",
        "jax.ad_checkpoint.print_saved_residuals(f4, W1, W2, W3, x)"
      ],
      "metadata": {
        "id": "bRy4OuItpa96",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1731050457710,
          "user_tz": 480,
          "elapsed": 164,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "fb5b4025-7a4f-4238-f5d1-d792bfb0c0f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "f32[5,4] from the argument W1\n",
            "f32[6,5] from the argument W2\n",
            "f32[7,6] from the argument W3\n",
            "f32[4] from the argument x\n",
            "f32[5] output of reduce_precision from <ipython-input-30-fc0ed1c14b8d>:4:6 (f4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# JAX Internals: primitives"
      ],
      "metadata": {
        "id": "OVLGBMsg8Hi1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using existing JAX primitives"
      ],
      "metadata": {
        "id": "dxqLVPSe82M7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from jax import lax\n",
        "from jax._src import api\n",
        "\n",
        "def multiply_add_lax(x, y, z):\n",
        "  \"\"\"Implementation of multiply-add using the `jax.lax` primitives.\"\"\"\n",
        "  return lax.add(lax.mul(x, y), z)\n",
        "\n",
        "\n",
        "def square_add_lax(a, b):\n",
        "  \"\"\"A square-add function using the newly defined multiply-add.\"\"\"\n",
        "  return multiply_add_lax(a, a, b)\n",
        "\n",
        "print(\"square_add_lax = \", square_add_lax(2., 10.))\n",
        "# Differentiate w.r.t. the first argument\n",
        "print(\"grad(square_add_lax) = \", api.grad(square_add_lax, argnums=0)(2.0, 10.))"
      ],
      "metadata": {
        "id": "SnuY2gSx8ISz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Helper functions (execute this cell)\n",
        "import functools\n",
        "import traceback\n",
        "\n",
        "_indentation = 0\n",
        "def _trace(msg=None):\n",
        "    \"\"\"Print a message at current indentation.\"\"\"\n",
        "    if msg is not None:\n",
        "        print(\"  \" * _indentation + msg)\n",
        "\n",
        "def _trace_indent(msg=None):\n",
        "    \"\"\"Print a message and then indent the rest.\"\"\"\n",
        "    global _indentation\n",
        "    _trace(msg)\n",
        "    _indentation = 1 + _indentation\n",
        "\n",
        "def _trace_unindent(msg=None):\n",
        "    \"\"\"Unindent then print a message.\"\"\"\n",
        "    global _indentation\n",
        "    _indentation = _indentation - 1\n",
        "    _trace(msg)\n",
        "\n",
        "def trace(name):\n",
        "  \"\"\"A decorator for functions to trace arguments and results.\"\"\"\n",
        "\n",
        "  def trace_func(func):  # pylint: disable=missing-docstring\n",
        "    def pp(v):\n",
        "        \"\"\"Print certain values more succinctly\"\"\"\n",
        "        vtype = str(type(v))\n",
        "        if \"jax._src.xla_bridge._JaxComputationBuilder\" in vtype:\n",
        "            return \"<JaxComputationBuilder>\"\n",
        "        elif \"jaxlib.xla_extension.XlaOp\" in vtype:\n",
        "            return \"<XlaOp at 0x{:x}>\".format(id(v))\n",
        "        elif (\"partial_eval.JaxprTracer\" in vtype or\n",
        "              \"batching.BatchTracer\" in vtype or\n",
        "              \"ad.JVPTracer\" in vtype):\n",
        "            return \"Traced<{}>\".format(v.aval)\n",
        "        elif isinstance(v, tuple):\n",
        "            return \"({})\".format(pp_values(v))\n",
        "        else:\n",
        "            return str(v)\n",
        "    def pp_values(args):\n",
        "        return \", \".join([pp(arg) for arg in args])\n",
        "\n",
        "    @functools.wraps(func)\n",
        "    def func_wrapper(*args):\n",
        "      _trace_indent(\"call {}({})\".format(name, pp_values(args)))\n",
        "      res = func(*args)\n",
        "      _trace_unindent(\"|<- {} = {}\".format(name, pp(res)))\n",
        "      return res\n",
        "\n",
        "    return func_wrapper\n",
        "\n",
        "  return trace_func\n",
        "\n",
        "class expectNotImplementedError(object):\n",
        "  \"\"\"Context manager to check for NotImplementedError.\"\"\"\n",
        "  def __enter__(self): pass\n",
        "  def __exit__(self, type, value, tb):\n",
        "    global _indentation\n",
        "    _indentation = 0\n",
        "    if type is NotImplementedError:\n",
        "      print(\"\\nFound expected exception:\")\n",
        "      traceback.print_exc(limit=3)\n",
        "      return True\n",
        "    elif type is None:  # No exception\n",
        "      assert False, \"Expected NotImplementedError\"\n",
        "    else:\n",
        "      return False"
      ],
      "metadata": {
        "id": "gjeiAdBi8Trm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import jax.numpy as jnp\n",
        "import numpy as np\n",
        "\n",
        "@trace(\"multiply_add_numpy\")\n",
        "def multiply_add_numpy(x, y, z):\n",
        "    return jnp.add(jnp.multiply(x, y), z)\n",
        "\n",
        "@trace(\"square_add_numpy\")\n",
        "def square_add_numpy(a, b):\n",
        "    return multiply_add_numpy(a, a, b)\n",
        "\n",
        "print(\"\\nNormal evaluation:\")\n",
        "print(\"square_add_numpy = \", square_add_numpy(2., 10.))\n",
        "print(\"\\nGradient evaluation:\")\n",
        "print(\"grad(square_add_numpy) = \", api.grad(square_add_numpy)(2.0, 10.))"
      ],
      "metadata": {
        "id": "9J_DA13Z8ccA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining new JAX primitives"
      ],
      "metadata": {
        "id": "reLdoV_w840J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from jax import core\n",
        "\n",
        "multiply_add_p = core.Primitive(\"multiply_add\")  # Create the primitive\n",
        "\n",
        "@trace(\"multiply_add_prim\")\n",
        "def multiply_add_prim(x, y, z):\n",
        "  \"\"\"The JAX-traceable way to use the JAX primitive.\n",
        "\n",
        "  Note that the traced arguments must be passed as positional arguments\n",
        "  to `bind`.\n",
        "  \"\"\"\n",
        "  return multiply_add_p.bind(x, y, z)\n",
        "\n",
        "@trace(\"square_add_prim\")\n",
        "def square_add_prim(a, b):\n",
        "  \"\"\"A square-add function implemented using the new JAX-primitive.\"\"\"\n",
        "  return multiply_add_prim(a, a, b)"
      ],
      "metadata": {
        "id": "7D0olqTO871J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@trace(\"multiply_add_impl\")\n",
        "def multiply_add_impl(x, y, z):\n",
        "  \"\"\"Concrete implementation of the primitive.\n",
        "\n",
        "  This function does not need to be JAX traceable.\n",
        "\n",
        "  Args:\n",
        "    x, y, z: The concrete arguments of the primitive. Will only be called with\n",
        "      concrete values.\n",
        "\n",
        "  Returns:\n",
        "    the concrete result of the primitive.\n",
        "  \"\"\"\n",
        "  # Note: you can use the ordinary (non-JAX) NumPy, which is not JAX-traceable.\n",
        "  return np.add(np.multiply(x, y), z)\n",
        "\n",
        "# Now, register the primal implementation with JAX:\n",
        "multiply_add_p.def_impl(multiply_add_impl)"
      ],
      "metadata": {
        "id": "E8Rfrw1j9X8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from jax import core\n",
        "\n",
        "@trace(\"multiply_add_abstract_eval\")\n",
        "def multiply_add_abstract_eval(xs, ys, zs):\n",
        "  \"\"\"Abstract evaluation of the primitive.\n",
        "\n",
        "  This function does not need to be JAX traceable. It will be invoked with\n",
        "  abstractions of the actual arguments\n",
        "\n",
        "  Args:\n",
        "    xs, ys, zs: Abstractions of the arguments.\n",
        "\n",
        "  Result:\n",
        "    a ShapedArray for the result of the primitive.\n",
        "  \"\"\"\n",
        "  assert xs.shape == ys.shape\n",
        "  assert xs.shape == zs.shape\n",
        "  return core.ShapedArray(xs.shape, xs.dtype)\n",
        "\n",
        "# Now, register the abstract evaluation with JAX:\n",
        "multiply_add_p.def_abstract_eval(multiply_add_abstract_eval)"
      ],
      "metadata": {
        "id": "7rEAKx6oAbnE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from jax._src.lib.mlir.dialects import hlo\n",
        "\n",
        "@trace(\"multiply_add_lowering\")\n",
        "def multiply_add_lowering(ctx, xc, yc, zc):\n",
        "  \"\"\"The compilation to XLA of the primitive.\n",
        "\n",
        "  Given an mlir.ir.Value for each argument, return the mlir.ir.Values for\n",
        "  the results of the function.\n",
        "\n",
        "  Does not need to be a JAX-traceable function.\n",
        "  \"\"\"\n",
        "  return [hlo.AddOp(hlo.MulOp(xc, yc), zc).result]\n",
        "\n",
        "# Now, register the lowering rule with JAX.\n",
        "# For GPU, refer to the https://jax.readthedocs.io/en/latest/Custom_Operation_for_GPUs.html\n",
        "from jax.interpreters import mlir\n",
        "\n",
        "mlir.register_lowering(multiply_add_p, multiply_add_lowering, platform='cpu')"
      ],
      "metadata": {
        "id": "gsVgJuo-AcBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from jax.interpreters import ad\n",
        "\n",
        "@trace(\"multiply_add_value_and_jvp\")\n",
        "def multiply_add_value_and_jvp(arg_values, arg_tangents):\n",
        "  \"\"\"Evaluates the primal output and the tangents (Jacobian-vector product).\n",
        "\n",
        "  Given values of the arguments and perturbation of the arguments (tangents),\n",
        "  compute the output of the primitive and the perturbation of the output.\n",
        "\n",
        "  This method must be JAX-traceable. JAX may invoke it with abstract values\n",
        "  for the arguments and tangents.\n",
        "\n",
        "  Args:\n",
        "    arg_values: A tuple of arguments\n",
        "    arg_tangents: A tuple with the tangents of the arguments. The tuple has\n",
        "      the same length as the arg_values. Some of the tangents may also be the\n",
        "      special value `ad.Zero` to specify a zero tangent\n",
        "\n",
        "  Returns:\n",
        "     A pair of the primal output and the tangent.\n",
        "  \"\"\"\n",
        "  x, y, z = arg_values\n",
        "  xt, yt, zt = arg_tangents\n",
        "  _trace(\"Primal evaluation:\")\n",
        "  # Now, you have a JAX-traceable computation of the output.\n",
        "  # Normally, you can use the multiply add (`ma`) primitive itself to compute the primal output.\n",
        "  primal_out = multiply_add_prim(x, y, z)\n",
        "\n",
        "  _trace(\"Tangent evaluation:\")\n",
        "  # You must use a JAX-traceable way to compute the tangent. It turns out that\n",
        "  # the output tangent can be computed as (xt * y + x * yt + zt),\n",
        "  # which you can implement in a JAX-traceable way using the same \"multiply_add_prim\" primitive.\n",
        "\n",
        "  # You do need to deal specially with `Zero`. Here, you just turn it into a\n",
        "  # proper tensor of 0s (of the same shape as 'x').\n",
        "  # An alternative would be to check for `Zero` and perform algebraic\n",
        "  # simplification of the output tangent computation.\n",
        "  def make_zero(tan):\n",
        "    return lax.zeros_like_array(x) if type(tan) is ad.Zero else tan\n",
        "\n",
        "  output_tangent = multiply_add_prim(make_zero(xt), y, multiply_add_prim(x, make_zero(yt), make_zero(zt)))\n",
        "  return (primal_out, output_tangent)\n",
        "\n",
        "# Register the forward differentiation rule with JAX:\n",
        "ad.primitive_jvps[multiply_add_p] = multiply_add_value_and_jvp"
      ],
      "metadata": {
        "id": "tbpkqCBMArv2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# JAX internals: The jaxpr language\n",
        "\n",
        "if one wants to understand how JAX works internally, or to make use of the result of JAX tracing, it is useful to understand jaxprs."
      ],
      "metadata": {
        "id": "1vDt5qEaPi_v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Jax - The Sharp Bits"
      ],
      "metadata": {
        "id": "mBZpE77UcqkO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Pure functions: the proper way to use JAX is to use it only on functionally pure Python functions.\n",
        "\n",
        "# Pure functions: all the input data is passed through the function parameters,\n",
        "# all the results are output through the function results. A pure function will\n",
        "# always return the same result if invoked with the same inputs.\n",
        "\n",
        "# Here are some examples of functions that are not functionally pure for which\n",
        "# JAX behaves differently than the Python interpreter. Note that these behaviors\n",
        "# are not guaranteed by the JAX system; the proper way to use JAX is to use it\n",
        "# only on functionally pure Python functions.\n",
        "\n",
        "def impure_print_side_effect(x):\n",
        "  print(\"Executing function\")  # This is a side-effect\n",
        "  return x\n",
        "\n",
        "# The side-effects appear during the first run\n",
        "print (\"First call: \", jit(impure_print_side_effect)(4.))\n",
        "\n",
        "# Subsequent runs with parameters of same type and shape may not show the side-effect\n",
        "# This is because JAX now invokes a cached compilation of the function\n",
        "print (\"Second call: \", jit(impure_print_side_effect)(5.))\n",
        "\n",
        "# JAX re-runs the Python function when the type or shape of the argument changes\n",
        "print (\"Third call, different type: \", jit(impure_print_side_effect)(jnp.array([5.])))\n",
        "\n",
        "\n",
        "g = 0.\n",
        "def impure_uses_globals(x):\n",
        "  return x + g\n",
        "\n",
        "# JAX captures the value of the global during the first run\n",
        "print (\"First call: \", jit(impure_uses_globals)(4.))\n",
        "g = 10.  # Update the global\n",
        "\n",
        "# Subsequent runs may silently use the cached value of the globals\n",
        "print (\"Second call: \", jit(impure_uses_globals)(5.))\n",
        "\n",
        "# JAX re-runs the Python function when the type or shape of the argument changes\n",
        "# This will end up reading the latest value of the global\n",
        "print (\"Third call, different type: \", jit(impure_uses_globals)(jnp.array([4.])))\n",
        "\n",
        "g = 0.\n",
        "def impure_saves_global(x):\n",
        "  global g\n",
        "  g = x\n",
        "  return x\n",
        "\n",
        "# JAX runs once the transformed function with special Traced values for arguments\n",
        "print (\"First call: \", jit(impure_saves_global)(4.))\n",
        "print (\"Saved global: \", g)  # Saved global has an internal JAX value"
      ],
      "metadata": {
        "id": "_Ykv7iFkPjqm",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1731197913684,
          "user_tz": 480,
          "elapsed": 5613,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "858baf7c-b394-4367-a91e-1accf2c30062"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:2024-11-09 16:18:28,124:jax._src.xla_bridge:906: Unable to initialize backend 'rocm': Your process properly initialized the GPU backend, but //learning/brain/research/jax:gpu_support is not linked in. You most likely should add that build dependency to your program.\n",
            "INFO:2024-11-09 16:18:30,680:jax._src.xla_bridge:906: Unable to initialize backend 'pathways': Could not initialize backend 'pathways'\n",
            "INFO:2024-11-09 16:18:30,680:jax._src.xla_bridge:906: Unable to initialize backend 'mock_tpu': Must pass --mock_tpu_platform flag to initialize the mock_tpu backend\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executing function\n",
            "First call:  4.0\n",
            "Second call:  5.0\n",
            "Executing function\n",
            "Third call, different type:  [5.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "g = 0.\n",
        "def impure_saves_global(x):\n",
        "  global g\n",
        "  g = x\n",
        "  return x\n",
        "\n",
        "# JAX runs once the transformed function with special Traced values for arguments\n",
        "print (\"First call: \", jit(impure_saves_global)(4.))\n",
        "print (\"Saved global: \", g)  # Saved global has an internal JAX value\n",
        "print (\"Second call: \", jit(impure_saves_global)(4.))\n",
        "print (\"Saved global: \", g)  # Saved global has an internal JAX value"
      ],
      "metadata": {
        "id": "_wd1y8EqcU04",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1731198184170,
          "user_tz": 480,
          "elapsed": 609,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "1e3debe6-c491-402c-fed8-fb01e29d379d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First call:  4.0\n",
            "Saved global:  Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace>\n",
            "Second call:  4.0\n",
            "Saved global:  Traced<ShapedArray(float32[], weak_type=True)>with<DynamicJaxprTrace>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Uu2os2eadH-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A Python function can be functionally pure even if it actually uses stateful\n",
        "# objects internally, as long as it does not read or write external state:\n",
        "def pure_uses_internal_state(x):\n",
        "  state = dict(even=0, odd=0)\n",
        "  for i in range(10):\n",
        "    state['even' if i % 2 == 0 else 'odd'] += x\n",
        "  return state['even'] + state['odd']\n",
        "\n",
        "print(jit(pure_uses_internal_state)(5.))"
      ],
      "metadata": {
        "id": "89OoQG-ldFhh",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1731198215299,
          "user_tz": 480,
          "elapsed": 1065,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "cf380fa3-c8d5-453d-9b54-1a009599219e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "50.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# It is not recommended to use iterators in any JAX function you want to jit or in any control-flow primitive. The reason is that an iterator is a python object which introduces state to retrieve the next element. Therefore, it is incompatible with JAX functional programming model. In the code below, there are some examples of incorrect attempts to use iterators with JAX. Most of them return an error, but some give unexpected results.\n",
        "\n",
        "import jax.numpy as jnp\n",
        "from jax import make_jaxpr\n",
        "from jax import lax\n",
        "\n",
        "# lax.fori_loop\n",
        "array = jnp.arange(10)\n",
        "print(lax.fori_loop(0, 10, lambda i,x: x+array[i], 0)) # expected result 45\n",
        "iterator = iter(range(10))\n",
        "print(lax.fori_loop(0, 10, lambda i,x: x+next(iterator), 0)) # unexpected result 0\n",
        "\n",
        "# lax.scan\n",
        "def func11(arr, extra):\n",
        "    ones = jnp.ones(arr.shape)\n",
        "    def body(carry, aelems):\n",
        "        ae1, ae2 = aelems\n",
        "        return (carry + ae1 * ae2 + extra, carry)\n",
        "    return lax.scan(body, 0., (arr, ones))\n",
        "make_jaxpr(func11)(jnp.arange(16), 5.)\n",
        "# make_jaxpr(func11)(iter(range(16)), 5.) # throws error\n",
        "\n",
        "# lax.cond\n",
        "array_operand = jnp.array([0.])\n",
        "lax.cond(True, lambda x: x+1, lambda x: x-1, array_operand)\n",
        "iter_operand = iter(range(10))\n",
        "# lax.cond(True, lambda x: next(x)+1, lambda x: next(x)-1, iter_operand) # throws error"
      ],
      "metadata": {
        "id": "5HySMLCXd6PT",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1731198461983,
          "user_tz": 480,
          "elapsed": 4465,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "598269c2-0a81-4c9f-a51a-386332c67343"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "45\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title In-place updates\n",
        "\n",
        "# Numpy works, but Jax does not work.\n",
        "import numpy as np\n",
        "\n",
        "numpy_array = np.zeros((3,3), dtype=np.float32)\n",
        "print(\"original array:\")\n",
        "print(numpy_array)\n",
        "\n",
        "# In place, mutating update\n",
        "numpy_array[1, :] = 1.0\n",
        "print(\"updated array:\")\n",
        "print(numpy_array)\n",
        "\n",
        "\n",
        "print(\"\\n Jax\")\n",
        "# %xmode Minimal\n",
        "jax_array = jnp.zeros((3,3), dtype=jnp.float32)\n",
        "\n",
        "# In place update of JAX's array will yield an error!\n",
        "# jax_array[1, :] = 1.0\n",
        "\n",
        "# Out of place\n",
        "updated_array = jax_array.at[1, :].set(1.0)\n",
        "print(\"updated array:\\n\", updated_array)\n",
        "print(\"original array unchanged:\\n\", jax_array)"
      ],
      "metadata": {
        "id": "MLGpVSHGe7l8",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1731199264984,
          "user_tz": 480,
          "elapsed": 52,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "6a3f8f99-ac6c-4496-c53b-8d577fc23ad2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "original array:\n",
            "[[0. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 0.]]\n",
            "updated array:\n",
            "[[0. 0. 0.]\n",
            " [1. 1. 1.]\n",
            " [0. 0. 0.]]\n",
            "\n",
            " Jax\n",
            "updated array:\n",
            " [[0. 0. 0.]\n",
            " [1. 1. 1.]\n",
            " [0. 0. 0.]]\n",
            "original array unchanged:\n",
            " [[0. 0. 0.]\n",
            " [0. 0. 0.]\n",
            " [0. 0. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Out-of-bounds indexing: think of out-of-bounds indexing in JAX as a case of undefined behavior.\n",
        "\n",
        "# Numpy will report an error\n",
        "# np.arange(10)[11]\n",
        "\n",
        "# raising an error from code running on an accelerator can be difficult or impossible\n",
        "# the last value is returned for\n",
        "print(jnp.arange(10)[11])\n",
        "\n",
        "# customize the behavior\n",
        "print(jnp.arange(10.0).at[11].get(mode='fill', fill_value=jnp.nan))"
      ],
      "metadata": {
        "id": "MUiRECYpf_pi",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1731199630705,
          "user_tz": 480,
          "elapsed": 54,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "edec04ae-089f-4d3b-8344-5c2588216811"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9\n",
            "nan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Non-array inputs: NumPy vs. JAX: passing lists or tuples to traced functions can lead to silent performance degradation that might otherwise be difficult to detect.\n",
        "\n",
        "print(np.sum([1, 2, 3]))\n",
        "# Error: jnp sum requires ndarray or scalar arguments\n",
        "# jnp.sum([1, 2, 3])\n",
        "x = [1, 2, 3]\n",
        "print(jnp.sum(jnp.array(x)))\n"
      ],
      "metadata": {
        "id": "vbMrhd9-naJ7",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1731201092547,
          "user_tz": 480,
          "elapsed": 54,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "50a99b95-eb6c-46f9-b178-c09723fca68f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "6\n",
            "6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Random numbers\n",
        "\n",
        "# Numpy: it’s hard to reason about how it’s being used and updated across different threads, processes, and devices\n",
        "print(np.random.random())\n",
        "print(np.random.random())\n",
        "print(np.random.random())\n",
        "\n",
        "np.random.seed(0)\n",
        "rng_state = np.random.get_state()\n",
        "# print(rng_state)\n",
        "\n",
        "_ = np.random.uniform()\n",
        "rng_state = np.random.get_state()\n",
        "#print(rng_state)\n",
        "# --> ('MT19937', array([2443250962, 1093594115, 1878467924,\n",
        "#       ..., 2648828502, 1678096082], dtype=uint32), 2, 0, 0.0)\n",
        "\n",
        "# Let's exhaust the entropy in this PRNG statevector\n",
        "for i in range(311):\n",
        "  _ = np.random.uniform()\n",
        "rng_state = np.random.get_state()\n",
        "#print(rng_state)\n",
        "# --> ('MT19937', array([2443250962, 1093594115, 1878467924,\n",
        "#       ..., 2648828502, 1678096082], dtype=uint32), 624, 0, 0.0)\n",
        "\n",
        "# Next call iterates the RNG state for a new batch of fake \"entropy\".\n",
        "_ = np.random.uniform()\n",
        "rng_state = np.random.get_state()\n",
        "# print(rng_state)\n",
        "# --> ('MT19937', array([1499117434, 2949980591, 2242547484,\n",
        "#      4162027047, 3277342478], dtype=uint32), 2, 0, 0.0)\n",
        "\n",
        "# Jax\n",
        "key = random.key(0)\n",
        "key\n",
        "\n",
        "print(random.normal(key, shape=(1,)))\n",
        "print(key)\n",
        "# No no no! the same value\n",
        "print(random.normal(key, shape=(1,)))\n",
        "print(key)\n",
        "\n",
        "print(\"old key\", key)\n",
        "key, subkey = random.split(key)\n",
        "normal_pseudorandom = random.normal(subkey, shape=(1,))\n",
        "print(r\"    \\---SPLIT --> new key   \", key)\n",
        "print(r\"             \\--> new subkey\", subkey, \"--> normal\", normal_pseudorandom)\n",
        "\n",
        "key, *subkeys = random.split(key, 4)\n",
        "for subkey in subkeys:\n",
        "  print(random.normal(subkey, shape=(1,)))"
      ],
      "metadata": {
        "id": "BbjYctb7oIbq",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1731201282294,
          "user_tz": 480,
          "elapsed": 1074,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "2f074f0c-faf4-4d1a-b494-0e3cc0ffcac3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.08960303423860538\n",
            "0.6720478073539145\n",
            "0.24536720985284477\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Array((), dtype=key<fry>) overlaying:\n",
              "[0 0]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Control flow"
      ],
      "metadata": {
        "id": "sGYYuTjvpBKO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Python control_flow + autodiff\n",
        "\n",
        "def f(x):\n",
        "  if x < 3:\n",
        "    return 3. * x ** 2\n",
        "  else:\n",
        "    return -4 * x\n",
        "\n",
        "print(grad(f)(2.))  # ok!\n",
        "print(grad(f)(4.))  # ok!"
      ],
      "metadata": {
        "id": "IVTdFlmIpDnf",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1731212382678,
          "user_tz": 480,
          "elapsed": 9118,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "71aacbb6-0445-4e68-9eee-06fd90930c11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12.0\n",
            "-4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Python control flow + JIT\n",
        "\n",
        "@jit\n",
        "def f(x):\n",
        "  for i in range(3):\n",
        "    x = 2 * x\n",
        "  return x\n",
        "\n",
        "print(f(3))\n",
        "\n",
        "@jit\n",
        "def g(x):\n",
        "  y = 0.\n",
        "  for i in range(x.shape[0]):\n",
        "    y = y + x[i]\n",
        "  return y\n",
        "\n",
        "print(g(jnp.array([1., 2., 3.])))"
      ],
      "metadata": {
        "id": "zKjHBKwGTLkY",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1731212421772,
          "user_tz": 480,
          "elapsed": 823,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "de705319-e626-4c6c-97bd-c13f26ae7888"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@jit\n",
        "def f(x):\n",
        "  if x < 3:\n",
        "    return 3. * x ** 2\n",
        "  else:\n",
        "    return -4 * x\n",
        "\n",
        "# This will fail!\n",
        "# When we jit-compile a function, we usually want to compile a version of the function that works for many different argument values, so that we can cache and reuse the compiled code. That way we don’t have to re-compile on each function evaluation.\n",
        "# f(2)\n",
        "\n",
        "# specify to trace on concrete values of some arguments\n",
        "def f(x):\n",
        "  if x < 3:\n",
        "    return 3. * x ** 2\n",
        "  else:\n",
        "    return -4 * x\n",
        "\n",
        "f = jit(f, static_argnums=(0,))\n",
        "\n",
        "print(f(2.))\n",
        "\n",
        "def f(x, n):\n",
        "  y = 0.\n",
        "  for i in range(n):\n",
        "    y = y + x[i]\n",
        "  return y\n",
        "\n",
        "f = jit(f, static_argnums=(1,))\n",
        "\n",
        "f(jnp.array([2., 3., 4.]), 2)"
      ],
      "metadata": {
        "id": "aaHuKNPTTgpr",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1731212580628,
          "user_tz": 480,
          "elapsed": 599,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "6a37e67a-d128-4488-ed6f-4e12451a9ed3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def example_fun(length, val):\n",
        "  return jnp.ones((length,)) * val\n",
        "# un-jit'd works fine\n",
        "print(example_fun(5, 4))\n",
        "\n",
        "bad_example_jit = jit(example_fun)\n",
        "# this will fail:\n",
        "# bad_example_jit(10, 4)\n",
        "\n",
        "# static_argnums tells JAX to recompile on changes at these argument positions:\n",
        "good_example_jit = jit(example_fun, static_argnums=(0,))\n",
        "# first compile\n",
        "print(good_example_jit(10, 4))\n",
        "# recompiles\n",
        "print(good_example_jit(5, 4))"
      ],
      "metadata": {
        "id": "xcT5pV9oVMlk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Structured control flow primitives\n",
        "\n",
        "def cond(pred, true_fun, false_fun, operand):\n",
        "  if pred:\n",
        "    return true_fun(operand)\n",
        "  else:\n",
        "    return false_fun(operand)\n",
        "\n",
        "from jax import lax\n",
        "\n",
        "operand = jnp.array([0.])\n",
        "print(lax.cond(True, lambda x: x+1, lambda x: x-1, operand))\n",
        "# --> array([1.], dtype=float32)\n",
        "print(lax.cond(False, lambda x: x+1, lambda x: x-1, operand))\n",
        "# --> array([-1.], dtype=float32)\n",
        "\n",
        "def while_loop(cond_fun, body_fun, init_val):\n",
        "  val = init_val\n",
        "  while cond_fun(val):\n",
        "    val = body_fun(val)\n",
        "  return val\n",
        "\n",
        "init_val = 0\n",
        "cond_fun = lambda x: x < 10\n",
        "body_fun = lambda x: x+1\n",
        "lax.while_loop(cond_fun, body_fun, init_val)\n",
        "# --> array(10, dtype=int32)\n",
        "\n",
        "def fori_loop(start, stop, body_fun, init_val):\n",
        "  val = init_val\n",
        "  for i in range(start, stop):\n",
        "    val = body_fun(i, val)\n",
        "  return val\n",
        "\n",
        "init_val = 0\n",
        "start = 0\n",
        "stop = 10\n",
        "body_fun = lambda i,x: x+i\n",
        "lax.fori_loop(start, stop, body_fun, init_val)\n",
        "# --> array(45, dtype=int32)"
      ],
      "metadata": {
        "id": "t6kvmetpVrA0",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1731213119412,
          "user_tz": 480,
          "elapsed": 1960,
          "user": {
            "displayName": "Fanny Wei",
            "userId": "10420040816114080596"
          }
        },
        "outputId": "a985c7f4-1626-4cc5-c58d-96577c0c8a4d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.]\n",
            "[-1.]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dynamic shapes"
      ],
      "metadata": {
        "id": "ub3EgZakZlGZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def nansum(x):\n",
        "  mask = ~jnp.isnan(x)  # boolean mask selecting non-nan values\n",
        "  x_without_nans = x[mask]\n",
        "  return x_without_nans.sum()\n",
        "\n",
        "x = jnp.array([1, 2, jnp.nan, 3, 4])\n",
        "print(nansum(x))\n",
        "\n",
        "# fails because the size of x_without_nans is dependent on the values within x, which is another way of saying its size is dynamic\n",
        "# jax.jit(nansum)(x)\n",
        "\n",
        "@jax.jit\n",
        "def nansum_2(x):\n",
        "  mask = ~jnp.isnan(x)  # boolean mask selecting non-nan values\n",
        "  return jnp.where(mask, x, 0).sum()\n",
        "\n",
        "print(nansum_2(x))"
      ],
      "metadata": {
        "id": "XlzcMDa9akPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Debugging NaNs"
      ],
      "metadata": {
        "id": "SITJfsyYa7sS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Double (64bit) precision"
      ],
      "metadata": {
        "id": "vWOgtAdMhiKP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Miscellaneous divergences from NumPy"
      ],
      "metadata": {
        "id": "r2rlzkU1h2gE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BBxzUtFna8ul"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}